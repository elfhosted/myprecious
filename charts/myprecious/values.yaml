# This controls whether our automation will auto-release this to stable during the daily maint window
safeToRelease: true

# false by default means the FSN cluster (so no migrations)
location:
  enabled: false
zurgling:
  enabled: false
debridling:
  enabled: false
sidekick:
  enabled: false

# false by default
volsync:
  enabled: false
  restic_repository:
  restic_password:
  aws_access_key_id:
  aws_secret_access_key:

# Set these to the default if nothing else is set
storageclass:
  rwx:
    name: ceph-filesystem-ssd
    accessMode: ReadWriteMany
    volumeSnapshotClassName: ceph-filesystem
  rwo:
    name: ceph-block-ssd
    accessMode: ReadWriteOnce
    volumeSnapshotClassName: ceph-block

# These control the egress bandwidth of the semi-dedi products
hobbit_streamer_podAnnotations: &hobbit_streamer_podAnnotations
  kubernetes.io/egress-bandwidth: "250M"
ranger_streamer_podAnnotations: &ranger_streamer_podAnnotations
  kubernetes.io/egress-bandwidth: "500M"
halfling_streamer_podAnnotations: &halfling_streamer_podAnnotations
  kubernetes.io/egress-bandwidth: "1000M"
nazgul_streamer_podAnnotations: &nazgul_streamer_podAnnotations
  kubernetes.io/egress-bandwidth: "1000M"

# These control the requests used to "anchor" a stack to a particular dedicated node. The following defaults can be overridden on a per-cluster basis:
hobbit_zurg_resources: &hobbit_zurg_resources
  requests:
    cpu: "1.8"
    memory: 30Mi
  limits:
    cpu: "2"
    memory: 4Gi

ranger_zurg_resources: &ranger_zurg_resources
  requests:
    cpu: "3500m"
    memory: 30Mi
  limits:
    cpu: "4"
    memory: 4Gi

halfling_zurg_resources: &halfling_zurg_resources
  requests:
    cpu: "7"
    memory: 30Mi
  limits:
    cpu: "8"
    memory: 4Gi

nazgul_zurg_resources: &nazgul_zurg_resources
  requests:
    cpu: "7"
    memory: 30Mi
  limits:
    cpu: "16"
    memory: 4Gi


# These allow us to manage RAM usage on streamers
hobbit_streamer_resources: &hobbit_streamer_resources
  requests:
    cpu: "10m"
    memory: 30Mi
  limits:
    cpu: "2"
    memory: 4Gi

ranger_streamer_resources: &ranger_streamer_resources
  requests:
    cpu: 10m
    memory: 30Mi
  limits:
    cpu: 4
    memory: 4Gi

# Giving more than 4 CPU to a streamer is unwise regardless
halfling_streamer_resources: &halfling_streamer_resources
  requests:
    cpu: 10m
    memory: 30Mi
  limits:
    cpu: 4
    memory: 4Gi

nazgul_streamer_resources: &nazgul_streamer_resources
  requests:
    cpu: 10m
    memory: 30Mi
  limits:
    cpu: 4
    memory: 4Gi

# sets the user's base dns domain
dns_domain: elfhosted.com

tooling_image: &tooling_image ghcr.io/elfhosted/tooling:focal-20240530@sha256:458d1f3b54e9455b5cdad3c341d6853a6fdd75ac3f1120931ca3c09ac4b588de

# all RD pods have to exist with zurg - make this soft for now
standard_affinity: &standard_affinity
  podAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app.elfhosted.com/role
          operator: In
          values:
          - nodefinder # use nodefinder in the absence of zurg...
      topologyKey: "kubernetes.io/hostname"

dedicated_affinity: &dedicated_affinity
  podAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app.elfhosted.com/role
          operator: In
          values:
          - nodefinder # use nodefinder in the absence of zurg...
      topologyKey: "kubernetes.io/hostname"

standard_tolerations: &standard_tolerations
# not using tolerations anymore
# - key: node-role.elfhosted.com/dedicated
#   operator: Exists
# - key: node-role.elfhosted.com/hobbit
#   operator: Exists

hobbit_tolerations: &hobbit_tolerations
# not using tolerations anymore
# - key: node-role.elfhosted.com/hobbit
#   operator: Exists

# Set minimal requests so that pods can co-exist with streamers
hobbit_resources: &hobbit_resources
  requests:
    cpu: "1m"
    memory: "16Mi"
  limits:
    cpu: "1"
    memory: 4Gi

ranger_resources: &ranger_resources
  requests:
    cpu: "1m"
    memory: "16Mi"
  limits:
    cpu: "2"
    memory: 8Gi

volumespec_ephemeral_volume_1000g: &volumespec_ephemeral_volume_1000g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 1000Gi

volumespec_ephemeral_volume_100g: &volumespec_ephemeral_volume_100g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 100Gi

volumespec_ephemeral_volume_1g: &volumespec_ephemeral_volume_1g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 1Gi

volumespec_ephemeral_volume_10g: &volumespec_ephemeral_volume_10g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 10Gi

volumespec_ephemeral_volume_50g: &volumespec_ephemeral_volume_50g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 50Gi

volumespec_ephemeral_volume_200g: &volumespec_ephemeral_volume_200g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 200Gi

volumespec_ephemeral_volume_500g: &volumespec_ephemeral_volume_500g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 500Gi

# And this makes the media / rclone mounts tidier.
rclonemountrealdebridzurg: &rclonemountrealdebridzurg
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: realdebrid-zurg
  mountPath: /storage/realdebrid-zurg
rclonemountdebridlink: &rclonemountdebridlink
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: debridlink
  mountPath: /storage/debridlink
rclonemountalldebrid: &rclonemountalldebrid
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: alldebrid
  mountPath: /storage/alldebrid
rclonemountpremiumize: &rclonemountpremiumize
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: premiumize
  mountPath: /storage/premiumize
rclonemounttorbox: &rclonemounttorbox
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: torbox
  mountPath: /storage/torbox
rclonemountdebridav: &rclonemountdebridav
  enabled: false 
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: debridav
  mountPath: /storage/debridav    
rclone: &rclone
  enabled: true # everyone gets an rclone mount
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: rclone
  mountPath: /storage/rclone  

# This simplfies the process of adding all the optional mounts to every app
storagemounts: &storagemounts
  rclone: *rclone
  rclonemountrealdebridzurg: *rclonemountrealdebridzurg
  rclonemountdebridlink: *rclonemountdebridlink
  rclonemountalldebrid: *rclonemountalldebrid
  rclonemountpremiumize: *rclonemountpremiumize
  rclonemounttorbox: *rclonemounttorbox
  rclonemountdebridav: *rclonemountdebridav
  tmp: &tmp
    enabled: true
    type: emptyDir
    mountPath: /tmp
  symlinks: &symlinks
    enabled: true
    type: custom
    volumeSpec:
      persistentVolumeClaim:
        claimName: symlinks
    mountPath: /storage/symlinks
  backup: &backup
    enabled: true
    type: custom
    volumeSpec:
      persistentVolumeClaim:
        claimName: backup

# The entire bootstrap sidecar/additionalcontainer
default_resources: &default_resources
  requests:
    cpu: 0m
    memory: 1Mi
    # ephemeral-storage: 50Mi
  limits:
    cpu: 1
    memory: 4Gi # just a safety net against bugs!
    # ephemeral-storage: 2Gi # a safety net against node ephemeral space exhaustion

default_securitycontext: &default_securitycontext
  seccompProfile:
    type: RuntimeDefault
  readOnlyRootFilesystem: true
  allowPrivilegeEscalation: false
  runAsUser: 568
  runAsGroup: 568
  capabilities:
    drop:
    - ALL

speedtest_securitycontext: &speedtest_securitycontext
  seccompProfile:
    type: RuntimeDefault
  readOnlyRootFilesystem: false
  allowPrivilegeEscalation: false
  runAsUser: 101
  runAsGroup: 101
  capabilities:
    drop:
    - ALL

# We use this to provide env not only to bootstrap, but also to the torrent clients which use elfvpn
# it's necessary since the wireguard configs are in S3
bootstrap_env: &bootstrap_env
- name: AWS_ACCESS_KEY_ID
  valueFrom:
    secretKeyRef:
      key: access-key-id
      name: b2-elfhosted-config-ro
- name: AWS_SECRET_ACCESS_KEY
  valueFrom:
    secretKeyRef:
      key: secret-key
      name: b2-elfhosted-config-ro
- name: S3_ENDPOINT_URL
  value: https://s3.us-west-000.backblazeb2.com
- name: K8S_APP_NAME
  valueFrom:
    fieldRef:
      fieldPath: metadata.labels['app.kubernetes.io/name']
- name: ELF_APP_NAME
  valueFrom:
    fieldRef:
      fieldPath: metadata.labels['app.elfhosted.com/name']

migrate_data: &migrate_data
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /bin/bash
  - -c
  - |

    if [[ ! -f /config/.migrated-20241007 ]]
    then
      if [[ ! -z "$(ls -A /migration)" ]]
      then
        echo "Migrating from /migration/..."
        cp -rfpv /migration/* /config/
        touch /config/.migrated-20241007
      fi
    else
      echo "No migration necessary"
    fi

  volumeMounts:
  - mountPath: /config
    name: config
  - mountPath: /migration
    name: migration

  env: *bootstrap_env
  resources: *default_resources
  securityContext: *default_securitycontext

bootstrap: &bootstrap
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /bin/bash
  - -c
  - |
    set -e

    # Allows us to use app.elfhosted.com/name, but fall back to app.kubernetes.io/name if the former doesn't exist
    if [[ -z "$ELF_APP_NAME" ]]; then
      ELF_APP_NAME=$K8S_APP_NAME
    fi

    # look for commands - we match specific names in order of least-destructive
    TIMESTAMP_NOW=$(date +%s)
    if [[ -f /etc/elfbot/pause ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/pause)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=pause
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/backup && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/backup)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=backup
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/reset && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/reset)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=reset
      fi
    fi

    case $COMMAND in

      "pause")
        echo "Recent pause command found, sleeping 5m.."
        sleep 300
        ;;

      "reset")
        echo "Recent reset command found, resetting"
        rm -rf /config/*
        ;;

      "backup")
        echo "Recent backup command found, backing up to /storage/backup/${ELF_APP_NAME}-${TIMESTAMP}"
        TIMESTAMP=$(printf '%(%Y-%m-%d--%H-%M)T\n' -1)
        cp -rfp /config /storage/backup/$ELF_APP_NAME-$TIMESTAMP
        ;;

    esac

    if [[ ! -f /config/i-am-bootstrapped ]]
    then
      echo "Bootstrapping from goldilocks config..."
      s5cmd sync s3://elfhosted-config/goldilocks/$ELF_APP_NAME/* /config/
      touch /config/i-am-bootstrapped
    fi

  volumeMounts:
  - mountPath: /etc/elfbot
    name: elfbot
  - mountPath: /config
    name: config
  - mountPath: /storage/backup
    name: backup
  - mountPath: /tmp
    name: tmp
  env: *bootstrap_env
  resources: *default_resources
  securityContext: *default_securitycontext

bootstrap_elfbot: &bootstrap_elfbot
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /bin/bash
  - -c
  - |
    set -e

    # Allows us to use app.elfhosted.com/name, but fall back to app.kubernetes.io/name if the former doesn't exist
    if [[ -z "$ELF_APP_NAME" ]]; then
      ELF_APP_NAME=$K8S_APP_NAME
    fi

    # look for commands - we match specific names in order of least-destructive
    TIMESTAMP_NOW=$(date +%s)
    if [[ -f /etc/elfbot/pause ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/pause)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=pause
      fi
    fi


    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/backup && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/backup)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=backup
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/reset && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/reset)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=reset
      fi
    fi

    case $COMMAND in

      "pause")
        echo "Recent pause command found, sleeping 5m.."
        sleep 300
        ;;

      "reset")
        echo "Recent reset command found, resetting"
        rm -rf /config/*
        ;;

      "backup")
        echo "Recent backup command found, backing up to /storage/elfstorage/backup/${ELF_APP_NAME}-${TIMESTAMP}"
        mkdir -p /storage/elfstorage/backup
        TIMESTAMP=$(printf '%(%Y-%m-%d--%H-%M)T\n' -1)
        cp -rfp /config /storage/elfstorage/backup/$ELF_APP_NAME-$TIMESTAMP
        ;;

    esac
  volumeMounts:
  - mountPath: /etc/elfbot
    name: elfbot
  - mountPath: /config
    name: config
  - mountPath: /tmp
    name: tmp
  env: *bootstrap_env
  resources: *default_resources
  securityContext: *default_securitycontext

storagehub_bootstrap: &storagehub_bootstrap
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /bin/bash
  - -c
  - |
    set -e

    # Allows us to use app.elfhosted.com/name, but fall back to app.kubernetes.io/name if the former doesn't exist
    if [[ -z "$ELF_APP_NAME" ]]; then
      ELF_APP_NAME=$K8S_APP_NAME
    fi

    # look for commands - we match specific names in order of least-destructive
    TIMESTAMP_NOW=$(date +%s)
    if [[ -f /etc/elfbot/pause ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/pause)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=pause
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/backup && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/backup)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=backup
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/reset && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/reset)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=reset
      fi
    fi

    case $COMMAND in

      "pause")
        echo "Recent pause command found, sleeping 5m.."
        sleep 300
        ;;

      "reset")
        echo "Recent reset command found, resetting"
        rm -rf /config/${ELF_APP_NAME}/*
        ;;

      "backup")
        echo "Recent backup command found, backing up to /storage/elfstorage/backup/${ELF_APP_NAME}-${TIMESTAMP}"
        mkdir -p /storage/elfstorage/backup
        TIMESTAMP=$(printf '%(%Y-%m-%d--%H-%M)T\n' -1)
        cp -rfp /config/${ELF_APP_NAME} /storage/elfstorage/backup/$ELF_APP_NAME-$TIMESTAMP
        ;;

    esac

    if [[ ! -f /config/${ELF_APP_NAME}/i-am-bootstrapped ]]
    then
      echo "Bootstrapping from goldilocks config..."
      s5cmd sync s3://elfhosted-config/goldilocks/$ELF_APP_NAME/* /config/${ELF_APP_NAME}/
      touch /config/${ELF_APP_NAME}/i-am-bootstrapped
    fi

  volumeMounts:
  - mountPath: /etc/elfbot
    name: elfbot
  - mountPath: /config
    name: config
  - mountPath: /tmp
    name: tmp
  env: *bootstrap_env
  resources: *default_resources
  securityContext: *default_securitycontext

# Eventually we'll remove the old one, and rename this to bootstrap
bootstrap_migration: &bootstrap_migration
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /bin/bash
  - -c
  - |
    set -e

    # Allows us to use app.elfhosted.com/name, but fall back to app.kubernetes.io/name if the former doesn't exist
    if [[ -z "$ELF_APP_NAME" ]]; then
      ELF_APP_NAME=$K8S_APP_NAME
    fi

    # look for commands - we match specific names in order of least-destructive
    TIMESTAMP_NOW=$(date +%s)
    if [[ -f /etc/elfbot/pause ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/pause)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=pause
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/backup && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/backup)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=backup
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/reset && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/reset)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=reset
      fi
    fi

    case $COMMAND in

      "pause")
        echo "Recent pause command found, sleeping 5m.."
        sleep 300
        ;;

      "reset")
        echo "Recent reset command found, resetting"
        rm -rfv /config/*
        ;;

      "backup")
        echo "Recent backup command found, backing up to /storage/elfstorage/backup/${ELF_APP_NAME}-${TIMESTAMP}"
        mkdir -p /storage/elfstorage/backup
        TIMESTAMP=$(printf '%(%Y-%m-%d--%H-%M)T\n' -1)
        cp -rfp /config /storage/elfstorage/backup/$ELF_APP_NAME-$TIMESTAMP
        ;;

    esac

    if [[ ! -f /config/i-am-migrated ]]
    then
      if [[ ! -z "$(ls -A /config-hdd)" ]]
      then
        echo "Migrating from /config-hdd/..."
        time cp /config-hdd/* /config/ -rfpv
        touch /config/i-am-migrated
      fi
    fi


    if [[ ! -f /config/i-am-bootstrapped ]]
    then
      echo "Bootstrapping from goldilocks config..."
      s5cmd sync s3://elfhosted-config/goldilocks/$ELF_APP_NAME/* /config/
      touch /config/i-am-bootstrapped
    fi
  volumeMounts:
  - mountPath: /etc/elfbot
    name: elfbot
  - mountPath: /config
    name: config
  - mountPath: /migation
    name: confighdd
  env: *bootstrap_env
  resources: *default_resources
  securityContext: *default_securitycontext

# This lets users buy blocks of 1TB storage, and add it to their 100Gi
elfstoragetb:
  quantity: 0
# And this lets a user buy a bundle (different SKU), and then still add more elfstorage later
elfstoragetbbundled:
  quantity: 0

# provide a default
userId: 1

# our VPN loadbalancerIP
torrentLoadBalancerIP: 10.0.42.101

# these are the "exposed" services which allow users to override SSO
# by themselves, they do nothing, but they allow us to selectively disable
# SSO on ingressroutes, or to use non-standard API keys in Homer
radarrexposed:
  enabled: false
  apikey: 041776c8d5f74bf295aa486d9d51c33a
radarr4kexposed:
  enabled: false
  apikey: 7da5d4ba79804527b78a78b68c7a0781
sonarrexposed:
  enabled: false
  apikey: a6f1c7d07fab4be49c5c1cb545f85a76
sonarr4kexposed:
  enabled: false
  apikey: e4f93c115169484bbed19821f7ac8e49
lidarrexposed:
  enabled: false
  apikey: 0e68e28531a249659737513d3102bfe9
readarrexposed:
  enabled: false
  apikey: 74b033ff59964011b8a32c014fdb9b68
readarraudioexposed:
  enabled: false
  apikey: 8496cefe2c6b46ee921e18caddf6a943
prowlarrexposed:
  enabled: false
  apikey: c53bc3bd17c645c3a457e5342a02cd66
bazarrexposed:
  enabled: false
  apikey: 94ab8212a12378fa5333cbf75a3c0390
bazarr4kexposed:
  enabled: false
  apikey: 393bda5f898886a2b87413e6452313af
qbittorrentexposed:
  enabled: false
rdtclientexposed:
  enabled: false
rdtclientalldebridexposed:
  enabled: false
delugeexposed:
  enabled: false
rutorrentexposed:
  enabled: false
sabnzbdexposed:
  enabled: false
  apikey: 8flkbru7ncdps3dzzgk48q2msz41m4on
nzbgetexposed:
  enabled: false
mylarrexposed:
  enabled: false
  apikey: 0f97f6a7f352c63eb43fcb7e53ea9d8f
rivenexposed:
  enabled: false
  apikey: 1nMZNC0Cg6UP7sblvFirUi9Sad4ga84u  
tunarrexposed:
  enabled: false
cometexposed:
  enabled: false  
aiostreamsexposed:
  enabled: false    
davioexposed:
  enabled: false  
jackettioexposed:
  enabled: false  
knightcrawlerexposed:
  enabled: false  
mediafusionexposed:
  enabled: false
stremthruexposed:
  enabled: false  
stremifyexposed:
  enabled: false    
stremiojackettexposed:
  enabled: false  
youriptvexposed:
  enabled: false
threadfinexposed:
  enabled: false
zurgexposed:
  enabled: false
elfassessment:
  enabled: false
uptimekumacustomdomain:
  enabled: false
mattermostcustomdomain:
  enabled: false
vaultwardencustomdomain:
  enabled: false
jellyseerrcustomdomain:
  enabled: false
overseerrcustomdomain:
  enabled: false
plexcustomdomain:
  enabled: false
jellyfincustomdomain:
  enabled: false
embycustomdomain:
  enabled: false
flixiocustomdomain:
  enabled: false
pairdropcustomdomain:
  enabled: false
gotosocialcustomdomain:
  enabled: false  
blueskypdscustomdomain:
  enabled: false    

rutorrentgluetun: &rutorrent
  enabled: false
  automountServiceAccountToken: false
  image:
      repository: ghcr.io/elfhosted/rutorrent
      tag: 4.3.6-60@sha256:d26fa29c30bd2622137426dca451d0d670811096676e7a07b5d4d00bab275fd1
  priorityClassName: tenant-bulk
  podLabels:
    app.elfhosted.com/name: rutorrent
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,rutorrent-config,rutorrent-gluetun-config,elfbot-rutorrent" # Reload the deployment every time the yaml config changes
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    # runAsUser: 568 # enforced in env vars
    # runAsGroup: 568
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  envFrom:
  - configMapRef:
      name: elfbot-rutorrent
      optional: true
  # we need the injected initcontainer to run as root, so we can't change the pod-level uid/gid
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 568 # s6's fault
    # runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  persistence:
    <<: *storagemounts
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_100g
    config:
      enabled: true
      type: custom
      mountPath: /data/rtorrent/
      subPath: rutorrent
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rutorrent
          optional: true
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    port-range: # Used for dynamic port-forwarding
      enabled: true
      type: emptyDir
      mountPath: /port-range
      sizeLimit: 1Gi
    custom-rtlocal:
      enabled: "true"
      mountPath: "/.rtlocal.rc-elfhosted"
      subPath: ".rtlocal.rc-elfhosted"
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
    custom-rtorrentrc:
      enabled: "true"
      mountPath: "/.rtorrent.rc-elfhosted"
      subPath: ".rtorrent.rc-elfhosted"
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
    custom-s6-init-05:
      enabled: "true"
      mountPath: "/etc/cont-init.d/05-apply-elfhosted-config.sh"
      subPath: "05-apply-elfhosted-config.sh"
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
          defaultMode: 0755
    custom-s6-init-06:
      enabled: "true"
      mountPath: "/etc/cont-init.d/02-wait-for-vpn.sh"
      subPath: "02-wait-for-vpn.sh"
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
          defaultMode: 0755
    custom-s6-init-07:
      enabled: "true"
      mountPath: "/etc/cont-init.d/03-set-inbound-port.sh"
      subPath: "03-set-inbound-port.sh "
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
          defaultMode: 0755
    dante-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: dante-config
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: false # necessary for probes, but probes aren't working with vpn addon currently
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1200Mi
  env:
    # -- Set the container timezone
    PUID: 568
    GUID: 568
    RUTORRENT_PORT: 8080 # necessary for health checks
    # S6_READ_ONLY_ROOT: 1 # this seems to break rutorrent :(
    WAIT_FOR_VPN: "true"
    PORT_FILE: /data/rtorrent/forwarded-port
    WAN_IP_CMD: 'curl -s ifconfig.me'
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rutorrent
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi
      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext: *default_securitycontext
      resources: *default_resources
      envFrom:
      - configMapRef:
          name: rutorrent-gluetun-config
  addons:
    vpn: &rutorrent_addons_vpn
      enabled: true
      type: gluetun
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      envFrom:
      - configMapRef:
          name: rutorrent-gluetun-config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
        subPath: rutorrent
      config: # We have to set this to null so that we can override with our own config

      # The scripts that get run when the VPN connection opens/closes are defined here.
      # The default scripts will write a string to represent the current connection state to a file.
      # Our qBittorrent image has a feature that can wait for this file to contain the word 'connected' before actually starting the application.
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus
  additionalContainers:
    # Use this to provied proxied access to arrs
    dante:
      image: ghcr.io/elfhosted/dante:v1.4.3
      env: *bootstrap_env
      securityContext: *default_securitycontext
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /etc/sockd.conf
        name: dante-config
        subPath: sockd.conf
    mam-helper:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |
        set -e
        set -x

        echo "Waiting for VPN to be connected..."
        while ! grep -s -q "connected" /shared/vpnstatus; do
            # Also account for gluetun-style http controller
            if (curl -s http://localhost:8042/v1/openvpn/status | grep -q running); then
                break
            fi
            echo "VPN not connected"
            sleep 2
        done
        echo "VPN Connected, processing cookies..."

        # If we have a cookie already, try to use it
        if [[ -f /config/mam/saved.cookies ]]; then
          curl -c /config/mam/saved.cookies -b /config/mam/saved.cookies https://t.myanonamouse.net/json/dynamicSeedbox.php  -o /config/mam/mam_id-curl-output.log
        fi

        # Now whether that worked or not, look for /config/mam/mam_id
        mkdir -p /config/mam
        while [ 1 ]; do
          if [[ -f /config/mam/mam_id ]]; then
            curl -c /config/mam/saved.cookies -b "mam_id=$(cat /config/mam/mam_id)" https://t.myanonamouse.net/json/dynamicSeedbox.php -o /config/mam/mam_id-curl-output.log
            mv /config/mam/mam_id /config/mam/mam_id_processed
          fi
          sleep 1m
        done
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: rutorrent
      - mountPath: /shared
        name: shared
      resources: *default_resources
      securityContext: *default_securitycontext

rutorrentpia:
  <<: *rutorrent
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,rutorrent-config,rutorrent-pia-config,elfbot-rutorrent,dante-config" # Reload the deployment every time the yaml config changes
  addons:
    vpn:
      <<: *rutorrent_addons_vpn
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      envFrom:
      - configMapRef:
          name: rutorrent-pia-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rutorrent
      - mountPath: /tmp
        name: tmp

delugegluetun: &deluge
  enabled: false
  podLabels:
    app.elfhosted.com/name: deluge
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M"
  automountServiceAccountToken: false
  image:
    repository: ghcr.io/geek-cookbook/deluge
    tag: 2.1.1@sha256:448324e342c47020e4e9fbc236282ceb80ebebd7934a486a6f1e487a7e4034bf
  priorityClassName: tenant-bulk
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  # we need the injected initcontainer to run as root, so we can't change the pod-level uid/gid
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-deluge,deluge-gluetun-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    <<: *storagemounts
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_100g
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: deluge
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-deluge
          optional: true
    elfscripts:
      enabled: "true"
      mountPath: "/elfscripts/"
      type: "custom"
      volumeSpec:
        configMap:
          name: deluge-elfscripts
          defaultMode: 0755
    dante-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: dante-config
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 8112
  env:
    # -- Set the container timezone
    TZ: UTC
    PUID: 568
    PGID: 568
    DELUGE_LOGLEVEL: "info"
  envFrom:
  - configMapRef:
      name: elfbot-deluge
      optional: true
  extraEnvVars:
  - name: PORT_FILE
    valueFrom:
      configMapKeyRef:
        name: deluge-gluetun-config
        key: PORT_FILE
    optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/deluge/torrent_files

        JQ_FILTER=".listen_random_port=false"
        JQ_FILTER="${JQ_FILTER} | .pre_allocate_storage=false"
        JQ_FILTER="${JQ_FILTER} | .stop_seed_ratio=2"
        JQ_FILTER="${JQ_FILTER} | .cache_size=52428"
        JQ_FILTER="${JQ_FILTER} | .share_ratio_limit=2"
        JQ_FILTER="${JQ_FILTER} | .stop_seed_at_ratio=true"

        jq "${JQ_FILTER}" /config/core.conf > /config/core-new.conf
        cp /config/core-new.conf /config/core.conf

        # # Avoid session timeouts
        # sed -i  "s/session_timeout:\".*/session_timeout\": 99999,/" /config/web.conf

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /shared
        name: shared
      env: *bootstrap_env
      securityContext: *default_securitycontext
      envFrom:
      - configMapRef:
          name: deluge-gluetun-config
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1500Mi
  addons:
    vpn: &deluge_addons_vpn
      enabled: true
      type: gluetun
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      envFrom:
      - configMapRef:
          name: deluge-gluetun-config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
        subPath: deluge
      config: # We have to set this to null so that we can override with our own config

      # The scripts that get run when the VPN connection opens/closes are defined here.
      # The default scripts will write a string to represent the current connection state to a file.
      # Our qBittorrent image has a feature that can wait for this file to contain the word 'connected' before actually starting the application.
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus
  additionalContainers:
    deluge-web:
      image: ghcr.io/geek-cookbook/deluge:2.1.1@sha256:448324e342c47020e4e9fbc236282ceb80ebebd7934a486a6f1e487a7e4034bf
      command:
      - /usr/bin/deluge-web
      - -L
      - info
      - -d
      - -c
      - /config
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /tmp
        name: tmp
      env:
        PYTHON_EGG_CACHE: /tmp/.cache

delugepia:
  <<: *deluge
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,deluge-config,deluge-pia-config,elfbot-deluge" # Reload the deployment every time the yaml config changes
  addons:
    vpn:
      <<: *deluge_addons_vpn
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      envFrom:
      - configMapRef:
          name: deluge-pia-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/deluge/torrent_files

        JQ_FILTER=".listen_random_port=false"
        JQ_FILTER="${JQ_FILTER} | .pre_allocate_storage=false"
        JQ_FILTER="${JQ_FILTER} | .stop_seed_ratio=2"
        JQ_FILTER="${JQ_FILTER} | .cache_size=52428"
        JQ_FILTER="${JQ_FILTER} | .share_ratio_limit=2"
        JQ_FILTER="${JQ_FILTER} | .stop_seed_at_ratio=true"

        jq "${JQ_FILTER}" /config/core.conf > /config/core-new.conf
        cp /config/core-new.conf /config/core.conf

        # # Avoid session timeouts
        # sed -i  "s/session_timeout:\".*/session_timeout\": 99999,/" /config/web.conf

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /shared
        name: shared
      env: *bootstrap_env
      securityContext: *default_securitycontext
  additionalContainers:
    deluge-web:
      image: ghcr.io/geek-cookbook/deluge:2.1.1@sha256:448324e342c47020e4e9fbc236282ceb80ebebd7934a486a6f1e487a7e4034bf
      command:
      - /usr/bin/deluge-web
      - -L
      - info
      - -d
      - -c
      - /config
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /tmp
        name: tmp
      env:
        PYTHON_EGG_CACHE: /tmp/.cache
    # Use this to provied proxied access to arrs
    dante:
      image: ghcr.io/elfhosted/dante:v1.4.3
      env: *bootstrap_env
      securityContext: *default_securitycontext
      volumeMounts:
      - mountPath: /tmp
        name: tmp

qbittorrentgluetun: &qbittorrent
  podLabels:
    app.elfhosted.com/name: qbittorrent
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M"
  enabled: false
  automountServiceAccountToken: false
  image:
    registry: ghcr.io
    repository: elfhosted/qbittorrent
    tag: 5.0.2@sha256:7ff09d6a5ca2267f78161fb46eeafaf5b2af7806288fe7f3d2dfed2521374e3d
  priorityClassName: tenant-bulk
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't seem to work well with entrypoint
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-qbittorrent,qbittorrent-gluetun-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    <<: *storagemounts
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_100g
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: qbittorrent
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-qbittorrent
          optional: true
    elfscripts:
      enabled: "true"
      mountPath: "/elfscripts/"
      type: "custom"
      volumeSpec:
        configMap:
          name: qbittorrent-elfscripts
          defaultMode: 0755
    dante-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: dante-config
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
      nameOverride: spanky
  env:
    # -- Set the container timezone
    TZ: UTC
    HOME: /config
    XDG_CONFIG_HOME: /config
    XDG_DATA_HOME: /config
    WAIT_FOR_VPN: "true"
  envFrom:
  - configMapRef:
      name: elfbot-qbittorrent
      optional: true
  extraEnvVars:
  - name: PORT_FILE
    valueFrom:
      configMapKeyRef:
        name: qbittorrent-gluetun-config
        key: PORT_FILE
    optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Remove the lockfile if it exists
        if [[ -f /config/qBittorrent/lockfile ]]; then
          rm /config/qBittorrent/lockfile
        fi

        mkdir -p /config/qBittorrent/torrent_files/complete
        mkdir -p /config/qBittorrent/torrent_files/incomplete

        # Enforce 1:1 seeding ratio, and then delete
        sed -i  "s/Session\\\GlobalMaxRatio=.*/Session\\\GlobalMaxRatio=1/" /config/qBittorrent/qBittorrent.conf

        # Permit TCP only
        sed -i  "s/Session\\\BTProtocol=.*/Session\\\BTProtocol=TCP/" /config/qBittorrent/qBittorrent.conf

        # Disable CSRF protection so that Homer can show qBit stats
        sed -i  "s/WebUI\\\CSRFProtection=.*/WebUI\\\CSRFProtection=false/" /config/qBittorrent/qBittorrent.conf

        # Insist on tun0
        sed -i  "s/Session\\\Interface=.*/Session\\\Interface=tun0/" /config/qBittorrent/qBittorrent.conf
        sed -i  "s/Session\\\InterfaceName=.*/Session\\\InterfaceName=tun0/" /config/qBittorrent/qBittorrent.conf

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /shared
        name: shared
      securityContext: *default_securitycontext
      resources: *default_resources
      envFrom:
      - configMapRef:
          name: qbittorrent-gluetun-config
  additionalContainers:
    # Use this to provied proxied access to arrs
    dante:
      image: ghcr.io/elfhosted/dante:v1.4.3
      env: *bootstrap_env
      securityContext: *default_securitycontext
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /etc/sockd.conf
        name: dante-config
        subPath: sockd.conf
    mam-helper:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |
        set -e
        set -x

        echo "Waiting for VPN to be connected..."
        while ! grep -s -q "connected" /shared/vpnstatus; do
            # Also account for gluetun-style http controller
            if (curl -s http://localhost:8042/v1/openvpn/status | grep -q running); then
                break
            fi
            echo "VPN not connected"
            sleep 2
        done
        echo "VPN Connected, processing cookies..."

        # If we have a cookie already, try to use it
        if [[ -f /config/mam/saved.cookies ]]; then
          curl -c /config/mam/saved.cookies -b /config/mam/saved.cookies https://t.myanonamouse.net/json/dynamicSeedbox.php  -o /config/mam/mam_id-curl-output.log
        fi

        # Now whether that worked or not, look for /config/mam/mam_id
        mkdir -p /config/mam
        while [ 1 ]; do
          if [[ -f /config/mam/mam_id ]]; then
            curl -c /config/mam/saved.cookies -b "mam_id=$(cat /config/mam/mam_id)" https://t.myanonamouse.net/json/dynamicSeedbox.php -o /config/mam/mam_id-curl-output.log
            mv /config/mam/mam_id /config/mam/mam_id_processed
          fi
          sleep 1m
        done
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /shared
        name: shared
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
      ephemeral-storage: 50Mi
    limits:
      cpu: 500m
      memory: 2Gi # .2 GB for headroom
      ephemeral-storage: 100Mi # a safety net against node ephemeral space exhaustion
  addons:
    vpn: &qbittorrent_addons_vpn
      enabled: true
      type: gluetun
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      envFrom:
      - configMapRef:
          name: qbittorrent-gluetun-config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
        subPath: qbittorrent
      config: # We have to set this to null so that we can override with our own config

      # The scripts that get run when the VPN connection opens/closes are defined here.
      # The default scripts will write a string to represent the current connection state to a file.
      # Our qBittorrent image has a feature that can wait for this file to contain the word 'connected' before actually starting the application.
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus


# Custom service for pia
qbittorrentpia:
  <<: *qbittorrent
  env:
    PORT_FILE: /config/forwarded-port
    WAIT_FOR_VPN: "true"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-qbittorrent,qbittorrent-pia-config"
  addons:
    vpn:
      <<: *qbittorrent_addons_vpn
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      envFrom:
      - configMapRef:
          name: qbittorrent-pia-config
  additionalContainers:
    # Use this to provied proxied access to arrs
    dante:
      image: ghcr.io/elfhosted/dante:v1.4.3
      env: *bootstrap_env
      securityContext: *default_securitycontext
      volumeMounts:
      - mountPath: /tmp
        name: tmp
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Remove the lockfile if it exists
        if [[ -f /config/qBittorrent/lockfile ]]; then
          rm /config/qBittorrent/lockfile
        fi

        mkdir -p /config/qBittorrent/torrent_files/complete
        mkdir -p /config/qBittorrent/torrent_files/incomplete

        # Enforce 1:1 seeding ratio, and then delete
        sed -i  "s/Session\\\GlobalMaxRatio=.*/Session\\\GlobalMaxRatio=1/" /config/qBittorrent/qBittorrent.conf

        # Permit TCP only
        sed -i  "s/Session\\\BTProtocol=.*/Session\\\BTProtocol=TCP/" /config/qBittorrent/qBittorrent.conf

        # Disable CSRF protection so that Homer can show qBit stats
        sed -i  "s/WebUI\\\CSRFProtection=.*/WebUI\\\CSRFProtection=false/" /config/qBittorrent/qBittorrent.conf

        # Insist on tun0
        sed -i  "s/Session\\\Interface=.*/Session\\\Interface=tun0/" /config/qBittorrent/qBittorrent.conf
        sed -i  "s/Session\\\InterfaceName=.*/Session\\\InterfaceName=tun0/" /config/qBittorrent/qBittorrent.conf

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /shared
        name: shared
      securityContext: *default_securitycontext
      resources: *default_resources

nzbget:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/nzbget
    tag: 24.8@sha256:3f049d480d70b593e7567f982bb0fb962db31b4e4c01f5fafd4f9dbdd3a4aad2
  priorityClassName: tenant-bulk
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-nzbget"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: nzbget
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_500g
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-nzbget
          optional: true

  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6789
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: nzbget
      - mountPath: /tmp
        name: tmp

sabnzbd:
  enabled: false
  hostname: sabnzbd # required to prevent whitelisting requirement per https://sabnzbd.org/wiki/extra/hostname-check.html
  podLabels:
    app.elfhosted.com/class: nzb
  image:
    registry: ghcr.io
    repository: elfhosted/sabnzbd
    tag: 4.3.3@sha256:af2ef54052d0d340064997aeb76bb8e612f3b47a8a0fc5c446e821a8bacd80cc
  priorityClassName: tenant-bulk
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-sabnzbd"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: sabnzbd
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_500g
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-sabnzbd
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: sabnzbd
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # fix host_whitelist
        sed -i  's/goldilocks/{{ .Release.Name }}/g' /config/sabnzbd.ini

        # If we've previously backed up a queue, then restore it to /tmp
        files=$(shopt -s nullglob dotglob; echo /config/queue-backup/*)
        if (( ${#files} ))
        then
          cp /config/queue-backup/* /tmp/ -rfp
          rm -rf /config/queue-backup
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: sabnzbd
      - mountPath: /tmp
        name: tmp
      env: *bootstrap_env
      securityContext: *default_securitycontext
      resources: *default_resources
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1500m # if par threads is 1, this leaves 0.5cpu for downloading
      memory: 1500Mi
  additionalContainers:
    backup-queue:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |
        set -e
        IFS=$'\n' # in case of paths with spaces (looking at you, Plex!)

        function backupqueue_on_shutdown {
            echo "Received SIGTERM, waiting 5s for app to shut down..."
            mkdir -p /config/queue-backup
            sleep 5s

            # sync any files < 1MB
            cd /tmp
            find ./ -type f -size -1024k | rsync -avr --files-from=- /tmp /config/queue-backup
        }

        # When we terminate, perform the backup
        trap backupqueue_on_shutdown SIGTERM

        # Hang around doing nothing until terminated
        while true
        do
            echo "Waiting for SIGTERM to backup queue from /tmp"
            sleep infinity
        done
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: sabnzbd
      - mountPath: /tmp
        name: tmp
      env: *bootstrap_env
      securityContext: *default_securitycontext
      resources: *default_resources

  env:
    HOST_WHITELIST_ENTRIES: "{{ .Release.Name }}.sabnzbd.elfhosted.com"
    SABNZBD_UID: 568
    SABNZBD_GID: 568

tautulli:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/tautulli
    tag: 2.15.2@sha256:b84a2ec3c44fa60261869f264702f151822f1a62e4c5a5bd09822a736fd60e45
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-tautulli"
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: tautulli
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-tautulli
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8181
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: tautulli
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/sh
      - -c
      - |
        set -x
        set -e

        # Clear out logs older than 24h
        if [ -d "/config/logs" ]; then
            # Find and delete files older than 7 days
            find "/config/logs" -type f -mtime +1 -exec rm -f {} \;
            echo "Files older than 1 day have been removed from /config/logs."
        fi

        # Clear out backups older than 2d
        if [ -d "/config/backups" ]; then
            # Find and delete files older than 2 days
            find "/config/backups" -type f -mtime +2 -exec rm -f {} \;
            echo "Files older than 1 day have been removed from /config/backups."
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: tautulli
      resources: *default_resources
      securityContext: *default_securitycontext

radarr: &app_radarr
  enabled: false
  podLabels:
    app.elfhosted.com/name: radarr
    app.elfhosted.com/class: debrid
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/radarr
    tag: 5.21.1.9799@sha256:f3c0f693dc1a3504ef35ff161e92446930ab7ab9c1ff9a0222322e140ad0191b
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-radarr" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: radarr-env
  - configMapRef:
      name: elfbot-radarr
      optional: true       
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: radarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    mediacover:
      enabled: true
      type: custom
      mountPath: /config/MediaCover
      subPath: radarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: mediacovers          
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: radarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: radarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-radarr
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory          
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 7878
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: radarr
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault    
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: radarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/radarr
        mkdir -p /storage/symlinks/movies

        # for database to use 
        mkdir -p /config/postgresql/database

        # if /config/MediaCover exists (on the config volume), purge it, since this is now handled on a dedicated volume
        if [ -d /config/MediaCover ]; then
          rm -rf /config/MediaCover
        fi

      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      - mountPath: /config
        name: config
        subPath: radarr        
      resources: *default_resources
      securityContext: *default_securitycontext
        
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      env:
        - name: POSTGRES_PASSWORD
          value: radarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: radarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: radarr/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi     
    database-backup:
      image: ghcr.io/elfhosted/radarr:5.21.1.9799@sha256:f3c0f693dc1a3504ef35ff161e92446930ab7ab9c1ff9a0222322e140ad0191b
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: radarr-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: radarr/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi            
    huntarr-radarr:
      image: ghcr.io/elfhosted/huntarr-radarr:3.0@sha256:cff870f042d8487c83b747fd63d060d2d4a14b2ea9d3b56a47ccaf74c6e5d585
      envFrom:
      - configMapRef:
          name: radarr-env
      - configMapRef:
          name: elfbot-radarr
          optional: true
      volumeMounts:
      - name: tmp
        mountPath: /tmp  
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 512Mi           
  resources:
    requests:
      cpu: 0m
      memory: 500Mi
    limits:
      cpu: 1
      memory: 4Gi # reduce once sqlite-to-db-migration is done
  probes: # need a long startup for database migrations
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 300
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 7878
        timeoutSeconds: 1

radarr4k: &app_radarr4k
  enabled: false
  podLabels:
    app.elfhosted.com/name: radarr4k
    app.elfhosted.com/class: debrid
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/radarr
    tag: 5.21.1.9799@sha256:f3c0f693dc1a3504ef35ff161e92446930ab7ab9c1ff9a0222322e140ad0191b
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-radarr4k" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: radarr4k-env
  - configMapRef:
      name: elfbot-radarr4k
      optional: true       
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: radarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    mediacover:
      enabled: true
      type: custom
      mountPath: /config/MediaCover
      subPath: radarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: mediacovers            
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-radarr4k
          optional: true
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: radarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: radarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory          
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 7878
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config/ -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi
        
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: radarr4k
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault      
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: radarr4k
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/radarr4k
        mkdir -p /storage/symlinks/movies-4k

        # for database to use 
        mkdir -p /config/postgresql/database        

        # if /config/MediaCover exists (on the config volume), purge it, since this is now handled on a dedicated volume
        if [ -d /config/MediaCover ]; then
          rm -rf /config/MediaCover
        fi
        
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      - mountPath: /config
        name: config
        subPath: radarr4k        
      resources: *default_resources
      securityContext: *default_securitycontext    
  resources:
    requests:
      cpu: 0m
      memory: 500Mi
    limits:
      cpu: 1
      memory: 4Gi # reduce once sqlite-to-db-migration is done
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      env:
        - name: POSTGRES_PASSWORD
          value: radarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: radarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: radarr4k/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi   
    database-backup:
      image: ghcr.io/elfhosted/radarr:5.21.1.9799@sha256:f3c0f693dc1a3504ef35ff161e92446930ab7ab9c1ff9a0222322e140ad0191b
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: radarr4k-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: radarr4k/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi            
    huntarr-radarr:
      image: ghcr.io/elfhosted/huntarr-radarr:3.0@sha256:cff870f042d8487c83b747fd63d060d2d4a14b2ea9d3b56a47ccaf74c6e5d585
      envFrom:
      - configMapRef:
          name: radarr4k-env
      - configMapRef:
          name: elfbot-radarr4k
          optional: true
      volumeMounts:
      - name: tmp
        mountPath: /tmp  
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 512Mi  
  probes: # need a long startup for database migrations
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 300
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 7878
        timeoutSeconds: 1

ombi:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/ombi
    tag: 4.47.1@sha256:7fbede0dd51482cad6216ca02c1391beda5520a1fd9ad299ad03bd46d8552fb9
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-ombi"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: ombi
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-ombi
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5000
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: ombi
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 3m
      memory: 150Mi
    limits:
      cpu: 2
      memory: 1Gi

scannarr: &app_scannarr
  enabled: false
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/scannarr
    tag: rolling@sha256:a0d7e6f755ef8f9f3cc61d2cd7f540acd81acb39c0431e6cae91030a02a3c7ff
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-scannarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    sonarr-settings:
      enabled: "true"
      mountPath: "/app/settings_sonarr.json"
      subPath: "settings_sonarr.json"
      type: "custom"
      volumeSpec:
        configMap:
          name: scannarr-config
    radarr-settings:
      enabled: "true"
      mountPath: "/app/settings_radarr.json"
      subPath: "settings_radarr.json"
      type: "custom"
      volumeSpec:
        configMap:
          name: scannarr-config
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true
      ports:
        http:
          port: 9898 # doesn't matter this doesn,t actually use ports
  additionalContainers:
    podinfo:
      image: stefanprodan/podinfo # used to run probes from gatus
  resources: *default_resources

scannarr4k:
  <<: *app_scannarr
  enabled: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-scannarr4k"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
    sonarr-settings:
      enabled: "true"
      mountPath: "/app/settings_sonarr.json"
      subPath: "settings_sonarr.json"
      type: "custom"
      volumeSpec:
        configMap:
          name: scannarr4k-config
    radarr-settings:
      enabled: "true"
      mountPath: "/app/settings_radarr.json"
      subPath: "settings_radarr.json"
      type: "custom"
      volumeSpec:
        configMap:
          name: scannarr4k-config

bazarr:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/bazarr
    tag: 1.5.1@sha256:d0459284d4cf5abedb49f2a7637333ec6815802a79531f047c216a486e3f6686
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-bazarr"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: bazarr-config
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: bazarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-bazarr
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6767
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: bazarr
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

bazarr4k:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/bazarr
    tag: 1.5.1@sha256:d0459284d4cf5abedb49f2a7637333ec6815802a79531f047c216a486e3f6686
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-bazarr4k"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: bazarr4k-config
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: bazarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-bazarr4k
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6767
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: bazarr4k
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

filebrowser:
  hostname: elfhosted
  enabled: true
  podLabels:
    app.elfhosted.com/name: filebrowser
  image:
    repository: ghcr.io/elfhosted/filebrowser
    tag: 2.23.0@sha256:296e3a3d08c5ca07a26350358fa2e58a597a41adb253a65bba27b557f36383e5
  podAnnotations:
    kubernetes.io/egress-bandwidth: "5M" # filebrowser is not for streaming
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  envFrom:
  - configMapRef:
      name: filebrowser-env
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  deploymentStrategy:
    type: Recreate
    rollingUpdate: null
  controller:
    replicas: 1 # not sure we need 2 replicas anymore
    strategy: Recreate
    # rollingUpdate:
    #   unavailable: 1
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,filebrowser-elfbot-script,elfbot-filebrowser" # Reload the deployment every time the rclones change
  # We will use this to alter configmaps to trigger pod restarts
  serviceAccount:
    create: true
    name: filebrowser
  automountServiceAccountToken: true
  persistence:
    <<: *storagemounts  
    backup:
      enabled: true
      type: custom
      mountPath: /storage/backup
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    config:
      enabled: true
      type: custom
      mountPath: /storage/config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /storage/logs
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    elfterm-state: # so auto-provisioning doesn't break
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mouthPath: /home/elfie/.local/state
    dummy-storage: # so auto-provisioning doesn't break
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
    elfbot:
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mountPath: /elfbot
    elfbot-script:
      enabled: "true"
      mountPath: "/usr/local/bin/elfbot"
      subPath: "elfbot"
      type: "custom"
      volumeSpec:
        configMap:
          name: filebrowser-elfbot-script
          defaultMode: 0755
    elfbot-script-ucfirst:
      enabled: "true"
      mountPath: "/usr/local/bin/Elfbot" # make it easier for mobile users
      subPath: "elfbot"
      type: "custom"
      volumeSpec:
        configMap:
          name: filebrowser-elfbot-script
          defaultMode: 0755
    recyclarr-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: recyclarr-config
    symlinks: *symlinks
    tmp: *tmp
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080 # this allows us to run as non-root

  ingress:
    main:
      enabled: false
  initContainers:
    setup:
      image: ghcr.io/elfhosted/filebrowser:2.23.0@sha256:296e3a3d08c5ca07a26350358fa2e58a597a41adb253a65bba27b557f36383e5
      # 2.23.0@sha256:1db0f0114a169ea2a877d75c47903a6d01534340421948845d5e298c7ac7ceb4 is the last good version for TFA
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Delete tmp db if necessary
        if [ -f /tmp/filebrowser.db ]
        then
          rm /tmp/filebrowser.db
        fi


        /filebrowser config init \
          --disable-preview-resize \
          --disable-thumbnails \
          --disable-type-detection-by-header \
          --branding.name="{{ .Release.Name }}, by ElfHosted 🧝 " \
          --branding.files=/branding \
          --branding.disableExternal \
          --auth.method=noauth \
          --lockPassword \
          --database /tmp/filebrowser.db \
          --root /storage \
          --cache-dir /tmp

        # allow zip, unzip, rar, unrar, ls, pwd, cd, mv
        /filebrowser config set --database /tmp/filebrowser.db --commands zip,unzip,rar,unrar,ls,pwd,cd,mv,cp,ln,find,echo,grep,cat,touch,tar,gzip,rm,tree,du,mlocate,updatedb,locate,elfbot,Elfbot
        # /filebrowser config set --database /tmp/filebrowser.db --shell 'vstat -c'

        # now tell filebrowser about the user (who gets authenticated via the proxy)
        /filebrowser users add 1 bogus --database /tmp/filebrowser.db

      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /storage
        name: dummy-storage
      resources: *default_resources
      securityContext: *default_securitycontext
    copy-recyclarr-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e
        # If we don't already have an example config, create one
        if [ ! -f /config/recyclarr.yaml ];
        then
          cp /bootstrap/recyclarr.yaml /config/
        fi
      volumeMounts:
      - mountPath: /config/
        name: config
        subPath: recyclarr
      - name: recyclarr-config
        mountPath: "/bootstrap/"
      securityContext: *default_securitycontext     
  additionalContainers:
    # this container exists to watch for restarts requested by elfbot, and to use create configmaps to trigger restarts using reloader
    elfbot:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |
        # respond to creation or modification, but not deletion
        inotifywait -m -e create -e modify --format "%f" /elfbot \
          | while read APP
            do
              # if we are force-killing the pod, then don't bother with the configmap
              if (cat /elfbot/$APP | grep -q forcerestart); then
                echo "forcerestart requested, deleting $APP pod with --force.."
                kubectl delete pod -l app.kubernetes.io/name=$APP --force
                kubectl delete pod -l app.elfhosted.com/name=$APP --force
              else

                # put the contents of the file into the configmap which will trigger the restart
                echo command received for ${APP} : [$(cat /elfbot/$APP)]
                # create the configmap if it doesn't exist, since reloader only looks at _changes_ to configmaps
                if ! $(kubectl get configmap -n {{ .Release.Namespace }} elfbot-${APP} 2>&1 >/dev/null); then
                    kubectl create configmap -n {{ .Release.Namespace }} elfbot-${APP} --from-literal=elfbot_last_action=$(date +%s)
                    sleep 10s
                fi

                # If we were passed a key=value string in /etc/elfbot, then split it
                COMMAND=$(cat /elfbot/$APP)

                # We separate key and value with an '=', but sometimes the value may contain __another__ '=' (like Plex preferences)
                sep='='
                case $COMMAND in
                  # If we are separated by an =
                  (*"$sep"*)
                    KEY=${COMMAND%%"$sep"*}
                    VALUE=${COMMAND#*"$sep"}
                    ;;
                  # if not, we are a simple command like "backup"
                  (*)
                    KEY=$COMMAND
                    VALUE=$(date +%s)
                    ;;
                esac


                # patch the configmap with the latest key/value
                kubectl patch configmap -n {{ .Release.Namespace }} elfbot-${APP} -p "{\"data\":{\"${KEY}\":\"${VALUE}\"}}"
              fi
            done
      volumeMounts:
      - mountPath: /elfbot
        name: elfbot
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 0m
      memory: 6Mi
    limits:
      cpu: 1
      memory: 1Gi


uptimekuma:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/uptime-kuma
    tag: 1.23.16@sha256:e89ee95acc49f109bf136c2f15eb7aba928a97807cf22dd690cd103b6a2fef91
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-uptimekuma"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/data/
      subPath: uptimekuma
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-uptimekuba
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: uptimekuma
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 100m
      memory: 1Gi

privatebin:
  enabled: false
  image:
    repository: privatebin/fs
    tag: 1.7.6
  priorityClassName:
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-privatebin"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # crashes privatebin, TBD to determine why, and whether an emptydir /tmpfs might help
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /srv/data
      subPath: privatebin
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-privatebin
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
      ephemeral-storage: 50Mi
    limits:
      cpu: 100m
      memory: 128Mi
      ephemeral-storage: 100Mi # a safety net against node ephemeral space exhaustion
  config:
    main:
      discussion: false
      opendiscussion: false
      password: true
      fileupload: true
      burnafterreadingselected: false
      defaultformatter: "plaintext"
      syntaxhighlightingtheme: "sons-of-obsidian"
      sizelimit: 1048576
      template: "bootstrap-dark"
      info: "Hosted with ❤️ by ElfHosted 🧝"
      languageselection: true
      languagedefault: "en"
      # urlshortener: "https://shortener.example.com/api?link="
      qrcode: false
      icon: "none"
      zerobincompatibility: false
      # httpwarning: true
      compression: "zlib"
    expire:
      default: "1week"
    expire_options:
      5min: 300
      10min: 600
      1hour: 3600
      1day: 86400
      1week: 604800
    formatter_options:
      plaintext: "Plain Text"
      syntaxhighlighting: "Source Code"
      markdown: "Markdown"
    traffic:
      limit: 10
      # exemptedIp: "1.2.3.4,10.10.10/24"

kapowarr:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/kapowarr
    tag: V1.0.0@sha256:09443693816a5152c0e6beeb0afe810e8bdf048c8a641009b2d2dea50140ce1e
  priorityClassName:
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-kapowarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # breaks kapowarr
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  probes:
    liveness:
      enabled: false
    startup:
      enabled: false
    readiness:
      enabled: false
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: kapowarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    temp-downloads:
      enabled: true
      type: emptyDir
      mountPath: /app/temp_downloads
      sizeLimit: 50Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-kapowarr
          optional: true
    tmp: *tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5656
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: kapowarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/comics
        mkdir -p /storage/symlinks/comics

      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      resources: *default_resources
      securityContext: *default_securitycontext


calibreweb:
  enabled: false
  podLabels:
    app.elfhosted.com/name: calibre-web
  priorityClassName: tenant-normal  
  image:
    repository: ghcr.io/elfhosted/calibre-web-automated
    tag: v3.0.4@sha256:5b81a6f32f2947c7ed82b47afda8289e92158f9d19c9ea739fa30de98cecfa8a
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-calibre-web"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
    DOCKER_MODS: linuxserver/mods:universal-calibre
  envFrom:
  - configMapRef:
      name: elfbot-calibre-web
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: calibre-web
      volumeSpec:
        persistentVolumeClaim:
          claimName: config     
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-calibre-web
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 8083
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: calibre-web
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # There are symlinks pre-prepared for these
        mkdir -p /config/calibre-library
        mkdir -p /config/cwa-book-ingest

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: calibre-web
      resources: *default_resources
      securityContext: *default_securitycontext        
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

cwabookdownloader:
  enabled: false
  podLabels:
    app.elfhosted.com/name: cwa-book-downloader
  priorityClassName: tenant-normal  
  image:
    repository: ghcr.io/elfhosted/cwa-downloader
    tag: rolling@sha256:07a35adacd254ed89c0d5e26485e8b26c1573e450bb271f3bc9c7c8168515139
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-cwa-book-downloader"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
  envFrom:
  - configMapRef:
      name: elfbot-cwa-book-downloader
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /cwa-book-ingest/
      subPath: calibre-web/cwa-book-ingest
      volumeSpec:
        persistentVolumeClaim:
          claimName: config     
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-cwa-book-downloader
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 8084
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: calibre-web-book-downloader
      - mountPath: /tmp
        name: tmp      
  additionalContainers:
    cloudflarebypassforscraping:
      image: ghcr.io/elfhosted/cloudflarebypassforscraping@sha256:7bdf614b57f57e47a6cecdaf6048869037123e0731456ed186b5430d6bfbefad
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

lazylibrarian:
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/lazylibrarian
    tag: rolling@sha256:5095b4270cd7132983e6b6b1088a552d20051c2cf0f1f7e265c733c2af899e00
  enabled: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-lazylibrarian"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    runAsUser: 568
    runAsGroup: 568
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: lazylibrarian
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-lazylibrarian
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 5299
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: lazylibrarian
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 96Mi
    limits:
      cpu: 1
      memory: 1Gi

mylar:
  enabled: false
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/mylar3
    tag: 0.8.2@sha256:d4e8d1fdd795b25226ec1a8324385c2038aa86db9f7c16da57fed7b03884f281
  env:
    PUID: 568
    PGID: 568
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mylar" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: mylar
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-mylar
          optional: true

  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8090
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: mylar
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

komga:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/komga
    tag: 1.21.2@sha256:8aadbec3a027975cec4422d984f982fee1f5bf23096aac7c6cba33ec9f3d90d3
  env:
    KOMGA_CONFIGDIR: /config
    KOMGA_REMEMBERME_KEY: yesplease
    JAVA_TOOL_OPTIONS: -Xmx2g
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-komga" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: komga
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-komga
          optional: true

  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 25600
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: komga
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 2Gi

kavita:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/kavita
    tag: 0.8.6@sha256:6c9d98087bf2913aecef3a4b1bf34f20d21d26b934d6021073578615ec25094d
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-kavita" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /kavita/config
      subPath: kavita
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-kavita
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: kavita
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 5000
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 2
      memory: 1Gi

calibre:
  enabled: false
  # runtimeClassName: kata
  image:
    repository: quay.io/linuxserver.io/calibre
    tag: 8.2.100@sha256:0b4d127ec02999fac6c3c3fc34745510daa3d6537f3999c8ca56b6956a215879
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with s6
    allowPrivilegeEscalation: false # do we need this too?
    # runAsUser: 568
    # runAsGroup: 568
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-calibre"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: calibre
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-calibre
          optional: true
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
  env:
    PUID: 568
    PGID: 568
    TITLE: Calibre | ElfHosted
    START_DOCKER: false
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 8080
  resources:
    requests:
      cpu: 0m
      memory: 1Gi
    limits:
      cpu: 1
      memory: 4Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: calibre
      - mountPath: /tmp
        name: tmp
  envFrom:
  - configMapRef:
      name: elfbot-calibre
      optional: true

sonarr: &app_sonarr
  enabled: false
  podLabels:
    app.elfhosted.com/name: sonarr
    app.elfhosted.com/class: debrid
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/sonarr
    tag: 4.0.14.2939@sha256:4d932c78e9fa74ad7a9dd4bcea535b2bba15cd2aba232ce89365600aaadb64b0
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-sonarr" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: sonarr-env
  - configMapRef:
      name: elfbot-sonarr
      optional: true       
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: sonarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    mediacover:
      enabled: true
      type: custom
      mountPath: /config/MediaCover
      subPath: sonarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: mediacovers            
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: sonarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: sonarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-sonarr
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory
    huntarr-sonarr-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: huntarr-sonarr-config          
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8989
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: sonarr
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault    
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: sonarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/sonarr
        mkdir -p /storage/symlinks/series

        # for database to use 
        mkdir -p /config/postgresql/database        

        # if /config/MediaCover exists (on the config volume), purge it, since this is now handled on a dedicated volume
        if [ -d /config/MediaCover ]; then
          rm -rf /config/MediaCover
        fi
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      - mountPath: /config
        name: config
        subPath: sonarr
      resources: *default_resources
      securityContext: *default_securitycontext
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e
        # for now, remove old defaults
        # if [ ! -f /config/huntarr.json ];
        # then
          cp /bootstrap/huntarr.json /config/
        # fi
      volumeMounts:
      - mountPath: /config/
        name: config
        subPath: sonarr/huntarr/settings
      - name: huntarr-sonarr-config
        mountPath: "/bootstrap/"
      securityContext: *default_securitycontext        
  resources:
    requests:
      cpu: 0m
      memory: 600Mi
    limits:
      cpu: 1
      memory: 4Gi # reduce once sqlite-to-db-migration is done
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      env:
        - name: POSTGRES_PASSWORD
          value: sonarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: sonarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: sonarr/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi         
    database-backup:
      image: ghcr.io/elfhosted/sonarr:4.0.14.2939@sha256:4d932c78e9fa74ad7a9dd4bcea535b2bba15cd2aba232ce89365600aaadb64b0
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: sonarr-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: sonarr/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi            
    huntarr-sonarr:
      image: ghcr.io/elfhosted/huntarr-sonarr:3.3.3@sha256:bbdb691535dc0f2aba4f52fde8c0b8afa9a6f22b99dd731bcadd2ee18a56072b
      envFrom:
      - configMapRef:
          name: sonarr-env
      - configMapRef:
          name: elfbot-sonarr
          optional: true
      volumeMounts:
      - name: tmp
        mountPath: /tmp  
      - name: config
        mountPath: /config
        subPath: sonarr/huntarr        
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 512Mi 
  probes: # need a long startup for database migrations
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 300
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 8989
        timeoutSeconds: 1

sonarr4k: &app_sonarr4k
  enabled: false
  podLabels:
    app.elfhosted.com/name: sonarr4k
    app.elfhosted.com/class: debrid
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/sonarr
    tag: 4.0.14.2939@sha256:4d932c78e9fa74ad7a9dd4bcea535b2bba15cd2aba232ce89365600aaadb64b0
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-sonarr4k" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: sonarr4k-env
  - configMapRef:
      name: elfbot-sonarr4k
      optional: true      
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: sonarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    mediacover:
      enabled: true
      type: custom
      mountPath: /config/MediaCover
      subPath: sonarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: mediacovers            
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: sonarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: sonarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-sonarr4k
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory     
    huntarr-sonarr-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: huntarr-sonarr-config             
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8989
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: sonarr4k
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault     
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: sonarr4k
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/sonarr4k
        mkdir -p /storage/symlinks/series-4k

        # for database to use 
        mkdir -p /config/postgresql/database        
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      - mountPath: /config
        name: config
        subPath: sonarr4k     
      resources: *default_resources
      securityContext: *default_securitycontext  
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e
        # If we don't already have an example config, create one
        # if [ ! -f /config/huntarr.json ];
        # then
          cp /bootstrap/huntarr.json /config/
        # fi
      volumeMounts:
      - mountPath: /config/
        name: config
        subPath: sonarr4k/huntarr/settings
      - name: huntarr-sonarr-config
        mountPath: "/bootstrap/"
      securityContext: *default_securitycontext         
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 1
      memory: 4Gi # reduce once sqlite-to-db-migration is done
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      env:
        - name: POSTGRES_PASSWORD
          value: sonarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: sonarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: sonarr4k/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi 
    database-backup:
      image: ghcr.io/elfhosted/sonarr:4.0.14.2939@sha256:4d932c78e9fa74ad7a9dd4bcea535b2bba15cd2aba232ce89365600aaadb64b0
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: sonarr4k-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: sonarr4k/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi             
    huntarr-sonarr:
      image: ghcr.io/elfhosted/huntarr-sonarr:3.3.3@sha256:bbdb691535dc0f2aba4f52fde8c0b8afa9a6f22b99dd731bcadd2ee18a56072b
      envFrom:
      - configMapRef:
          name: sonarr4k-env
      - configMapRef:
          name: elfbot-sonarr4k
          optional: true
      volumeMounts:
      - name: tmp
        mountPath: /tmp  
      - name: config
        mountPath: /config
        subPath: sonarr4k/huntarr
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 512Mi 
  probes: # need a long startup for database migrations
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 300
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 8989
        timeoutSeconds: 1

resiliosync:
  service:
    main:
      enabled: false
  command:
  - rslsync
  - --config
  - /sync.conf
  - --nodaemon
  enabled: false
  priorityClassName: tenant-bulk
  image:
    repository: ghcr.io/elfhosted/resilio-sync
    tag: 3.0.3.1065-1@sha256:544fe39e56b2beef67a0266f155a144ddcce20d5742a71b6113c7f1e4d9b6c28
  env:
    PUID: 568
    GUID: 568
    S6_READ_ONLY_ROOT: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # another s6 containeir!
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-resiliosync"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: resiliosync
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    setup-config:
      enabled: "true"
      mountPath: "/sync.conf"
      subPath: "sync.conf"
      type: "custom"
      volumeSpec:
        configMap:
          name: resiliosync-config
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-resiliosync
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: resiliosync
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi

prowlarr: &app_prowlarr
  enabled: false
  podLabels:
    app.elfhosted.com/name: prowlarr
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/prowlarr-develop
    tag: 1.34.0.5016@sha256:3cf38fc18985e643a262fd7bfdd09fbb382791fb950094264aed921a5db6e2de
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-prowlarr,prowlarr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: prowlarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: prowlarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: prowlarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-prowlarr
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory      
  envFrom:
  - configMapRef:
      name: prowlarr-env            
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9696
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: prowlarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/sh
      - -c
      - |
        set -x
        set -e
        # Set auth to external
        sed -i  "s|<AuthenticationMethod>None</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml

        # for database to use 
        mkdir -p /config/postgresql/database

        # Clear out logs older than 24h
        if [ -d "/config/logs" ]; then
            # Find and delete files older than 7 days
            find "/config/logs" -type f -mtime +1 -exec rm -f {} \;
            echo "Files older than 1 day have been removed from /config/logs."
        fi

        # Get custom torrent.io definition
        mkdir -p /config/Definitions/Custom

        if [ -f /config/Definitions/Custom/elfhosted-torrentio.yml ]; then rm /config/Definitions/Custom/elfhosted-torrentio.yml; fi
        curl https://raw.githubusercontent.com/elfhosted/prowlarr-indexers/main/Custom/elfzilean.yml > /config/Definitions/Custom/elfzilean.yml
        curl https://raw.githubusercontent.com/elfhosted/prowlarr-indexers/main/Custom/elfcomet.yml > /config/Definitions/Custom/elfcomet.yml
        curl https://raw.githubusercontent.com/elfhosted/prowlarr-indexers/main/Custom/elfeasynews.yml > /config/Definitions/Custom/elfeasynews.yml

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: prowlarr
      resources: *default_resources
      securityContext: *default_securitycontext
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: prowlarr
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault       
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi
  env:
    S6_READ_ONLY_ROOT: 1
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      env:
        - name: POSTGRES_PASSWORD
          value: postgres
        - name: POSTGRES_DB
          value: prowlarr
        - name: POSTGRES_USER
          value: prowlarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: prowlarr/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi     

nzbhydra:
  enabled: false
  podLabels:
    app.elfhosted.com/name: nzbhydra
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/nzbhydra2
    tag: 7.12.3@sha256:14be589c72dfa1817888242dd0391b3c106b8cb34f6c1be0e39dfd96fb172ecb
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-nzbhydra"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: nzbhydra
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: nzbhydra
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-nzbhydra
          optional: true          
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5076
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: nzbhydra
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

lidarr:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/lidarr-develop
    tag: 2.11.1.4621@sha256:20a0b3b3c2366c529b584008fcd1117082ee669aa6223cb96e755155e8955d32
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-lidarr" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: lidarr-config
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: lidarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    s6:
      enabled: true
      type: emptyDir
      mountPath: /var/run/s6
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-lidarr
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8686
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: lidarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Set auth to external
        sed -i  "s|<AuthenticationMethod>None</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml
        # Clean up wasteful temporary mediacover storage (Radarr will just re-download these)

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: lidarr
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 1Gi

navidrome:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/navidrome
    tag: 0.55.2@sha256:a313681b2bd320bfcc673c267be10d15a9ca5dad9d55cea452c6f5462bc54e00
  priorityClassName: tenant-streaming
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-navidrome"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  env:
    ND_MUSICFOLDER: /tmp
    ND_DATAFOLDER: /config
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: navidrome
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-navidrome
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: navidrome
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 4533
  resources:
    requests:
      cpu: 0m
      memory: 32Mi
    limits:
      cpu: 2
      memory: 1Gi

readarr:
  enabled: false
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/readarr-develop
    tag: 0.4.14.2782@sha256:62a45730b67f0f1751180f85dee7471d410864e43e58da985fbc33a4be5a70c6
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-readarr" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: readarr-config
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: readarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: readarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    tmp-readarr-backup:
      enabled: true
      type: emptyDir
      mountPath: /tmp/readarr_backup
      sizeLimit: 32Mi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-readarr
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8787
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: readarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Set auth to external
        sed -i  "s|<AuthenticationMethod>None</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml
        sed -i  "s|<AuthenticationMethod>Basic</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: readarr
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 2
      memory: 1Gi

readarraudio:
  enabled: false
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/readarr-develop
    tag: 0.4.14.2782@sha256:62a45730b67f0f1751180f85dee7471d410864e43e58da985fbc33a4be5a70c6
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: readarraudio-config
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-readarraudio" # Reload the deployment every time the rclones change
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: readarraudio
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: readarraudio
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    tmp-readarr-backup:
      enabled: true
      type: emptyDir
      mountPath: /tmp/readarr_backup
      sizeLimit: 32Mi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-readarraudio
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8787
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: readarraudio
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Set auth to external
        # Set auth to external
        sed -i  "s|<AuthenticationMethod>None</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml
        sed -i  "s|<AuthenticationMethod>Basic</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: readarraudio
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 1Gi

plex: &app_plex
  enabled: false
  priorityClassName: tenant-streaming
  podLabels:
    app.elfhosted.com/name: plex
    app.elfhosted.com/class: debrid
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M" # tested with _kilos in Discord on a 97Mbit remux
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    runAsUser: 568
    runAsGroup: 568    
    # fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex,elfbot-imagemaid,plex-config,imagemaid-env" # Reload the deployment every time the rclones change
  image:
    registry: ghcr.io
    repository: elfhosted/plex
    tag: 1.41.6.9685-d301f511a@sha256:7df3cce637b870ec0291d998d0d094fe5c145fd61bc851c0e7cb77bfcf4a2f36
  persistence: &app_plex_persistence
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-1g    
    phototranscoder:
      enabled: true
      mountPath: /phototranscoder
      type: emptyDir
      sizeLimit: 50Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex
          optional: true
    render-device: &streamer_render_device
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    # tinyproxy-conf:
    #   enabled: "true"
    #   type: "custom"
    #   volumeSpec:
    #     configMap:
    #       name: plex-tinyproxy-conf         
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 32400
  envFrom:
  - configMapRef:
      name: plex-config
  - configMapRef:
      name: elfbot-plex
      optional: true
  resources:
    requests:
      cpu: "100m"
      memory: 1Gi
    limits:
      cpu: "2" # 1.5 works, but results in buffering when playback starts, see https://github.com/elfhosted/charts/issues/501
      memory: 4Gi
  initContainers:
    restart-with-zurg:
      image: ghcr.io/elfhosted/zurg-rc:2025.03.24.0030-nightly@sha256:72ec4f1aba38d11f7271514d1e25ad5b89df0bc85d182b0725dda3ef1dcd551f
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause plex to restart when zurg is updated"
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Clean up wasteful temporary media storage (Plex will just re-download these)
        if [ -d "/config/Library/Application Support/Plex Media Server/Cache/PhotoTranscoder" ]; then
          rm -rf "/config/Library/Application Support/Plex Media Server/Cache/PhotoTranscoder"
          ln -s /phototranscoder '/config/Library/Application Support/Plex Media Server/Cache/PhotoTranscoder'
        fi
        
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: plex
      - mountPath: /phototranscoder
        name: phototranscoder
      # can't use default resources because the ephemeral limit kicks out /phototranscoder later
      # resources: *default_resources
      securityContext: *default_securitycontext 
    setup-warp:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      env:
        CHANGE_CONTAINER_PORTS: "True"
        HTTP_PORT: "3002"
        HTTPS_PORT: "3003"
      securityContext: *speedtest_securitycontext
    # tinyproxy:
    #   image: ghcr.io/elfhosted/tinyproxy:v1.4.3@sha256:9370be1434e80f8ac7d3a24d9c19335742b2c74ba15129dcdc166bb0dd3c9098
    #   volumeMounts:
    #   - mountPath: /etc/tinyproxy/tinyproxy.conf
    #     name: tinyproxy-conf
    #     subPath: tinyproxy.conf
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "32400,3000,8888,3001,3002"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /web/index.html
          port: 32400
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /web/index.html
          port: 32400
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /web/index.html
          port: 32400
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10

plexrequests:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/wests-blackhole-script
    tag: v1.5.1@sha256:3f7a6e09de005b57fec75762422c0880286a8c4d93a9871b3df1e88866fc8dc1
  command: [ "/request.sh" ]
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plexrequests,plexrequests-env"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568 # need this so that the bootstrap can run
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: plexrequests-env
  - configMapRef:
      name: elfbot-plexrequests
      optional: true    
  persistence:
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plexrequests
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    app-cache:
      mountPath: /app/cache
      enabled: true
      type: emptyDir
      volumeSpec:
        medium: Memory                      
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 32501
  resources:
    requests:
      cpu: 0m
      memory: 10Mi
    limits:
      cpu: 0.5
      memory: 1Gi
  additionalContainers:
    auth:
      image: ghcr.io/elfhosted/wests-blackhole-script:v1.5.1@sha256:3f7a6e09de005b57fec75762422c0880286a8c4d93a9871b3df1e88866fc8dc1
      command: [ "/auth.sh" ]
      envFrom:
      - configMapRef:
          name: plexrequests-env
      - configMapRef:
          name: elfbot-plexrequests
          optional: true    
      volumeMounts:
      - name: config
        mountPath: /config
        subPath: plexrequests     
    watchlist:
      image: ghcr.io/elfhosted/wests-blackhole-script:v1.5.1@sha256:3f7a6e09de005b57fec75762422c0880286a8c4d93a9871b3df1e88866fc8dc1
      command: [ "/watchlist.sh" ]
      envFrom:
      - configMapRef:
          name: plexrequests-env
      - configMapRef:
          name: elfbot-plexrequests
          optional: true
      volumeMounts:
      - name: config
        mountPath: /config
        subPath: plexrequests             

jellyfin: &app_jellyfin
  hostname: elfhosted
  image:
    repository: ghcr.io/elfhosted/jellyfin
    tag: 10.10.7@sha256:27f605a206e149022214f282eabe08a9047ed6dac54cfd8c08a79de089e59edd
  enabled: false
  podLabels:
    app.elfhosted.com/name: jellyfin
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M" # tested with _kilos in Discord on a 97Mbit remux
  priorityClassName: tenant-streaming
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    runAsUser: 568
    runAsGroup: 568    
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jellyfin,jellyfin-env" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence: &app_jellyfin_persistence
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: "/config/log/"
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs             
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-1g  
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jellyfin
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jellyfin
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: false # necessary for probes
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
  resources:
    requests:
      cpu: "50m"
      memory: 1Gi
    limits:
      cpu: 2
      memory: 4Gi
  envFrom:
  - configMapRef:
      name: jellyfin-env
  - configMapRef:
      name: elfbot-jellyfin
      optional: true          
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      securityContext: *speedtest_securitycontext
    jellyfixer:
      image: quay.io/xsteadfastx/jellyfixer:latest
      env:
        JELLYFIXER_INTERNAL_URL: http://jellyfin:8096
        JELLYFIXER_EXTERNAL_URL: https://{{ .Release.Name }}-jellyfin.elfhosted.com
      

jellyfinranger:
  <<: *app_jellyfin
  podLabels:
    app.elfhosted.com/name: jellyfin
    app.elfhosted.com/class: dedicated
  podAnnotations:
    kubernetes.io/egress-bandwidth: "500M"
  enabled: false
  automountServiceAccountToken: false
  resources: *ranger_streamer_resources
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-jellyfin,elfbot-all"
  persistence:
    <<: *app_jellyfin_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

emby: &app_emby
  hostname: elfhosted
  image:
    registry: ghcr.io
    repository: elfhosted/emby
    tag: 4.9.0.47@sha256:ea0ca4e09a7def34110d120ff321c9b0e89018e14b2358d09fec599cc2dba6b6
  enabled: false
  priorityClassName: tenant-streaming
  podLabels:
    app.elfhosted.com/class: debrid
    app.elfhosted.com/name: emby
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M" # tested with _kilos in Discord on a 97Mbit remux
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem:
    privileged: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-emby,emby-env" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  envFrom:
  - configMapRef:
      name: emby-env
  - configMapRef:
      name: elfbot-emby
      optional: true      
  persistence: &app_emby_persistence
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: "/config/log/"
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs                
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-1g  
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-emby
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: emby
      - mountPath: /tmp
        name: tmp
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      securityContext: *speedtest_securitycontext
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: false # necessary for probes
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
  resources:
    requests:
      cpu: "50m"
      memory: 1Gi
    limits:
      cpu: 2
      memory: 4Gi

embyranger:
  <<: *app_emby
  podLabels:
    app.elfhosted.com/name: emby
    app.elfhosted.com/class: dedicated
  podAnnotations:
    kubernetes.io/egress-bandwidth: "500M"
  enabled: false
  automountServiceAccountToken: false
  resources: *ranger_streamer_resources
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-emby,elfbot-all"
  persistence:
    <<: *app_emby_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

homer:
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    runAsNonRoot: false
    runAsUser: 568
    runAsGroup: 568
  podSecurityContext:
    runAsNonRoot: false
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "Always"
  automountServiceAccountToken: false
  image:
    repository: ghcr.io/elfhosted/tooling
    tag: focal-20230605@sha256:6088a1e9fc0ce83aec9910af0899661c23b5f2025428d7da631b9b9390241b6c
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
  podLabels:
    app.elfhosted.com/role: nodefinder # let this be an anchor for replicationdestinations
  persistence:
    <<: *storagemounts
    logs:
      enabled: true
      type: custom
      mountPath: /logs
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    config:
      enabled: true
      type: custom
      mountPath: /config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config          
    backup:
      enabled: true
      type: custom
      mountPath: /backup
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    config-yml:
      enabled: "true"
      subPath: "config.yml"
      type: "custom"
      volumeSpec:
        configMap:
          name: homer-config
    custom-css:
      enabled: "true"
      subPath: "custom-css"
      type: "custom"
      volumeSpec:
        configMap:
          name: homer-config
    gatus-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: gatus-config
    disk-usage:
      enabled: "true"
      mountPath: "/usr/local/bin/disk_usage.sh"
      subPath: "disk_usage.sh"
      type: "custom"
      volumeSpec:
        configMap:
          name: homer-config
    message:
      enabled: true
      type: emptyDir
      mountPath: /www/assets/message
  command:
  - /bin/bash
  - /usr/local/bin/disk_usage.sh
  additionalContainers:
    ui:
      image: ghcr.io/elfhosted/homer:v25.04.1@sha256:7d3cb724bcd4c082fffb46c8659355759d6630abbd8c9e7f07db3fb27aafd81d
      imagePullPolicy: IfNotPresent
      volumeMounts:
      - mountPath: /www/assets/config.yml
        name: config-yml
        subPath: "config.yml"
      - mountPath: /www/assets/custom.css
        name: custom-css
        subPath: "custom.css"
      - mountPath: /www/assets/message
        name: message
      - mountPath: /www/assets/backgrounds
        name: config
        subPath: homer/backgrounds
        readOnly: true
      resources: *default_resources
      securityContext: *default_securitycontext
  configmap:
    config:
      # -- Store homer configuration as a ConfigMap, but don't specify any config, since we'll supply our own
      enabled: false
  controller:
    replicas: 1
    strategy: RollingUpdate
    rollingUpdate:
      unavailable: 1
    annotations:
      configmap.reloader.stakater.com/reload: "homer-config, elfbot-homer" # Reload the deployment every time the yaml config changes
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 200m
      memory: 1Gi

traefikforwardauth:
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  whitelist: admin@elfhosted.com
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  controller:
    replicas: 1
    annotations:
      configmap.reloader.stakater.com/reload: traefik-forward-auth-config
    strategy: RollingUpdate
  image:
    repository: ghcr.io/elfhosted/traefik-forward-auth
    pullPolicy: IfNotPresent
    tag: 3.1.0@sha256:19cd990fae90c544100676bc049f944becc8c454639e57d20f6f48e27de90776

  middleware:
    # middleware.enabled -- Enable to deploy a preconfigured middleware
    enabled: false

  envFrom:
  - configMapRef:
      name: traefik-forward-auth-config

  ingress:
    main:
      enabled: false

  service:
    main:
      enabled: true # necessary for probes

  resources:
    requests:
      cpu: 0m
      memory: 6Mi
    limits:
      cpu: 1
      memory: 32Mi

gatus:
  image:
    repository: ghcr.io/elfhosted/gatus
    tag: 5.17.0@sha256:da063af4d683d3ef7c92c0122adf1c404351d3d238de03d2c4f58261c2416538
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 20Mi
    limits:
      cpu: 1
      memory: 128Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  controller:
    # strategy: RollingUpdate
    annotations:
      configmap.reloader.stakater.com/reload: "gatus-config"
  env:
    GATUS_CONFIG_PATH: /config/config.yaml
    SMTP_FROM: 'health@elfhosted.com'
    SMTP_PORT: 587
  persistence:
    gatus-config:
      enabled: "true"
      mountPath: /config
      type: "custom"
      volumeSpec:
        configMap:
          name: gatus-config
    config:
      enabled: true
      type: custom
      mountPath: /data/
      subPath: gatus
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  envFrom:
  - secretRef:
      name: gatus-smtp-config
  configmap:
    config:
      # -- Store homer configuration as a ConfigMap, but don't specify any config, since we'll supply our own
      enabled: false

gotify:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/gotify
    tag: 2.5.0@sha256:f5c89bb3ccbf857bca816e4550b46c442cfb6c0eae0f081975ba5c5099779c3f
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-gotify"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  env:
    GOTIFY_SERVER_PORT: 8080
  resources:
    requests:
      cpu: 0m
      memory: 32Mi
    limits:
      cpu: 1
      memory: 64Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/data/
      subPath: gotify
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-gotify
          optional: true
    tmp: *tmp # Avoids issues with readOnlyRootFilesystem
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: gotify
      - mountPath: /tmp
        name: tmp

flaresolverr: &app_flaresolverr
  enabled: false
  podLabels:
    app.elfhosted.com/name: flaresolverr
  image:
    registry: ghcr.io
    repository: elfhosted/flaresolverr
    tag: 3.3.21@sha256:2c3c7087eaf809f2b032fb9df28cf8884546463a1c532c9dd3b244f424bfd6ad
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # makes node unhappy
  affinity:
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.elfhosted.com/name
              operator: In
              values:
              - zurg
          topologyKey: "kubernetes.io/hostname"
      - weight: 50
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - kubernetesdashboard
          topologyKey: "kubernetes.io/hostname"
  tolerations:
  - key: node-role.elfhosted.com/dedicated
    operator: Exists
  - key: node-role.elfhosted.com/hobbit
    operator: Exists
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-flaresolverr"
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-flaresolverr
          optional: true
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 1000
    runAsGroup: 1000
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 600m
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8191
  # env:
  #   WAIT_FOR_VPN: "true"
  #   LOG_LEVEL: debug
    # DRIVER: nodriver
  initContainers:      
    setup-warp:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false    
  addons:
    vpn:
      enabled: false # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "8191"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared

seafile:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/seafile
    tag: 10.0.1@sha256:58e423f05ed7a3ff84cb8a7cf1e5570a0c8b34218c348cbb28f5cff6210ddbc7
  priorityClassName: tenant-bulk
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't seem to work with seafile, no output from container either
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 568 # has to run as root, see https://github.com/haiwen/seafile-docker/issues/86
    # runAsGroup: 568
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-seafile"
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1024m
      memory: 512Mi
  env:
    # -- Set the container timezone
    TIME_ZONE: Etc/UTC
    # -- The hostname of your database
    DB_HOST: "{{ .Release.Name }}-seafile-mysql"
    # -- The root password for mysql (used for initial setup)
    DB_ROOT_PASSWD: wLu5UUuT@3Zu33eT
    # -- The initial admin user's password
    SEAFILE_ADMIN_PASSWORD: changeme
    # -- The hostname for the server (set to your ingress hostname)
    SEAFILE_SERVER_HOSTNAME: "{{ .Release.Name }}-seafile.elfhosted.com"
    SEAFILE_SERVER_LETSENCRYPT: false
    FORCE_HTTPS_IN_CONF: true
    NON_ROOT: true # yes, and with our custom image, this runs the seafile/seahub components as user 568
  envFrom:
  - configMapRef:
      name: seafile-config
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # debug whether this gets us probes
  memcached:
    nameOverride: seafile-memcached
    enabled: true
  mysql:
    nameOverride: seafile-mysql
    enabled: true
    architecture: standalone
    commonAnnotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-seafile"

    auth:
      rootPassword: "wLu5UUuT@3Zu33eT"
      database: "seafile"
      username: "seafile"
      password: "nXCXSmqU4TMk3okD"

    primary:
      readinessProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      livenessProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      startupProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      persistence:
        enabled: true
        existingClaim: config
        subPath: seafile/database
      resources:
        requests:
          cpu: 5m
          memory: 1Gi
        limits:
          cpu: 2
          memory: 1024Mi
      containerSecurityContext:
        enabled: true
        seccompProfile:
          type: RuntimeDefault
        runAsUser: 568
        runAsGroup: 568
      podSecurityContext:
        enabled: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
      extraVolumeMounts:
      - mountPath: /opt/bitnami/mysql/tmp/
        name: tmp
      extraVolumes:
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi
      - name: backup-database-script
        configMap:
          name: seafile-backup

  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /shared/seafile
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
      subPath: seafile/data
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-seafile
          optional: true

tunarr:
  enabled: false
  image:
    registry: ghcr.io
    repository: chrisbenincasa/tunarr
    tag: 0.18.10-vaapi
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    privileged: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-tunarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podAnnotations:
    kubernetes.io/egress-bandwidth: "128M"  
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 1
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /root/.local
      subPath: tunarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-tunarr
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    media: # in case users use /tmp
      enabled: true
      mountPath: /streams
      type: emptyDir
      sizeLimit: 50Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: tunarr
      - mountPath: /tmp
        name: tmp

ersatztv:
  enabled: false
  image:
    registry: docker.io
    repository: jasongdove/ersatztv
    tag: develop-vaapi@sha256:c5ff3c8e093bff18a185788c1c95f6d42dd6b5c0b70f1835d820f381a56c08f4
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    privileged: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-erzatztv"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 1
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8409
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /root/.local/share/ersatztv
      subPath: ersatztv
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    transcode:
      enabled: true
      type: custom
      mountPath: /root/.local/share/etv-transcode
      volumeSpec: *volumespec_ephemeral_volume_50g          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-tunarr
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: ersatztv
      - mountPath: /tmp
        name: tmp

threadfin:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/threadfin
    tag: 1.2.32@sha256:b731bc09fc82c70d98173d6224ae4a544c207999e818bd06cbdc519f3464dcc8
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-threadfin"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 1
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 34400
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /home/threadfin/conf/
      subPath: threadfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-tunarr
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: threadfin
      - mountPath: /tmp
        name: tmp

thelounge:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/thelounge
    tag: "4.4.3@sha256:74ae8d9fc36d5a8396bb70cfaa58d222730fa69d2f71c3e2ec3ae010f2a0b264"
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-thelounge"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because the node modules in /app try to create files
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  env:
    TZ: UTC
    THELOUNGE_HOME: /config/thelounge # avoids attempts to chown /config
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 40Mi
    limits:
      cpu: 100m
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9000
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: thelounge
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-thelounge
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: thelounge
      - mountPath: /tmp
        name: tmp
    create-user:
      image: ghcr.io/elfhosted/thelounge:4.4.3@sha256:74ae8d9fc36d5a8396bb70cfaa58d222730fa69d2f71c3e2ec3ae010f2a0b264
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e


        # If we don't already have a config, create one
        if [ ! -f /config/thelounge/config.json ];
        then
          mkdir -p /config/thelounge
          cp /config-bootstrap/* /config/thelounge/ -R
        fi

        # If we don't already have a user, create one
        if [ ! -f /config/thelounge/users/${USERNAME}.json ];
        then
          thelounge add ${USERNAME} --password ${PASSWORD}
        fi
      volumeMounts:
      - mountPath: /config
        subPath: thelounge
        name: config
      env:
      - name: THELOUNGE_HOME
        value: /config/thelounge # avoids attempts to chown /config
      - name: USERNAME
        valueFrom:
          configMapKeyRef:
            name: elfhosted-user-config
            key: USERNAME
      - name: PASSWORD
        value: ireadthedocs
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true

symlinkcleaner:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/symlink-cleaner
    tag: v1.4.7@sha256:2b980b95269aa534c46bc735a613efe1934af948d602e281958ac2891006acb6
  imagePullSecrets:
  - name: ghcr-io-elfhosted
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-symlink-cleaner,symlink-cleaner-example-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because the node modules in /app try to create files
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 40Mi
    limits:
      cpu: 100m
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5000
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 5000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 5000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 45
        httpGet:
          path: /health
          port: 5000
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 5
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/config
      subPath: symlink-cleaner
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /app/logs
      subPath: symlink-cleaner
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-symlink-cleaner
          optional: true          
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: symlink-cleaner-example-config          
  envFrom:
  - configMapRef:
      name: symlink-cleaner-env
  - configMapRef:
      name: elfbot-symlink-cleaner
      optional: true    
  initContainers:  
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: symlink-cleaner
      - mountPath: /tmp
        name: tmp      
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e
        # If we don't already have an example config, create one
        if [ ! -f /config/config.json ];
        then
          cp /bootstrap/config.json /config/
        fi
      volumeMounts:
      - mountPath: /config/
        name: config
        subPath: symlink-cleaner
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext: *default_securitycontext  

overseerr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/overseerr
    tag: 1.34.0@sha256:25915ea8bf16d0506e3c93f7ffdc7f2778ec6d7bd113fd69d683489617984519
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: overseerr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-overseerr,overseerr-config"
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: overseerr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-overseerr
          optional: true
    tmp: *tmp
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
    overseerr-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: overseerr-config
          optional: true          
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: overseerr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        # run the setup script from the configmap, so that we can make templated changes
        bash /bootstrap/setup.sh
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: overseerr
      - name: overseerr-config
        mountPath: "/bootstrap/"
    # We do this so that we can override the /app/jellyseer/public path with our own, allowing the user to customize the branding
    copy-branding:
      image: ghcr.io/elfhosted/overseerr:1.34.0@sha256:25915ea8bf16d0506e3c93f7ffdc7f2778ec6d7bd113fd69d683489617984519
      command:
        - /bin/bash
        - -c
        - |
          mkdir -p /config/branding
          cp --no-clobber -rf /app/overseerr/public/logo_* /config/branding
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: overseerr
      resources: *default_resources
      securityContext: *default_securitycontext
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5055
  resources:
    requests:
      cpu: 0m
      memory: 175Mi
    limits:
      cpu: 2
      memory: 1Gi
  additionalContainers:
    branding:
      image: nginxinc/nginx-unprivileged
      volumeMounts:
      - mountPath: /usr/share/nginx/html
        name: config
        subPath: overseerr/branding
        readOnly: true
      - mountPath: /tmp
        name: tmp
      resources: *default_resources
      securityContext: *default_securitycontext

jellyseerr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/jellyseerr
    tag: 2.5.2@sha256:f8bc515864f06a4eec9dfa09d1d63a41c687e2839654da35f6a96815717a9187
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: jellyseerr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jellyseerr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: jellyseerr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jellyseerr
          optional: true
    tmp: *tmp
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
    jellyseerr-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: jellyseerr-config
          optional: true              
  envFrom:
  - configMapRef:
      name: jellyseerr-env
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jellyseerr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        # run the setup script from the configmap, so that we can make templated changes
        bash /bootstrap/setup.sh
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: jellyseerr
      - name: jellyseerr-config
        mountPath: "/bootstrap/"        
    # We do this so that we can override the /app/jellyseer/public path with our own, allowing the user to customize the branding
    copy-branding:
      image: ghcr.io/elfhosted/jellyseerr:2.5.2@sha256:f8bc515864f06a4eec9dfa09d1d63a41c687e2839654da35f6a96815717a9187
      command:
        - /bin/ash
        - -c
        - |
          mkdir -p /config/branding
          cp --no-clobber -rf /app/public/logo_* /config/branding/
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: jellyseerr
      resources: *default_resources
      securityContext: *default_securitycontext
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5055
  resources:
    requests:
      cpu: 0m
      memory: 160Mi
    limits:
      cpu: 2
      memory: 1Gi
  additionalContainers:
    branding:
      image: nginxinc/nginx-unprivileged
      volumeMounts:
      - mountPath: /usr/share/nginx/html
        name: config
        subPath: jellyseerr/branding
        readOnly: true
      - mountPath: /tmp
        name: tmp
      resources: *default_resources
      securityContext: *default_securitycontext

jellystat:
  enabled: false
  podLabels:
    app.elfhosted.com/name: jellystat
  image:
    repository: ghcr.io/elfhosted/jellystat
    tag: 1.1.3@sha256:b6dfb29c5e86eff839ebd7a10e8dd387e3fa9b3b739696efeb63ad63b5791238
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jellystat,jellystat-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with ilikedanger currently
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: jellystat-env
  - configMapRef:
      name: elfbot-jellystat
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 20Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: jellystat/config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jellystat
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jellystat
      - mountPath: /tmp
        name: tmp
    setup-postgres:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/database
        chown elfie:elfie /config/database -R

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: jellystat
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault    
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      env:
        - name: POSTGRES_PASSWORD
          value: jellystat
        - name: POSTGRES_DB
          value: jellystat
        - name: POSTGRES_USER
          value: jellystat
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: jellystat/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 1Gi

audiobookshelf:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/audiobookshelf
    tag: v2.20.0@sha256:194e33b60c46c0b1b480aa20b4666dade9425ffd7d390dd7a7d5b18c740475aa
  priorityClassName: tenant-streaming
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: audiobookshelf
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-audiobookshelf,elfbot-all"
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: audiobookshelf
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    # never used, just satisfies startup scripts
    metadata:
      enabled: true
      type: emptyDir
      mountPath: /metadata
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-audiobookshelf
          optional: true
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  env:
    METADATA_PATH: /config/metadata
    SOURCE: ElfHosted
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: audiobookshelf
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 2
      memory: 1Gi

audiobookrequest:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/audiobookrequest
    tag: 1.4.5@sha256:f3a34c0899b7873b5af2057bdf1f2711859bfa4c7f445b7aeebd06cdff4982ab
  priorityClassName: tenant-streaming
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-audiobookrequest,elfbot-all"
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: audiobookrequest
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-audiobookrequest
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: audiobookrequest
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 2
      memory: 1Gi

audiobookbayautomated:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/audiobookbay-automated
    tag: rolling@sha256:3b25673582df696f2ee1fcc4d917fa83a8a714f714c1bb5253a70a8a5d284877
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-audiobookbay-automated"
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: audiobookbay-automated
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-audiobookbay-automated
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: audiobookbay-automated
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5078
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 2
      memory: 1Gi      
  envFrom:
  - configMapRef:
      name: audiobookbay-automated-env      

openbooks:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/openbooks
    tag: 4.5.0@sha256:c17d8e86d55e35cd1edb20fe1e1ff95a8e93cbef4c571ead2bfeb07276845477
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  persistence:
    <<: *storagemounts
  command:
  - /bin/bash
  - -c
  - |
    set -x
    set -e
    sleep 5s
    USER=$(tr -dc A-Za-z0-9 </dev/urandom | head -c 13 ; echo '')
    ./openbooks server \
      --dir ${DATA_DIR-/tmp} \
      --port 8000 \
      --name $USER \
      --tls=false \
      --persist \
      --server irc.irchighway.net:6661 \
      --no-browser-downloads \
      --debug
  envFrom:
  - configMapRef:
      name: elfbot-openbooks
      optional: true
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-openbooks"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 2
      memory: 1Gi

vaultwarden:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/vaultwarden
    tag: 1.33.2@sha256:0340579937286ad521db19505c052fc1c6ce83948be43beb0e5094f2444b10c2
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-vaultwarden"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: elfbot-vaultwarden
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: vaultwarden
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-vaultwarden
          optional: true
    tmp: *tmp
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: vaultwarden
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 1
      memory: 1Gi


notifiarr:
  enabled: false
  hostname: elfhosted
  image:
    repository: ghcr.io/elfhosted/notifiarr
    tag: 0.8.3@sha256:96229b77c2ddca79d3b90fabca65dc240bce6cdb13951ed97e48c8f463af486c
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because the node modules in /app try to create files
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-notifiarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 2
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5454
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: notifiarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: notifiarr-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-notifiarr
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: notifiarr
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [ ! -f /config/notifiarr.conf ];
        then
          cp /bootstrap/notifiarr.conf /config/
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: notifiarr
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true

shoko:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/shokoserver
    tag: v5.1.0@sha256:a386f39bcfd7c1e8f9c38b4171c8e59162e3e1b2c579728465fd4ea52978a69e
  env:
    PUID: 568
    PGID: 568
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-shoko"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # again, s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /home/shoko/.shoko/
      subPath: shoko
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-shoko
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: shoko
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8111
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi

filebot:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/filebot-node
    tag: 0.4.8@sha256:06125d533c15f7a36976976d9a067d691c263383286f0a2dcd3753cd92af6689
  env:
    PUID: 568
    PGID: 568
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-filebot"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # again, s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: filebot
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-filebot
          optional: true

    tmp: # to avoid errors about storing java prefs
      enabled: true
      type: emptyDir
      mountPath: /home/seedy
      sizeLimit: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: filebot
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5452
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 2
      memory: 1Gi

rpdb:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rpdb
    tag: 0.3.3@sha256:b87f9253ca86e184e83db6d67fcc12e9c06b05ab1c75c9aa06faee69e45540a6
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,storage-changed,elfbot-rpdb"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /.config
      subPath: rpdb
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rpdb
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rpdb
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8750
  resources:
    requests:
      cpu: 0m
      memory: 40Mi
    limits:
      cpu: 1
      memory: 1Gi

kometa: &app_kometa
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/kometa
    tag: v2.2.0@sha256:071dfb0cf5a7d7aa88d1bed63a6010cb4c65d58d3ad6e22e8170216030849782
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: kometa
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-kometa"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: elfbot-kometa
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: kometa
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs/
      subPath: kometa
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-kometa
          optional: true
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: kometa-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: kometa
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [ ! -f /config/config.yml ];
        then
          cp /bootstrap/config.yml /config/
        fi

        # Create directories we need by default
        mkdir -p /config/kometa/assets
        mkdir -p /config/kometa/logs
        mkdir -p /config/kometa/metadata
        mkdir -p /config/kometa/missing
        mkdir -p /config/kometa/overlays

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: kometa
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 4Gi

imagemaid:
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/imagemaid
    tag: v1.1.1@sha256:574796393e4366e19b602f530d61d22835b15d3367253c9225587a692e3e060b
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: imagemaid
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-imagemaid"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: imagemaid-env
  - configMapRef:
      name: elfbot-imagemaid
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-imagemaid
          optional: true
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: kometa-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: imagemaid
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 4Gi  

cinesync:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/cinesync
    tag: v2.4.1@sha256:bf18a4f162dff4b43846efff00cd50877d8ac070185d8e56e9542d69516ae490
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: cinesync
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-cinesync,cinesync-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: cinesync-env
  - secretRef:
      name: cinesync-env      
  - configMapRef:
      name: elfbot-cinesync
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/db
      subPath: cinesync
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-cinesync
          optional: true  
    logs:
      enabled: true
      type: custom
      mountPath: /app/logs
      subPath: cinesync
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs              
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: cinesync
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x

        # Create directory structure if it doesn't exist yet
        mkdir -p /storage/symlinks/movies
        mkdir -p /storage/symlinks/movies-4k
        mkdir -p /storage/symlinks/movies-anime
        mkdir -p /storage/symlinks/series
        mkdir -p /storage/symlinks/series-4k
        mkdir -p /storage/symlinks/series-anime
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      resources: *default_resources
      securityContext: *default_securitycontext        
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 2Gi 

arrtools:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/arrtools
    tag: rolling@sha256:db962b3d011000a239a3878c8fc99116a507fdf54fd5d1b1f134cfdb6e8b6374
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  podLabels:
    app.elfhosted.com/name: arrtools
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-arrtools"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: arrtools
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: arrtools-config          
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 2Gi 
  initContainers:
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [[ ! -f /config/config.ini ]];
        then
          cp /bootstrap/config.ini /config/
        fi
        if [[ ! -f /config/config4k.ini ]];
        then
          cp /bootstrap/config4k.ini /config/
        fi        
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: arrtools
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true      

seerrbridge:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/seerrbridge
    tag: v0.5.0@sha256:ae8a04e503d44d36d5aa446313330e6f4e43c3721b8c932fffe6a71632786e5b
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # needs to write to /app/seerrbridge.log, apparently
  podLabels:
    app.elfhosted.com/name: seerrbridge
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-seerrbridge,seerrbridge-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
    backup: *backup
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory      
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-seerrbridge
          optional: true                
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: seerrbridge
      volumeSpec:
        persistentVolumeClaim:
          claimName: config  
    logs:
      enabled: true
      type: custom
      mountPath: /logs
      subPath: seerrbridge
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs                           
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: seerrbridge-env
  - secretRef:
      name: seerrbridge-env      
  - configMapRef:
      name: elfbot-seerrbridge
      optional: true
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 2Gi  
  initContainers:
    setup-warp:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false   
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: seerrbridge
      - mountPath: /tmp
        name: tmp          
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "3001,8777"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared               

listsync:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/listsync
    tag: 0.5.7@sha256:f6876ea2add6816bb401047a4306b902f3140e557bffe6154dfcecbf54932b7a
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # needs to create its own .venv
  podLabels:
    app.elfhosted.com/name: listsync
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-listsync,listsync-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    backup: *backup
    tmp: *tmp     
    config:
      enabled: true
      type: custom
      mountPath: /usr/src/app/data
      subPath: listsync
      volumeSpec:
        persistentVolumeClaim:
          claimName: config  
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-listsync
          optional: true    
    logs:
      enabled: true
      type: custom
      mountPath: /logs
      subPath: listsync
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs                         
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: listsync-env
  - configMapRef:
      name: elfbot-listsync
      optional: true
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 2Gi  
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: debridav
      - mountPath: /tmp
        name: tmp  

plextraktsync:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/plextraktsync
    tag: 0.34.7@sha256:61c00dd218df3a895c93e728a5e60a815a7d7ce81fb6a554bb4232360abc71cb
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: plextraktsync
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plextraktsync"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: elfbot-plextraktsync
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /home/elfie/.config/PlexTraktSync
      subPath: plextraktsync
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-kometa
          optional: true
    state: # plextraktsync needs this
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mountPath: /home/elfie/.local/state
    cache: # plextraktsync needs this
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mountPath: /home/elfie/.cache      
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plextraktsync
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 100m # no way this should be using so much resources
      memory: 1Gi

decluttarr: &app_decluttarr
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/decluttarr
    tag: rolling@sha256:84fe3188a12fa6f64380690e24662459aa622339b2b760a9a18673642014a8cf
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: decluttarr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-decluttarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: elfbot-decluttarr
      optional: true
  - configMapRef:
      name: zenv-decluttarr
      optional: true      
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi

rdebridui:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rdebrid-ui
    tag: rolling@sha256:2c9885f1918097d8b762e0e60fcee5e12f147c10d79f13decbe4f3b73d24e4ad
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: rdebrid-ui
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rdebrid-ui"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  envFrom:
  - configMapRef:
      name: elfbot-rdebrid-ui
      optional: true  
  - configMapRef:
      name: zenv-rdebrid-ui
  - secretRef:
      name: zenv-rdebrid-ui      
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi

suggestarr:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/suggestarr
    tag: v1.0.20@sha256:f567f7416f2e2092d54e68037e0886908ce583126281c9673c92bde0969bf1c5
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with app :(
  podLabels:
    app.elfhosted.com/name: suggestarr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-suggestarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  env:
    TZ: UTC
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /app/config/
      subPath: suggestarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /var/log
      subPath: suggestarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-suggestarr
          optional: true          
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5000
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: suggestarr
      - mountPath: /tmp
        name: tmp      

decluttarr4k: 
  <<: *app_decluttarr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-decluttarr4k"  
  envFrom:
  - configMapRef:
      name: elfbot-decluttarr4k
      optional: true
  - configMapRef:
      name: zenv-decluttarr4k
      optional: true      


rcloneui:
  enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.69.1@sha256:c89e34f0e1032d7ab7f4a927065dfb71856d37ec584bbe4750075f157f9de8ba
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rcloneui,elfhosted-user-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    privileged: true
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    backup: *backup
    cache: 
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      # volumeSpec: *volumespec_ephemeral_volume_10g    
    mount:
      enabled: true
      type: emptyDir
      mountPath: /mount
      sizeLimit: 1Gi 
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: rclone
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    rclone-remote-storage:
      enabled: "true"
      subPath: "rclone-remote-storage"
      type: "custom"
      volumeSpec:
        configMap:
          name: rclonefm-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rclonebrowser
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rclone
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # add local remote if it doesn't exist
        grep -q '/storage' /config/rclone.conf || cat /rclone-remote-storage >> /config/rclone.conf

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: rclone
      - mountPath: /tmp # need this for cating into a file
        name: tmp
      - mountPath: /rclone-remote-storage
        subPath: rclone-remote-storage
        name: rclone-remote-storage
      resources: *default_resources
      securityContext: *default_securitycontext
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5572
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 1
      memory: 512Mi

rclonefm:
  enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.69.1@sha256:c89e34f0e1032d7ab7f4a927065dfb71856d37ec584bbe4750075f157f9de8ba
  command:
  - /rclonefm.sh
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rclonefm,rclonefm-config"
  podAnnotations:
    kubernetes.io/egress-bandwidth: "40M"
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    cache:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      sizeLimit: 1Gi
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: rclone
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    rclonefm-config:
      enabled: "true"
      mountPath: /var/lib/rclonefm/js/settings.js
      subPath: "settings.js"
      type: "custom"
      volumeSpec:
        configMap:
          name: rclonefm-config
    rclone-remote-storage:
      enabled: "true"
      subPath: "rclone-remote-storage"
      type: "custom"
      volumeSpec:
        configMap:
          name: rclonefm-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rclonefm
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rclone
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # add local remote if it doesn't exist
        grep -q '/storage' /config/rclone.conf || cat /rclone-remote-storage >> /config/rclone.conf

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: rclone
      - mountPath: /tmp # need this for cating into a file
        name: tmp
      - mountPath: /rclone-remote-storage
        subPath: rclone-remote-storage
        name: rclone-remote-storage
      resources: *default_resources
      securityContext: *default_securitycontext
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5573
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi

webdav: &webdav
  enabled:
    false
  podLabels:
    app.elfhosted.com/name: webdav
  podAnnotations:
    kubernetes.io/egress-bandwidth: "40M"
  priorityClassName: tenant-normal
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.69.1@sha256:c89e34f0e1032d7ab7f4a927065dfb71856d37ec584bbe4750075f157f9de8ba
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-webdav-plus,elfbot-webdav"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  command:
  - /webdav.sh
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: webdav-config
  - configMapRef:
      name: elfbot-webdav
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /storage/config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-webdav-plus
          optional: true
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  service:
    main:
      enabled: false # necessary for probes
      ports:
        http:
          port: 5574
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi

storagehub:
  enabled: false # down for now
  podLabels:
    app.elfhosted.com/name: storagehub
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "storagehub-scripts,storagehub-env"
      secret.reloader.stakater.com/reload: ",storagehub-config,storagehub-env"
  # affinity:
  #   podAffinity:
  #     # prefer to be located with zurg, if tolerations permit
  #     preferredDuringSchedulingIgnoredDuringExecution:
  #     - weight: 100
  #       podAffinityTerm:
  #         labelSelector:
  #           matchExpressions:
  #           - key: app.elfhosted.com/name
  #             operator: In
  #             values:
  #             - zurg
  #         topologyKey: "kubernetes.io/hostname"
  # tolerations:
  # - key: node-role.elfhosted.com/download-only
  #   operator: Exists
  # - key: node-role.elfhosted.com/dedicated
  #   operator: Exists
  priorityClassName: tenant-critical
  image:
    repository: itsthenetwork/nfs-server-alpine
    tag: latest
  env:
    SHARED_DIRECTORY: /export
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    privileged: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    # # this is an ephemeral volume
    # storage:
    #   enabled: true
    #   type: custom
    #   mountPath: /storage
    #   volumeSpec: *volumespec_ephemeral_volume_50g
    # these are the persistent volumes we support currently
    # needed for the migration

    rclonemountrealdebridzurg: *rclonemountrealdebridzurg
    # this is the old symlinks on HDD
    # symlinks: *symlinks # these only get mounted on storagehub. Everything else accesses symlinks **through** storagehub
    # samba config
    # config:
    #   enabled: "true"
    #   subPath: "container.json"
    #   mountPath: /etc/samba/container.json
    #   type: "custom"
    #   volumeSpec:
    #     secret:
    #       secretName: storagehub-config
    # storagehub-scripts:
    #   enabled: "true"
    #   type: "custom"
    #   volumeSpec:
    #     configMap:
    #       name: storagehub-scripts
    #       defaultMode: 0755
    tmp:
      enabled: true
      type: emptyDir
      mountPath: /tmp
      sizeLimit: 1Gi
  service:
    main:
      enabled: false # necessary for probes
      ports:
        http:
          port: 2049
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 10Mi
    limits:
      cpu: 1
      memory: 512Mi
  # envFrom:
  # - configMapRef:
  #     name: storagehub-env
  # - secretRef:
  #     name: storagehub-env
  # initContainers:
    # setup:
    #   image: *tooling_image
    #   imagePullPolicy: IfNotPresent
    #   command:
    #   - /bin/bash
    #   - -c
    #   - |
    #     # don't error on failure
    #     # not doing this yet
    #     # /storagehub-scripts/restore.sh

    #     echo nothing
    #     # if [[ ! -f /storage/symlinks/i-was-migrated-to-storagehub ]]
    #     # then
    #     #   if [[ ! -z "$(ls -A /migration)" ]]
    #     #   then
    #     #     echo "Tar-migrating from /migration/..."
    #     #     tar -cf - -C /migration/ . | tar xvmf - -C /storage/symlinks/
    #     #     touch /storage/symlinks/i-was-migrated-to-storagehub
    #     #   fi
    #     # fi

    #   envFrom:
    #   - configMapRef:
    #       name: storagehub-env
    #   - configMapRef:
    #       name: elfhosted-user-config
    #   - secretRef:
    #       name: storagehub-env
    #   volumeMounts:
    #   - mountPath: /storagehub-scripts
    #     name: storagehub-scripts
    #   - mountPath: /home/elfie
    #     name: tmp
    #   - mountPath: /migration
    #     name: migration
    #   - mountPath: /storage/symlinks
    #     name: symlinks
    #   resources: *default_resources
    #   securityContext: *default_securitycontext
  # additionalContainers:
  #   backup-on-termination:
  #     image: *tooling_image
  #     command:
  #     - /usr/bin/dumb-init
  #     - /bin/bash
  #     - -c
  #     - /storagehub-scripts/backup.sh
  #     envFrom:
  #     - configMapRef:
  #         name: storagehub-env
  #     - configMapRef:
  #         name: elfhosted-user-config
  #     - secretRef:
  #         name: storagehub-env
  #     volumeMounts:
  #     - mountPath: /storagehub-scripts
  #       name: storagehub-scripts
  #     - mountPath: /home/elfie
  #       name: tmp
  #     - mountPath: /ephemeral
  #       name: ephemeral
  #     # need a folder here for each app. what a pita
  #     - mountPath: /persistent/plex
  #       name: persistent-plex
  #     - mountPath: /symlinks
  #       name: symlinks
  # terminationGracePeriodSeconds: "3600" # take up to an hour to backup

webdavplus:
  enabled: false
  <<: *webdav
  podLabels:
    app.elfhosted.com/name: webdav-plus
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M"
  envFrom:
  - configMapRef:
      name: webdav-plus-config

jfa:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/jfa-go
    tag: v0.5.1@sha256:02274e43d86ca9ba839c4f35be66c86560a809046e7528acb2bbc732a1d3eb71
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jfa"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: jfa
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jfa
          optional: true
    tmp:
      enabled: true
      type: emptyDir
      mountPath: /tmp
      sizeLimit: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jfa
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8056
  resources:
    requests:
      cpu: 0m
      memory: 150Mi
    limits:
      cpu: 2
      memory: 1Gi

mattermost:
  enabled: false
  # Default values for mattermost-team-edition.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.
  image:
    repository: mattermost/mattermost-team-edition
    tag: 10.7.0@sha256:2fa1224d5ccaedd6d505f6e18f5fc207c68029489d18de951213b624abf43e23
    imagePullPolicy: IfNotPresent
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mattermost"

  initContainerImage:
    repository: appropriate/curl
    tag: latest
    imagePullPolicy: IfNotPresent

  extraInitContainers: []

  ## Deployment Strategy
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  deploymentStrategy:
    type: Recreate
    rollingUpdate: null

  ## How many old ReplicaSets for Mattermost Deployment you want to retain
  revisionHistoryLimit: 1

  ## Enable persistence using Persistent Volume Claims
  ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
  ## ref: https://docs.gitlab.com/ee/install/requirements.html#storage
  ##
  persistence:
    ## This volume persists generated data from users, like images, attachments...
    ##
    data:
      enabled: true
      size: 10Gi
      ## If defined, volume.beta.kubernetes.io/storage-class: <storageClass>
      ## Default: volume.alpha.kubernetes.io/storage-class: default
      ##
      # storageClass:
      accessMode: ReadWriteOnce
      existingClaim: "config"
      subPath: mattermost/data
    plugins:
      enabled: false # these just end up under data anyway

  service:
    type: ClusterIP
    externalPort: 8065
    internalPort: 8065
    annotations: {}
    # loadBalancerIP:
    loadBalancerSourceRanges: []

  ingress:
    enabled: false
    path: /
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # certmanager.k8s.io/issuer:  your-issuer
      # nginx.ingress.kubernetes.io/proxy-body-size: 50m
      # nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
      # nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
      # nginx.ingress.kubernetes.io/proxy-buffering: "on"
      # nginx.ingress.kubernetes.io/configuration-snippet: |
      #   proxy_cache mattermost_cache;
      #   proxy_cache_revalidate on;
      #   proxy_cache_min_uses 2;
      #   proxy_cache_use_stale timeout;
      #   proxy_cache_lock on;
      #### To use the nginx cache you will need to set an http-snippet in the ingress-nginx configmap
      #### http-snippet: |
      ####     proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=mattermost_cache:10m max_size=3g inactive=120m use_temp_path=off;
    hosts:
      - mattermost.example.com
    tls:
      # - secretName: mattermost.example.com-tls
      #   hosts:
      #     - mattermost.example.com

  route:
    enabled: false

  ## If use this please disable the mysql chart by setting mysql.enable to false
  externalDB:
    enabled: true

    ## postgres or mysql
    externalDriverType: "mysql"

    ## postgres:  "<USERNAME>:<PASSWORD>@<HOST>:5432/<DATABASE_NAME>?sslmode=disable&connect_timeout=10"
    ## mysql:     "<USERNAME>:<PASSWORD>@tcp(<HOST>:3306)/<DATABASE_NAME>?charset=utf8mb4,utf8&readTimeout=30s&writeTimeout=30s"
    externalConnectionString: "mattermost:IUzI1NiJ9.eyJhdWQiOiIwMDk1MTkyYjhjZWIyYjVhNDQwMT@tcp(mattermost-mysql:3306)/mattermost?charset=utf8mb4,utf8&readTimeout=30s&writeTimeout=30s"

  mysql:
    nameOverride: mattermost-mysql
    enabled: true
    architecture: standalone
    # nameOverride: mattermost-mariadb

    commonAnnotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mattermost"

    auth:
      rootPassword: "3uAaJYGJLR3d2qbBM2FsSThJ"
      database: "mattermost"
      username: "mattermost"
      password: "IUzI1NiJ9.eyJhdWQiOiIwMDk1MTkyYjhjZWIyYjVhNDQwMT"

    primary:
      readinessProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      livenessProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      startupProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      persistence:
        enabled: true
        existingClaim: config
        subPath: mattermost/database
      resources:
        requests:
          cpu: 5m
          memory: 512Mi
        limits:
          cpu: 2
          memory: 1024Mi
      containerSecurityContext:
        enabled: true
        seccompProfile:
          type: RuntimeDefault
        runAsUser: 568
        runAsGroup: 568
      podSecurityContext:
        enabled: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
      extraVolumeMounts:
      - mountPath: /opt/bitnami/mysql/tmp/
        name: tmp
      extraVolumes:
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi
      - name: backup-database-script
        configMap:
          name: mattermost-backup
      - name: confighdd
        persistentVolumeClaim:
          claimName: config
          subPath: mattermost/database
      sidecars:
        - name: backup-database
          image: *tooling_image
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: 3uAaJYGJLR3d2qbBM2FsSThJ
            - name: MYSQL_DATABASE
              value: mattermost
          command:
          - /usr/bin/dumb-init
          - /bin/bash
          - -c
          - |

            sleep 2m # give mysql time to start up
            while true
            do
              now=$(date +"%s_%Y-%m-%d")
              /usr/bin/mysqldump --opt -h mattermost-mysql -u root -p${MYSQL_ROOT_PASSWORD} ${MYSQL_DATABASE} > "/backup/${now}_${MYSQL_DATABASE}.sql"
              sleep 1d
            done

  ## Additional pod annotations
  extraPodAnnotations: {}

  ## Additional env vars
  extraEnvVars: []
    # This is an example of extra env vars when using with the deployment with GitLab Helm Charts
    # - name: POSTGRES_PASSWORD_GITLAB
    #   valueFrom:
    #     secretKeyRef:
    #       # NOTE: Needs to be manually created
    #       # kubectl create secret generic gitlab-postgresql-password --namespace <NAMESPACE> --from-literal postgres-password=<PASSWORD>
    #       name: gitlab-postgresql-password
    #       key: postgres-password
    # - name: POSTGRES_USER_GITLAB
    #   value: gitlab
    # - name: POSTGRES_HOST_GITLAB
    #   value: gitlab-postgresql
    # - name: POSTGRES_PORT_GITLAB
    #   value: "5432"
    # - name: POSTGRES_DB_NAME_MATTERMOST
    #   value: mm5
    # - name: MM_SQLSETTINGS_DRIVERNAME
    #   value: "postgres"
    # - name: MM_SQLSETTINGS_DATASOURCE
    #   value: postgres://$(POSTGRES_USER_GITLAB):$(POSTGRES_PASSWORD_GITLAB)@$(POSTGRES_HOST_GITLAB):$(POSTGRES_PORT_GITLAB)/$(POSTGRES_DB_NAME_MATTERMOST)?sslmode=disable&connect_timeout=10

  ## Additional init containers
  # extraInitContainers: []
    # This is an example of extra Init Container when using with the deployment with GitLab Helm Charts
    # - name: bootstrap-database
    #   image: "postgres:9.6-alpine"
    #   imagePullPolicy: IfNotPresent
    #   env:
    #     - name: POSTGRES_PASSWORD_GITLAB
    #       valueFrom:
    #         secretKeyRef:
    #           name: gitlab-postgresql-password
    #           key: postgres-password
    #     - name: POSTGRES_USER_GITLAB
    #       value: gitlab
    #     - name: POSTGRES_HOST_GITLAB
    #       value: gitlab-postgresql
    #     - name: POSTGRES_PORT_GITLAB
    #       value: "5432"
    #     - name: POSTGRES_DB_NAME_MATTERMOST
    #       value: mm5
    #   command:
    #     - sh
    #     - "-c"
    #     - |
    #       if PGPASSWORD=$POSTGRES_PASSWORD_GITLAB psql -h $POSTGRES_HOST_GITLAB -p $POSTGRES_PORT_GITLAB -U $POSTGRES_USER_GITLAB -lqt | cut -d \| -f 1 | grep -qw $POSTGRES_DB_NAME_MATTERMOST; then
    #       echo "database already exist, exiting initContainer"
    #       exit 0
    #       else
    #       echo "Database does not exist. creating...."
    #       PGPASSWORD=$POSTGRES_PASSWORD_GITLAB createdb -h $POSTGRES_HOST_GITLAB -p $POSTGRES_PORT_GITLAB -U $POSTGRES_USER_GITLAB $POSTGRES_DB_NAME_MATTERMOST
    #       echo "Done"
    #       fi

  # Add additional volumes and mounts, for example to add SAML keys in the app or other files the app server may need to access
  extraVolumes: []
    # - hostPath:
    #     path: /var/log
    #   name: varlog
  extraVolumeMounts:
    - name: mattermost-data
      mountPath: mattermost/mattermost/logs
      subPath: mattermost/logs
      readOnly: true

  ## Node selector
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
  nodeSelector: {}

  ## Affinity
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}

  ## Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  ## Pod Security Context
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  securityContext:
    fsGroup: 568
    runAsGroup: 568
    runAsUser: 568
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL

  serviceAccount:
    create: false
    name:
    annotations: {}

  ## Configuration
  ## The config here will be injected as environment variables in the deployment
  ## Please refer to https://docs.mattermost.com/administration/config-settings.html#configuration-in-database for more information
  ## You can add any config here, but need to respect the format: MM_<GROUPSECTION>_<SETTING>. ie: MM_SERVICESETTINGS_ENABLECOMMANDS: false
  config:
    MM_PLUGINSETTINGS_CLIENTDIRECTORY: "./client/plugins"

syncthing:
  enabled: false
  hostname: elfhosted
  priorityClassName: tenant-bulk
  image:
    repository: ghcr.io/elfhosted/syncthing
    tag: 1.29.2@sha256:ff865a12c39fc3f92fc779d824cdd0c7b6d304a74a282fdf2d3354f56a590a5c
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-syncthing"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: syncthing
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-syncthing
          optional: true

  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8384
  resources:
    requests:
      cpu: 0m
      memory: 70Mi
    limits:
      cpu: 1
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: syncthing
      - mountPath: /tmp
        name: tmp
    setup:
      image: ghcr.io/elfhosted/syncthing:1.29.2@sha256:ff865a12c39fc3f92fc779d824cdd0c7b6d304a74a282fdf2d3354f56a590a5c
      imagePullPolicy: IfNotPresent
      envFrom:
      - configMapRef:
          name: elfhosted-user-config
      command:
      - /bin/ash
      - -c
      - |
        set -x
        set -e

        # Generate a new config if necessary
        if [ ! -f /config/config.xml ]
        then
          # We are generating a new config
          syncthing generate --config=/config
        fi

        # Apply the port every time (incase the user changes it and reboots)
        # sed -i  "s/<listenAddress>tcp.*/<listenAddress>tcp:\/\/0.0.0.0:${PORT_SYNCTHING}<\/listenAddress>/" /config/config.xml
        # sed -i  "s/<listenAddress>quic.*/<listenAddress>quic:\/\/0.0.0.0:${PORT_SYNCTHING}<\/listenAddress>/" /config/config.xml

        # # And if it's defaulted...
        # sed -i  "s/<<listenAddress>default<\/listenAddress>/<listenAddress>tcp:\/\/0.0.0.0:${PORT_SYNCTHING}<\/listenAddress>\n\t<listenAddress>quic:\/\/0.0.0.0:${PORT_SYNCTHING}<\/listenAddress>/" /config/config.xml

        # Ignore the fact that we have no password set
        # grep '<insecureAdminAccess>true</insecureAdminAccess>' /config/config.xml || sed -i  "s/<\/gui>/<insecureAdminAccess>true<\/insecureAdminAccess>\n\t<\/gui>/" /config/config.xml

        # Avoid foolish use of capital letters in default sync folder
        # sed -i  "s/\/storage\/elfstorage\/Sync/\/storage\/elfstorage\/syncthing/" /config/config.xml

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: syncthing
      resources: *default_resources
      securityContext: *default_securitycontext

rdtclient: &app_rdtclient
  enabled: false
  hostname: elfhosted
  priorityClassName: tenant-bulk
  podLabels:
    app.elfhosted.com/class: debrid
  image:
    repository: ghcr.io/elfhosted/rdtclient
    tag: v2.0.108@sha256:3dc2d777e217a3293c4a00a83d34ccebd2068d56ff016668228020fe76f93356
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rdtclient"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data/db
      subPath: rdtclient
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rdtclient
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6500
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
      ephemeral-storage: 50Mi
    limits:
      cpu: 100m
      memory: 2Gi
      ephemeral-storage: 100Mi # a safety net against node ephemeral space exhaustion
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rdtclient
      - mountPath: /tmp
        name: tmp

rdtclientpremiumize:
  enabled: false
  <<: *app_rdtclient
  podLabels:
    app.elfhosted.com/name: rdtclient-premiumize
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rdtclient-premiumize"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data/db
      subPath: rdtclient-premiumize
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rdtclient-premiumize
          optional: true
    download: # in case users use /tmp
      enabled: true
      type: custom
      mountPath: /data/downloads
      volumeSpec: *volumespec_ephemeral_volume_1g  
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rdtclient-premiumize
      - mountPath: /tmp
        name: tmp

rdtclienttorbox:
  enabled: false
  <<: *app_rdtclient
  podLabels:
    app.elfhosted.com/name: rdtclient-torbox
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rdtclient-torbox"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data/db
      subPath: rdtclient-torbox
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rdtclient-torbox
          optional: true
    download: # in case users use /tmp
      enabled: true
      type: custom
      mountPath: /data/downloads
      volumeSpec: *volumespec_ephemeral_volume_1g  
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rdtclient-torbox
      - mountPath: /tmp
        name: tmp

# RDTClient for AllDebrid
rdtclientalldebrid:
  enabled: false
  <<: *app_rdtclient
  podLabels:
    app.elfhosted.com/name: rdtclient-alldebrid
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rdtclient-alldebrid"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data/db
      subPath: rdtclient-alldebrid
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rdtclient-alldebrid
          optional: true
    download: # in case users use /tmp
      enabled: true
      type: custom
      mountPath: /data/downloads
      volumeSpec: *volumespec_ephemeral_volume_1g  
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rdtclient-alldebrid
      - mountPath: /tmp
        name: tmp      
  addons:
    vpn:
      enabled: true
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
      envFrom:
      - configMapRef:
          name: gluetun-config
      env:
        DOT: "off"
        FIREWALL_INPUT_PORTS: "6500"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

jdownloader:
  enabled: false
  hostname: elfhosted
  # runtimeClassName: kata
  priorityClassName: tenant-bulk
  image:
    repository: jlesage/jdownloader-2
    tag: v25.02.1
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jdownloader"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 568
    # runAsGroup: 568
    fsGroup: 568 # need this so that the bootstrap can run
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    JDOWNLOADER_HEADLESS: 1
    APP_NICENESS: 19
  envFrom:
  - configMapRef:
      name: jdownloader-config
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
      mountPath: /output
      subPath: jdownloader/downloads/completed/jdownloader
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jdownloader
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9898
  resources:
    requests:
      cpu: 0m
      memory: 10Mi
    limits:
      cpu: 0.5
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jdownloader
      - mountPath: /tmp
        name: tmp
  additionalContainers:
    podinfo:
      image: stefanprodan/podinfo # used to run probes from gatus

miniflux:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/miniflux
    tag: 2.2.7
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-miniflux,miniflux-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1500m # if par threads is 1, this leaves 0.5cpu for downloading
      memory: 1Gi
  envFrom:
  - configMapRef:
      name: miniflux-config
  postgresql:
    enabled: true
    nameOverride: miniflux-postgresql
    commonAnnotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-miniflux"
    auth:
      username: miniflux
      password: miniflux
      database: miniflux
      postgresPassword: miniflux
    primary:
      affinity: *standard_affinity
      tolerations: *standard_tolerations
      persistence:
        enabled: true
        existingClaim: config
        subPath: miniflux/database
      resources:
        requests:
          cpu: 5m
          memory: 128Mi
        limits:
          cpu: 2
          memory: 1024Mi
      containerSecurityContext:
        enabled: true
        seccompProfile:
          type: RuntimeDefault
        runAsUser: 568
        runAsGroup: 568
      podSecurityContext:
        enabled: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
      extraVolumeMounts:
      - mountPath: /opt/bitnami/postgresql/conf/
        name: conf
      - mountPath: /opt/bitnami/postgresql/tmp/
        name: tmp
      extraVolumes:
      - name: conf
        emptyDir:
          sizeLimit: 1Gi
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi
      sidecars:
        - name: backup-database
          image: *tooling_image
          env:
            - name: POSTGRES_PASSWORD
              value: miniflux
            - name: POSTGRES_DATABASE
              value: miniflux
            - name: POSTGRES_USER
              value: miniflux
          command:
          - /usr/bin/dumb-init
          - /bin/bash
          - -c
          - |

            set +e # for debug
            sleep 2m # give postgres time to start up
            while true
            do
              now=$(date +"%s_%Y-%m-%d")
              PGPASSWORD=$POSTGRES_PASSWORD pg_dump -U $POSTGRES_USER -h localhost -d $POSTGRES_DATABASE -F c -f /backup/${now}_${POSTGRES_DATABASE}.psql
              sleep 1d
            done

joplinserver:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/geek-cookbook/joplin-server
    tag: v2.14.2@sha256:b4f52bffce08541dd54e823b78bbfa18e091d53064ed507f69fe9e1ca92b719b
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-joplinserver,joplinserver-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # breaks migrations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1500m # if par threads is 1, this leaves 0.5cpu for downloading
      memory: 1Gi
  envFrom:
  - configMapRef:
      name: joplinserver-config
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: joplinserver/data
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  postgresql:
    enabled: true
    nameOverride: joplinserver-postgresql
    commonAnnotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-joplinserver"
    auth:
      username: joplinserver
      password: joplinserver
      database: joplinserver
      postgresPassword: joplinserver
    primary:
      affinity: *standard_affinity
      tolerations: *standard_tolerations
      persistence:
        enabled: true
        existingClaim: config
        subPath: joplinserver/database
      resources:
        requests:
          cpu: 5m
          memory: 128Mi
        limits:
          cpu: 2
          memory: 1024Mi
      containerSecurityContext:
        enabled: true
        seccompProfile:
          type: RuntimeDefault
        runAsUser: 568
        runAsGroup: 568
      podSecurityContext:
        enabled: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
      extraVolumeMounts:
      - mountPath: /opt/bitnami/postgresql/conf/
        name: conf
      - mountPath: /opt/bitnami/postgresql/tmp/
        name: tmp
      extraVolumes:
      - name: conf
        emptyDir:
          sizeLimit: 1Gi
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi
      sidecars:
        - name: backup-database
          image: *tooling_image
          env:
            - name: POSTGRES_PASSWORD
              value: joplinserver
            - name: POSTGRES_DATABASE
              value: joplinserver
            - name: POSTGRES_USER
              value: joplin
          command:
          - /usr/bin/dumb-init
          - /bin/bash
          - -c
          - |

            set +e # for debug
            sleep 2m # give postgres time to start up
            while true
            do
              now=$(date +"%s_%Y-%m-%d")
              PGPASSWORD=$POSTGRES_PASSWORD pg_dump -U $POSTGRES_USER -h localhost -d $POSTGRES_DATABASE -F c -f /backup/${now}_${POSTGRES_DATABASE}.psql
              sleep 1d
            done

homepage:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/gethomepage/homepage
    tag: v1.1.1
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-homepage,homepage-config,homepage-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
    privileged: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 568
    # runAsGroup: 568
    fsGroup: 568 # need this so that the bootstrap can run
    fsGroupChangePolicy: "OnRootMismatch"
  serviceAccount:
    create: true
    name: homepage
  automountServiceAccountToken: true
  env:
    PUID: 568
    PGID: 568
  envFrom:
  - configMapRef:
      name: homepage-env
  - configMapRef:
      name: elfbot-homepage
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/config
      subPath: homepage
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    config-default:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: homepage-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-homepage
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 250m # deliberately hobble the CPU in favor of GPU transcoding
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: homepage
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /app/config/user-change-these/
        touch /app/config/user-change-these/JELLYFIN_KEY
        touch /app/config/user-change-these/PLEX_KEY
        touch /app/config/user-change-these/EMBY_KEY
        touch /app/config/user-change-these/NAVIDROME_USER
        touch /app/config/user-change-these/NAVIDROME_TOKEN
        touch /app/config/user-change-these/NAVIDROME_SALT
        touch /app/config/user-change-these/CALIBREWEB_USERNAME
        touch /app/config/user-change-these/CALIBREWEB_PASSWORD
        touch /app/config/user-change-these/KOMGA_USERNAME
        touch /app/config/user-change-these/KOMGA_PASSWORD
        touch /app/config/user-change-these/KAVITA_USERNAME
        touch /app/config/user-change-these/KAVITA_PASSWORD
        touch /app/config/user-change-these/AUDIOBOOKSHELF_KEY
        touch /app/config/user-change-these/OMBI_KEY
        touch /app/config/user-change-these/OVERSEERR_KEY
        touch /app/config/user-change-these/JELLYSEERR_KEY
        touch /app/config/user-change-these/TAUTULLI_KEY
        touch /app/config/user-change-these/tunarr_USERNAME
        touch /app/config/user-change-these/tunarr_PASSWORD
        touch /app/config/user-change-these/MINIFLUX_KEY
        touch /app/config/user-change-these/UPTIMEKUMA_SLUG
        touch /app/config/user-change-these/GOTIFY_KEY

        # If we don't already have an example config, create one
        if [ ! -f /app/config/dont-overwrite-me ];
        then
          cp /bootstrap/* /app/config/
        fi
      volumeMounts:
      - mountPath: /app/config
        name: config
        subPath: homepage
      - name: config-default
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true


wallabag:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: stefanprodan/podinfo
    tag: latest
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-wallabag,wallabag-config"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
    privileged: false
  # runtimeClassName: kata
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568 # for the mounted volumes
  persistence:
    config:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 0m
      memory: 32Mi
    limits:
      cpu: 100m
      memory: 1Gi
  additionalContainers:
    ui:
      image: ghcr.io/elfhosted/wallabag:2.6.12@sha256:dfbdd2066ced71f7a6315b457e86caace27ecef44b88ba0c3a090aee386ff5b5
      volumeMounts:
      - mountPath: /var/www/wallabag/data
        name: config
        subPath: wallabag/data
      - mountPath: /var/www/wallabag/images
        name: config
        subPath: wallabag/images
      envFrom:
      - configMapRef:
          name: elfbot-wallbag
          optional: true
      - configMapRef:
          name: wallabag-config
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        allowPrivilegeEscalation: false
      resources:
        requests:
          cpu: 0m
          memory: 100Mi
        limits:
          cpu: 500m
          memory: 200Mi

gotosocial:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/gotosocial
    tag: 0.18.3@sha256:2b961d3ba060a2254877f2a9f87187e1dafe35be331320f2bb9fa24bb1cc67ce
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-gotosocial"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work with s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 1
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  env:
    S6_READ_ONLY_ROOT: "true"
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: gotosocial
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: gotosocial-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-gotosocial
          optional: true
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: gotosocial
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [[ ! -f /config/config.yaml ]];
        then
          cp /bootstrap/config.yaml /config/
        fi

        # Setup the fish env
        mkdir -p /config/.config/fish
        cp /bootstrap/config.fish /config/.config/fish
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: gotosocial
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /livez
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /readyz
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 60 # allow 10 min of failures to start up
        httpGet:
          path: /readyz
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10  

blueskypds:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/bluesky-pds
    tag: 0.4.123@sha256:2bf88b8fcdf62aaf63ac1f668f56a94d8bea725017218d565f007a7a1ecee776
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-bluesky-pds"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work with s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 1
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  env:
    S6_READ_ONLY_ROOT: "true"
  envFrom:
  - configMapRef:
      name: bluesky-pds-env
  - configMapRef:
      name: elfbot-bluesky-pds
      optional: true    
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: bluesky-pds
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-bluesky-pds
          optional: true
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: bluesky-pds
      - mountPath: /tmp
        name: tmp
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /xrpc/_health
          port: 3000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /xrpc/_health
          port: 3000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 60 # allow 10 min of failures to start up
        httpGet:
          path: /xrpc/_health
          port: 3000
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10  

riven: &app_riven
  enabled: false
  podLabels:
    app.elfhosted.com/name: riven
  image:
    repository: ghcr.io/elfhosted/riven
    tag: v0.21.19@sha256:89b51038244790d5942dbb5fb81bcf99c9500869cec0b1ab8ac38c2f20af8a60
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-riven,riven-env,riven-setup"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with ilikedanger currently
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: riven-env
  - configMapRef:
      name: elfbot-riven
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 20Mi
    limits:
      cpu: 2
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /riven/data
      subPath: riven
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /riven/data/logs
      subPath: riven
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-riven
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory
    setup:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: riven-setup        
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: riven
      - mountPath: /tmp
        name: tmp
    setup-postgres:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/postgresql/database
        mkdir -p /config/postgresql/backups
        chown elfie:elfie /config -R

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: riven
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        # run the setup script from the configmap, so that we can make templated changes
        bash /setup/setup.sh
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      - mountPath: /config
        name: config
        subPath: riven
      - name: setup
        mountPath: "/setup/"              
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      env:
        - name: POSTGRES_PASSWORD
          value: postgres
        - name: POSTGRES_DB
          value: riven
        - name: POSTGRES_USER
          value: postgres
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: riven/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi   

rivenvpn: 
  enabled: false
  <<: *app_riven
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-riven,riven-env,riven-setup,gluetun-config"  
  addons:
    vpn:
      enabled: true
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
      envFrom:
      - configMapRef:
          name: gluetun-config
      env:
        DOT: "off"
        FIREWALL_INPUT_PORTS: "3001,8080" # 3001 is ttyd, 8080 is the backend
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"        
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

rivenfrontend: &app_rivenfrontend
  podLabels:
    app.elfhosted.com/name: riven-frontend
  image:
    repository: ghcr.io/elfhosted/riven-frontend
    tag: v0.20.0@sha256:c4d792868a8113a2f57f504fe371cbcd8c52172d734c812c366bff352672998e
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rivenfrontend,riven-frontend-env,riven-frontend-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with ilikedanger currently
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    config:
      enabled: "true"
      type: "custom"
      mountPath: /riven/config
      volumeSpec:
        configMap:
          name: riven-frontend-config          
  envFrom:
  - configMapRef:
      name: riven-frontend-env
  - configMapRef:
      name: elfbot-rivenfrontend
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 20Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000

# So that we can toggle it on with rivenvpn
rivenfrontendvpn:
  enabled: false
  <<: *app_rivenfrontend

airdcpp: &app_airdcpp
  enabled: false
  image:
    repository: ghcr.io/geek-cookbook/airdcpp
    tag: 2.9.0@sha256:d9f6e597bcfc38946d0c4cafce775a559e7b8cf7c66397c9c506cb695ea01205
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: airdcpp
  podAnnotations:
    kubernetes.io/egress-bandwidth: "100M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-airdcpp"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 2
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5600
  env:
    WAIT_FOR_VPN: "true"
    PORT_FILE: /.airdcpp/forwarded-port
  probes:
    liveness:
      enabled: false
    startup:
      enabled: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /.airdcpp/
      subPath: airdcpp
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-airdcpp
          optional: true
  initContainers:
    bootstrap: *bootstrap
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      envFrom:
      - configMapRef:
          name: airdcpp-pia-config
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
        subPath: airdcpp
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus

airdcpppia:
  enabled: false
  <<: *app_airdcpp

airdcppgluetun:
  enabled: false
  <<: *app_airdcpp
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
      envFrom:
      - configMapRef:
          name: airdcpp-gluetun-config
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
        subPath: airdcpp
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus

jackett:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/jackett
    tag: 0.22.1778@sha256:15d65f5e9d1238d5e9ff78a135ee3cbca404635ca332e947dfda08b5d2513aba
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jackett"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9117
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: jackett
      volumeSpec:
        persistentVolumeClaim:
          claimName: config        
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jackett
          optional: true
  initContainers:
    bootstrap: *bootstrap

stremioserver: &app_stremioserver
  enabled: false
  image:
    repository: ghcr.io/elfhosted/stremio-server
    tag: v4.20.8@sha256:24977206eff9aba5c4f62339401dccec7799be06378c35c59d3c5cc451899107
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: stremio-server
  podAnnotations:
    kubernetes.io/egress-bandwidth: "100M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-stremio-server,stremio-server-env,stremio-server-config"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # review
    privileged: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    runAsUser: 568 # review
    runAsGroup: 568
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 250m
      memory: 1Gi
    limits:
      cpu: 500m
      memory: 6Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 11470
  envFrom:
  - configMapRef:
      name: stremio-server-env
  persistence:
    tmp: *tmp
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    config:
      enabled: "true"
      subPath: "server-settings.json"
      mountPath: /config/server-settings.json
      type: "custom"
      volumeSpec:
        configMap:
          name: stremio-server-config
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
    transcode: # in case users use /tmp
      enabled: true
      type: custom
      mountPath: /transcode
      volumeSpec: *volumespec_ephemeral_volume_10g
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
  additionalContainers:
    casting:
      image: nginxinc/nginx-unprivileged
      volumeMounts:
      - mountPath: /usr/share/nginx/html/casting.json
        name: config
        subPath: casting.json
        readOnly: true
      - mountPath: /tmp
        name: tmp
      resources: *default_resources
      securityContext: *default_securitycontext

# Stremioserver with PIA
stremioserverpia:
  <<: *app_stremioserver
  enabled: false
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      envFrom:
      - configMapRef:
          name: stremioserver-pia-config
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus

# Stremioserver with Gluetun
stremioservergluetun:
  <<: *app_stremioserver
  enabled: false
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
      envFrom:
      - configMapRef:
          name: stremioserver-gluetun-config
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus

flixio:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/stremio-web
    tag: rolling@sha256:de20030d591000b871519899ccbcead9e958a8e1eb5e98d05b65e033286a7e2a
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: flixio
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-flixio"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with iprom's patching trick
    privileged: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: flixio-env
  - configMapRef:
      name: elfbot-flixio
      optional: true  
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080

flixioapi:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/flixio-api
    tag: rolling@sha256:7fa8715f64e65cccd352b1ce56c0b4d8c7f1ee4c730bc0e8f93e4cabb90e53ca
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: flixio-api
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-flixio-api"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    runAsUser: 568
    runAsGroup: 568
  automountServiceAccountToken: false
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-flixio-api
          optional: true
    config:
      enabled: true
      type: custom
      mountPath: /app/data
      subPath: flixio-api
      volumeSpec:
        persistentVolumeClaim:
          claimName: config   
  envFrom:
  - configMapRef:
      name: flixio-api-env
  - configMapRef:
      name: elfbot-flixio-api
      optional: true           
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080

stremiojackett:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/stremio-jackett
    tag: v4.2.6@sha256:7ec5011079056b0eae117f13e8eb56b8f0169e1c9cd9db2e2b4db38b45e5dd75
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-stremio-jackett"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: stremio-jackett-env
  resources:
    requests:
      cpu: 10m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  persistence:
    pm2:
      enabled: true
      type: emptyDir
      mountPath: /.pm2
      sizeLimit: 1Gi
    npm:
      enabled: true
      type: emptyDir
      mountPath: /.npm
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-stremio-jackett
          optional: true

pairdrop:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/pairdrop
    tag: v1.11.2@sha256:6d9e05e71da669fc63a46dd0dfa4170474bf7afc6545353c5442a9beca83f3d9
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: pairdrop
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-pairdrop"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  # envFrom:
  # - configMapRef:
  #     name: pairdrop-env
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-pairdrop
          optional: true

actual:
  enabled: false
  image:
    repository: ghcr.io/actualbudget/actual-server
    tag: 25.4.0-alpine
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: actual
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-actual"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5006
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-actual
          optional: true
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: actual
      volumeSpec:
        persistentVolumeClaim:
          claimName: config          

petio:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/petio
    tag: v0.5.5@sha256:a28b7ffb5b1b04a8ad798112604c410a9a09f8882e9bf98b9f24e8f41571f505
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-petio"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 7777
  persistence:
    backup: *backup
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-petio
          optional: true
    config:
      enabled: true
      type: custom
      mountPath: /app/api/config/
      subPath: petio/config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    tmp: *tmp
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: petio
      - mountPath: /tmp
        name: tmp
  additionalContainers:
    mongodb:
      image: mongodb/mongodb-community-server:8.0.8-ubi8
      volumeMounts:
        - name: config
          subPath: petio/mongodb
          mountPath: /data/db/
        - name: tmp
          mountPath: /tmp
      securityContext: *default_securitycontext

pgadmin:
  enabled: false
  image:
    repository: dpage/pgadmin4
    tag: "9.2"
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-pgadmin"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    capabilities:
      add:
      - NET_BIND_SERVICE
      drop:
      - ALL
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: pgadmin-env
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 80
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: pgadmin
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    tmp: *tmp

redisinsight:
  enabled: false
  image:
    repository: redislabs/redisinsight
    tag: v2@sha256:7fef8b7ecf2e8597037f906fc69863345dd846d36577210569396f7917333355
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-redisinsight"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    capabilities:
      add:
      - IPC_LOCK
      drop:
      - ALL
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5540
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: redisinsight
      volumeSpec:
        persistentVolumeClaim:
          claimName: config

mongoexpress:
  enabled: false
  image:
    repository: mongo-express
    tag: 1.0.2-18
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mongoexpress"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8081
  envFrom:
  - configMapRef:
      name: elfbot-mongoexpress
      optional: true

comet:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/comet
    tag: v2.4.3@sha256:9afbe1519b11fc6197e011889f3d5f77f960709e419039ed01c1ce49f18279ae
  priorityClassName: tenant-normal
  podAnnotations:
    kubernetes.io/egress-bandwidth: "64M"
  podLabels:
      app.elfhosted.com/name: comet
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-comet,comet-env"
      secret.reloader.stakater.com/reload: "comet-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568    
  initContainers:
    setup-warp:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false    
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
    config:
      enabled: true
      type: custom
      mountPath: /app/data
      subPath: comet
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    cache:
      enabled: true
      type: emptyDir
      mountPath: /.cache
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-comet
          optional: true
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory          
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 2
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  envFrom:
  - configMapRef:
      name: comet-env
  - secretRef:
      name: comet-env
  - configMapRef:
      name: elfbot-comet
      optional: true
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      env:
        CHANGE_CONTAINER_PORTS: "True"
        HTTP_PORT: "3002"
        HTTPS_PORT: "3003"        
      securityContext: *speedtest_securitycontext
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8001"
        HTTP_CONTROL_SERVER_ADDRESS: ":8001"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "8000"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared      

jackettio:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/jackettio
    tag: v1.7.0@sha256:2159b41a44c59829600df565d24d09f1156aeb481b6b1b565abbf10c13d5174c
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jackettio,jackettio-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: jackettio
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory              
  resources:
    requests:
      cpu: 10m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 4000
  envFrom:
  - configMapRef:
      name: jackettio-env
  - configMapRef:
      name: elfbot-jackettio
      optional: true
  initContainers:      
    setup-warp:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false    
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "4000"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared

stremthru:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/stremthru
    tag: 0.63.0@sha256:8a8e5209426eacdaba4081e317bdb90259dcbc57a07ffce65fe8d006f9f6d753
  priorityClassName: tenant-normal
  podAnnotations:
    kubernetes.io/egress-bandwidth: "64M"  
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-stremthru,stremthru-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: stremthru
      volumeSpec:
        persistentVolumeClaim:
          claimName: config      
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755           
    # shared:
    #   enabled: true
    #   mountPath: /shared
    #   type: emptyDir
    #   volumeSpec:
    #     medium: Memory                    
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  envFrom:
  - configMapRef:
      name: stremthru-env
  - configMapRef:
      name: elfbot-stremthru
      optional: true
  - secretRef:
      name: stremthru-env      
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      env:
        IPTABLES_BACKEND: nft
        KILLSWITCH: "true"
        LOCAL_NETWORK: 10.0.0.0/8
        LOC: de-frankfurt
        PORT_FORWARDING: "0"
        PORT_PERSIST: "1"
        NFTABLES: "1"
        VPNDNS: "0"
      envFrom:
      - secretRef:
          name: stremthru-vpn
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      env:
        CHANGE_CONTAINER_PORTS: "True"
        HTTP_PORT: "3002"
        HTTPS_PORT: "3003"    
      securityContext: *speedtest_securitycontext  

davio:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/davio
    tag: v1.0.4@sha256:3508249d413b6b55bb2860358bbb92dd8ccd760969fadb0473540ddb74218523
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-davio,davio-env"
      secret.reloader.stakater.com/reload: "davio-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config
      volumeSpec: *volumespec_ephemeral_volume_1g
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 4000
  envFrom:
  - configMapRef:
      name: davio-env
  - secretRef:
      name: davio-env
  - configMapRef:
      name: elfbot-davio
      optional: true

mediafusion:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/mediafusion
    tag: 4.3.29@sha256:9525b038077e17c1460e6ce743855e10e1ca720ec7200219e91846ff3dbc0fa5
  podLabels:
      app.elfhosted.com/name: mediafusion
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mediafusion,mediafusion-env"
      secret.reloader.stakater.com/reload: "mediafusion-env,mediafusion-vpn"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    tinyproxy-conf:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tinyproxy-conf 
  resources:
    requests:
      cpu: 50m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8000
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10           
  envFrom:
  - configMapRef:
      name: mediafusion-env
  - configMapRef:
      name: elfbot-mediafusion
      optional: true
  - secretRef:
      name: mediafusion-env

piaproxy:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: thrnz/docker-wireguard-pia
    tag: latest
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "piaproxy-conf"
      secret.reloader.stakater.com/reload: "piaproxy-vpn"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations    
  env:
    IPTABLES_BACKEND: nft
    KILLSWITCH: "true"
    LOCAL_NETWORK: 10.0.0.0/8
    LOC: de-frankfurt
    PORT_FORWARDING: "0"
    PORT_PERSIST: "1"
    NFTABLES: "1"
    VPNDNS: "0"
  envFrom:
  - secretRef:
      name: piaproxy-vpn
  securityContext:
    privileged: true
    runAsUser: 0
    capabilities:
      add:
        - NET_ADMIN
        - SYS_MODULE
  persistence:
    tinyproxy-conf:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tinyproxy-conf 
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8888
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        exec:
          command:
          - /bin/bash
          - -c
          - curl -x http://localhost:8888 -s https://www.cloudflare.com/cdn-cgi/trace | grep www.cloudflare.com
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
  additionalContainers:
    tinyproxy:
      image: ghcr.io/elfhosted/tinyproxy:v1.4.3@sha256:9370be1434e80f8ac7d3a24d9c19335742b2c74ba15129dcdc166bb0dd3c9098
      volumeMounts:
      - mountPath: /etc/tinyproxy/tinyproxy.conf
        name: tinyproxy-conf
        subPath: tinyproxy.conf
      - mountPath: /shared
        name: shared
      # env:
      #   WAIT_FOR_VPN: "true"

vpnproxy:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/gluetun
    tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-vpnproxy"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations    
  securityContext:
    privileged: true
    runAsUser: 0
    capabilities:
      add:
        - NET_ADMIN
        - SYS_MODULE
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory          
  resources:
    requests:
      cpu: 0m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8888
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        exec:
          command:
          - /bin/ash
          - -c
          - curl -x http://localhost:8888 -s https://www.cloudflare.com/cdn-cgi/trace | grep www.cloudflare.com
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10            
  env:
    VPN_TYPE: wireguard
    VPN_SERVICE_PROVIDER: custom
    VPN_ENDPOINT_PORT: "2408"
    HTTPPROXY: "on"
    FIREWALL_INPUT_PORTS: "8888"
    DOT: "off"
  initContainers:      
    setup-warp:
      image: ghcr.io/elfhosted/tooling:focal-20240530@sha256:458d1f3b54e9455b5cdad3c341d6853a6fdd75ac3f1120931ca3c09ac4b588de
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false    

tinyproxy:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/tinyproxy
    tag: v1.4.3@sha256:9370be1434e80f8ac7d3a24d9c19335742b2c74ba15129dcdc166bb0dd3c9098
  podLabels:
      app.elfhosted.com/name: tinyproxy
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "tinyproxy-conf"
      secret.reloader.stakater.com/reload: "tinyproxy-vpn"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    tinyproxy-conf:
      enabled: "true"
      type: "custom"
      mountPath: /etc/tinyproxy/tinyproxy.conf
      subPath: tinyproxy.conf  
      volumeSpec:
        configMap:
          name: tinyproxy-conf 
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory          
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8888
  env:
    WAIT_FOR_VPN: "true"    
  initContainers:      
    setup-warp:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false    
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "8888"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared

mediaflowproxy:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/mediaflow-proxy
    tag: 1.9.7@sha256:b1d0504fa02a248e44270ff8ef20e456a681c9a112610f15562898a014bc4beb
  podLabels:
      app.elfhosted.com/name: mediaflow-proxy
  priorityClassName: tenant-normal
  podAnnotations:
    kubernetes.io/egress-bandwidth: "64M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mediaflow-proxy,mediaflow-proxy-env"
      secret.reloader.stakater.com/reload: "mediaflowproxy-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      env:
        CHANGE_CONTAINER_PORTS: "True"
        HTTP_PORT: "3002"
        HTTPS_PORT: "3003"    
      securityContext: *speedtest_securitycontext
  resources:
    requests:
      cpu: 100m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8888
  envFrom:
  - configMapRef:
      name: mediaflow-proxy-env
  - configMapRef:
      name: elfbot-mediaflow-proxy
      optional: true   

youriptv:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/youriptv-nobranding
    tag: rolling@sha256:b5b7cb9858e5d6b2edd636f6167ef1e78198deb0badea677256a50613b43211e
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-youriptv"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PORT: 3649
  persistence:
    tmp: *tmp
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3649

aiostreams:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/aiostreams
    tag: v1.14.2@sha256:ab0870d42225a1c4892d76e76cb9504ca060e80dd6a68b6ec56c7bc3ce61f498
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-aiostreams,aiostreams-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
  resources:
    requests:
      cpu: 0m
      memory: 50Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  envFrom:
  - configMapRef:
      name: aiostreams-env   
  - configMapRef:
      name: elfbot-aiostreams
      optional: true

stremify:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/stremify
    tag: rolling@sha256:dccb712ee19e8e16512175a4ca87471e24cd50a05c5c1aed18f4a6f8b25b208c
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-stremify"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp:
      enabled: true
      type: emptyDir
      mountPath: /tmp
    nuxt-node-modules:
      enabled: true
      type: emptyDir
      mountPath: /nuxt/node_modules
    nuxt:
      enabled: true
      type: emptyDir
      mountPath: /nuxt/.nuxt
    nitro:
      enabled: true
      type: emptyDir
      mountPath: /home/node/app/.nitro
  resources:
    requests:
      cpu: 10m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  envFrom:
  - configMapRef:
      name: stremify-env
  - configMapRef:
      name: elfbot-stremify
      optional: true
  - secretRef:
      name: stremify-env
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      env:
        IPTABLES_BACKEND: nft
        KILLSWITCH: "true"
        LOCAL_NETWORK: 10.0.0.0/8
        LOC: de-frankfurt
        PORT_FORWARDING: "0"
        PORT_PERSIST: "1"
        NFTABLES: "1"
        VPNDNS: "0"
      envFrom:
      - secretRef:
          name: stremify-vpn
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

recyclarr:
  enabled: false
  image:
    repository: ghcr.io/recyclarr/recyclarr
    tag: latest@sha256:759540877f95453eca8a26c1a93593e783a7a824c324fbd57523deffb67f48e1
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-recyclarr"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9898
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: recyclarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: recyclarr-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-recyclarr
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: recyclarr
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [ ! -f /config/recyclarr.yaml ];
        then
          cp /bootstrap/recyclarr.yaml /config/
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: recyclarr
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
  additionalContainers:
    podinfo:
      image: stefanprodan/podinfo # used to run probes from gatus
    sync:
      image: ghcr.io/recyclarr/recyclarr:latest@sha256:759540877f95453eca8a26c1a93593e783a7a824c324fbd57523deffb67f48e1
      command:
      - /bin/bash
      - -c
      - |
        recyclarr sync
        sleep infinity
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: recyclarr
      envFrom:
      - configMapRef:
          name: recyclarr-env

knightcrawler: &app_knightcrawler
  enabled: false
  image:
    repository: ghcr.io/elfhosted/knightcrawler-addon
    tag: v2.0.28@sha256:0e7eff73943a1c37b930cdb0bf4b8e76092ba7bd9017789d7c80c20204c4e561
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: knightcrawler
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-knightcrawler,elfbot-torrentio,knightcrawler-env"
      secret.reloader.stakater.com/reload: "knightcrawler-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 100m
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 7000
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-knightcrawler
          optional: true
    npm:
      enabled: true
      type: emptyDir
      mountPath: /.npm
    pm2:
      enabled: true
      mountPath: /.pm2
      type: emptyDir
  envFrom:
  - configMapRef:
      name: knightcrawler-env
  - secretRef:
      name: knightcrawler-env

zurg: &app_zurg
  enabled: false
  podLabels:
    app.elfhosted.com/class: debrid
    app.elfhosted.com/name: zurg
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M" # tested with _kilos in Discord on a 97Mbit remux
  image:
    repository: ghcr.io/elfhosted/zurg-rc
    tag: 2025.03.24.0030-nightly@sha256:72ec4f1aba38d11f7271514d1e25ad5b89df0bc85d182b0725dda3ef1dcd551f
  imagePullSecrets:
  - name: ghcr-io-elfhosted    
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-zurg,zurg-env,gluetun-config"
    strategy: Recreate
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: node-role.elfhosted.com/contended
            operator: In
            values:
            - "true"         
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.elfhosted.com/role
              operator: In
              values:
              - nodefinder # use nodefinder in the absense of zurg...
          topologyKey: "kubernetes.io/hostname"
      - weight: 2
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.elfhosted.com/name
              operator: In
              values:
              - zurg # .. but prefer zurg
          topologyKey: "kubernetes.io/hostname"
          namespaceSelector: {}  # i.e., in the absense of any better signal, pick a node which already has zurg on it
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 32Mi
    limits:
      cpu: 1
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9999
  persistence:
    tmp: *tmp
    backup: *backup # to pin zurg to the node with the backup PVC
    rclonemountrealdebridzurg: *rclonemountrealdebridzurg
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: zurg
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: zurg
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs          
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: zurg-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-zurg
          optional: true
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  envFrom:
  - configMapRef:
      name: zurg-env # this is here so we can use env vars to detect whether to enable warp
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: zurg
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # We need a /config/logs folder
        mkdir -p /config/logs

        # If we don't already have an example config, create one
        if [[ ! -f /config/config.yml ]];
        then
          cp /bootstrap/config.yml /config/
        fi

        # If we don't already have an example plex_update, create one
        if [[ ! -f /config/plex_update.sh ]];
        then
          cp /bootstrap/plex_update.sh /config/
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: zurg
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        # run the setup script from the configmap, so that we can make templated changes
        bash /bootstrap/setup.sh
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: zurg
      - name: example-config
        mountPath: "/bootstrap/"
  addons:
    vpn: &zurg_addons_vpn
      enabled: false # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
      envFrom:
      - configMapRef:
          name: gluetun-config
          optional: true
      - configMapRef:
          name: zurg-env # this is here so we can use env vars to detect whether to enable warp
      env:
        DOT: "off"
        FIREWALL_INPUT_PORTS: "9999" # 9999 is for zurg
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus
  probes:
    startup:
      spec:
        initialDelaySeconds: 0
        timeoutSeconds: 1
        ## This means it has a maximum of 5*120=720 seconds to start up before it fails
        periodSeconds: 5
        failureThreshold: 120

zurggluetun:
  <<: *app_zurg
  enabled: false
  podLabels:
    app.elfhosted.com/name: zurg
    app.elfhosted.com/class: debrid
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-zurg,zurg-gluetun-config,zurg-env"
  service:
    main:
      nameOverride: zurg
      enabled: true # necessary for probes, but probes aren't working with vpn addon currently
  env:
    WAIT_FOR_VPN: "true"
  addons:
    vpn:
      enabled: true
      <<: *zurg_addons_vpn
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
      envFrom:
      - configMapRef:
          name: gluetun-config

zurgranger:
  <<: *app_zurg
  podLabels:
    app.elfhosted.com/name: zurg
    app.elfhosted.com/class: dedicated
  podAnnotations:
    kubernetes.io/egress-bandwidth: "500M"
  enabled: false
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-zurg"
  affinity: *dedicated_affinity # force zurg to go onto the dedicated nodes
  resources: *ranger_zurg_resources

plexdebrid: &app_plexdebrid
  enabled: false
  # podLabels:
  #   app.elfhosted.com/name: plexdebrid
  image:
    repository: ghcr.io/elfhosted/plex-debrid
    tag: rolling@sha256:0c0251d2aef532ba62660b719b7e37e72b4eb262ca18e21d68b9509d305e12a1
  podLabels:
    app.elfhosted.com/name: plex-debrid    
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex-debrid"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because of s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
  resources:
    requests:
      cpu: 2m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 3Gi
  ingress:
    main:
      enabled: false
  envFrom:
  - secretRef:
      name: plex-debrid-env
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plex-debrid
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex-debrid
      - mountPath: /tmp
        name: tmp

clidebrid: &app_clidebrid
  enabled: false
  podLabels:
    app.elfhosted.com/name: cli-debrid
  image:
    repository: ghcr.io/elfhosted/cli_debrid-dev
    tag: v0.6.27@sha256:c9be74e0b1123dae270a872fbc70f8906f77c2802ff5baba3c033ffca97d3c26
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-cli-debrid,cli-debrid-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work because of s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
  envFrom:
  - configMapRef:
      name: cli-debrid-env
  - configMapRef:
      name: elfbot-cli-debrid
      optional: true        
  resources:
    requests:
      cpu: 2m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 3Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5000
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /user/
      subPath: cli-debrid
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /user/logs
      subPath: cli-debrid
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-cli-debrid
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: cli-debrid
      - mountPath: /tmp
        name: tmp

puter:
  enabled: false
  podLabels:
    app.elfhosted.com/name: puter
  image:
    repository: ghcr.io/elfhosted/puter
    tag: v2.5.1@sha256:4566493190067df16091134e9fa845ffa21f6e517874eb146cb4bca9c4177ca9
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-puter"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work because of s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
  resources:
    requests:
      cpu: 2m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 3Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 4100
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: puter
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-puter
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: puter
      - mountPath: /tmp
        name: tmp

plexdebriddebridlink: 
  <<: *app_plexdebrid  
  enabled: false
  podLabels:
    app.elfhosted.com/name: plex-debrid-debridlink  
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex-debrid-debridlink"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plex-debrid-debridlink
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid-debridlink
          optional: true          
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex-debrid-debridlink
      - mountPath: /tmp
        name: tmp

plexdebridtorbox: 
  <<: *app_plexdebrid  
  enabled: false  
  podLabels:
    app.elfhosted.com/name: plex-debrid-torbox  
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex-debrid-torbox"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plex-debrid-torbox
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid-torbox
          optional: true     
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex-debrid-torbox
      - mountPath: /tmp
        name: tmp               

plexdebridpremiumize: 
  <<: *app_plexdebrid  
  enabled: false  
  podLabels:
    app.elfhosted.com/name: plex-debrid-premiumize    
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex-debrid-premiumize"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plex-debrid-premiumize
      volumeSpec:
        persistentVolumeClaim:
          claimName: config    
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid-premiumize
          optional: true                     
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex-debrid-premiumize
      - mountPath: /tmp
        name: tmp

# This is a copy of plexdebrid plumbed into the user's VPN
plexdebridalldebrid: 
  <<: *app_plexdebrid
  podLabels:
    app.elfhosted.com/name: plex-debrid-alldebrid    
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex-debrid-alldebrid"  
  enabled: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plex-debrid-alldebrid
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid-alldebrid
          optional: true   
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex-debrid-alldebrid
      - mountPath: /tmp
        name: tmp                 
  addons:
    vpn:
      enabled: true
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
      envFrom:
      - configMapRef:
          name: gluetun-config
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DOT: "off"
        FIREWALL_INPUT_PORTS: "3001" # 9999 is for rclone
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

codeserver:
  enabled: false
  # runtimeClassName: kata
  image:
    repository: ghcr.io/elfhosted/codeserver
    tag: 4.99.2@sha256:e9f000835af34ba840960fa8212fdfd7d31b76e07162dc3edabb8bf77e08acd1
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-codeserver"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because of s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    # runAsUser: 568
    # runAsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 15m
      memory: 200Mi
    limits:
      cpu: 2
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config # no subpath, codeserver wants to see all
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid
          optional: true
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: codeserver-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: codeserver
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        mkdir -p /config/.config/code-server/
        if [ ! -f /config/.config/code-server/config.yaml ];
        then
          cp /bootstrap/config.yaml /config/.config/code-server/
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: codeserver
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true

doplarr: &app_doplarr
  enabled: false
  image:
    repository: ghcr.io/elfhosted/doplarr
    tag: v3.6.3@sha256:7703328fc7f9f4190606ba9f95e867a64db79bd06c19e923a83ad6f939f89097
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-doplarr,doplarr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: elfbot-doplarr
      optional: true

profilarr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/profilarr-frontend
    tag: v1.0.1@sha256:d64425278edd8987c7745f05106e8479656e65c2d4ff3ef304a53bd528911c09
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-profilarr,profilarr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6868
  envFrom:
  - configMapRef:
      name: elfbot-profilarr
      optional: true
  persistence:
    tmp: *tmp
    logs:
      enabled: true
      type: custom
      mountPath: /app/logs
      subPath: profilarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: profilarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-profilarr
          optional: true   
    app:
      enabled: "true"
      type: emptyDir
      mountPath: /app
      volumeSpec:
        medium: Memory           
  additionalContainers:
    backend:
      image: ghcr.io/elfhosted/profilarr-backend:v1.0.1@sha256:3f21a61e761d714a8f1daecf26f656c27236db63aa6ce4e95e894c7dba400064     
      volumeMounts:
      - name: config
        mountPath: /config
        subPath: profilarr
  initContainers:
    setup:
      image: ghcr.io/elfhosted/profilarr-frontend:v1.0.1@sha256:d64425278edd8987c7745f05106e8479656e65c2d4ff3ef304a53bd528911c09
      command:
      - /bin/ash
      - -c
      - |
        # copy the image's build-cache directory into the emptyDir
        cp /app/* /tmp -rfp
      volumeMounts:
      - mountPath: /tmp
        name: app

pulsarr: &app_pulsarr
  enabled: false
  image:
    repository: ghcr.io/elfhosted/pulsarr
    tag: v0.2.14@sha256:e2dd22fe0fda300c786b371adf37dc795abbcdddb10ac5d02a74af2e5fa7cd44
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-pulsarr,pulsarr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  persistence:
    backup: *backup
    tmp: *tmp
    logs:
      enabled: true
      type: custom
      mountPath: /app/data/logs
      subPath: pulsarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs    
    config:
      enabled: true
      type: custom
      mountPath: /app/data
      subPath: pulsarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-pulsarr
          optional: true  
    app:
      enabled: "true"
      type: emptyDir
      mountPath: /app
      volumeSpec:
        medium: Memory                
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3003
  envFrom:
  - configMapRef:
      name: pulsarr-env
  - configMapRef:
      name: elfbot-pulsarr
      optional: true      
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: pulsarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: ghcr.io/elfhosted/pulsarr:v0.2.14@sha256:e2dd22fe0fda300c786b371adf37dc795abbcdddb10ac5d02a74af2e5fa7cd44
      command:
      - /bin/ash
      - -c
      - |
        # copy the image's build-cache directory into the emptyDir
        cp /app/* /tmp -rfp
      volumeMounts:
      - mountPath: /tmp
        name: app
  additionalContainers:
    apprise-api:
      image: ghcr.io/elfhosted/apprise-api:v1.2.0@sha256:ed6e963a2cdd1bdef63c51e82ca7b8bd2549f285f2e0510ea78d6f0adac40588
      volumeMounts:
      - name: config
        mountPath: /config
        subPath: pulsarr/apprise
      env:
        PUID: 568
        PGID: 568
        APPRISE_STATEFUL_MODE: simple
        APPRISE_WORKER_COUNT: "1"

movieroulette:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/movie-roulette
    tag: v4.0@sha256:da7fc78bf83b071172bd83a898c667f418a03579011079d3c466127054f75a81
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-movie-roulette,movie-roulette-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    backup: *backup
    logs:
      enabled: true
      type: custom
      mountPath: /app/logs
      subPath: movie-roulette
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs    
    config:
      enabled: true
      type: custom
      mountPath: /app/data
      subPath: movie-roulette
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-movie-roulette
          optional: true  
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 4000
  envFrom:
  - configMapRef:
      name: movie-roulette-env
  - configMapRef:
      name: elfbot-movie-roulette
      optional: true      
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: movie-roulette
      - mountPath: /tmp
        name: tmp

requestrr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/requestrr
    tag: v2.1.6@sha256:d9daf341af2608f8351ae4cfdb6f685c1ea675e18b88860d0bfbc6343202402b
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-requestrr,requestrr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /app/config
      subPath: requestrr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-requestrr
          optional: true
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 4545
  envFrom:
  - configMapRef:
      name: elfbot-requestrr
      optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: requestrr
      - mountPath: /tmp
        name: tmp

debridav: &app_debridav
  enabled: false
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: debridav  
  image:
    repository: ghcr.io/elfhosted/debridav
    tag: 0.9.0@sha256:407db0474433e23fbefadef1ba2ea5c5da0ad89874a0be9b61ac6992b16b27b5
  imagePullSecrets:
  - name: ghcr-io-elfhosted    
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-debridav,debridav-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    tmp: *tmp
    backup: *backup
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory    
    config:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-debridav
          optional: true    
    import:
      enabled: true
      type: emptyDir
      mountPath: /import
      sizeLimit: 1Gi                   
  envFrom:
  - configMapRef:
      name: debridav-env
  - configMapRef:
      name: elfbot-debridav
      optional: true      
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080      
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /actuator/health/liveness
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /actuator/health/readiness
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10         
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 2Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: debridav
      - mountPath: /tmp
        name: tmp  
    # Only trigger this when moving to database
    setup-postgres:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/database
        mkdir -p /config/backups
        chown elfie:elfie /config/database -R
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: debridav
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault                
  additionalContainers:
    make-folders:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |

        # Define the folders to check/create
        FOLDERS=("/storage/debridav/movies" "/storage/debridav/movies-4k" "/storage/debridav/series" "/storage/debridav/series-4k")

        # Function to check and create folders
        create_folders() {
          local all_created=true
          
          for folder in "${FOLDERS[@]}"; do
            if [ ! -d "$folder" ]; then
              echo "Folder $folder does not exist. Creating..."
              
              # sleep so that we have time to avoid a race on local vs dbfolders
              sleep 20s
              mkdir -p "$folder"
              
              # Check if creation was successful
              if [ ! -d "$folder" ]; then
                echo "Error creating $folder. Will retry in 10 seconds."
                all_created=false
              else
                echo "Successfully created $folder."
              fi
            else
              echo "Folder $folder already exists."
            fi
          done
          
          return $([ "$all_created" = true ] && echo 0 || echo 1)
        }

        # Main loop to create folders
        while true; do
          if create_folders; then
            echo "All required folders exist or were successfully created."
            break
          else
            echo "Waiting 10 seconds before retrying..."
            sleep 10
          fi
        done

        echo "Success! All required folders are now available."
        echo "Script will now sleep indefinitely."

        # Sleep forever
        while true; do
          sleep 3600  # Sleep for an hour at a time (to be a bit nicer to the system than an infinite tight loop)
        done
      volumeMounts:
      - name: rclonemountdebridav
        mountPath: /storage/debridav
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310
      env:
        - name: POSTGRES_PASSWORD
          value: debridav
        - name: POSTGRES_DB
          value: debridav
        - name: POSTGRES_USER
          value: debridav
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: debridav/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi
    fakearr:
      image: ghcr.io/elfhosted/fakearr:rolling@sha256:db534345e7401c41310f2a1a333e72490e4afe8508a9b4df3b4916bb2c2bdced
      envFrom:
      - configMapRef:
          name: elfbot-debridav
          optional: true  
      volumeMounts:
      - name: tmp
        mountPath: /tmp    
          
rclonedebridlink:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/davdebrid
    tag: v1.2.2@sha256:6571776a022a36889d1cc8392440ad58fa2654535e82b9741f1c287e045c7fd0
  priorityClassName: tenant-normal
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podLabels:
    app.elfhosted.com/name: debridlink
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-debridlink,debridlink-env"
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
    readOnlyRootFilesystem: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: debridlink
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  envFrom:
  - configMapRef:
      name: debridlink-env
  - configMapRef:
      name: elfbot-debridlink
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 100Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080


rclonealldebrid:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.69.1@sha256:c89e34f0e1032d7ab7f4a927065dfb71856d37ec584bbe4750075f157f9de8ba
  command:
  - /debrid-provider.sh
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-alldebrid,alldebrid-config,gluetun-config"
  podAnnotations:
    kubernetes.io/egress-bandwidth: "128M"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    cache:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      sizeLimit: 1Gi
    config: 
      enabled: "true"
      type: emptyDir
      mountPath: /config
    bootstrap:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: alldebrid-config
    tinyproxy-conf:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: alldebrid-tinyproxy-conf 
  additionalContainers:
    # Use this to provied proxied access to mediaflowproxy
    tinyproxy:
      image: docker.io/kalaksi/tinyproxy
      volumeMounts:
      - name: tinyproxy-conf
        mountPath: /etc/tinyproxy/tinyproxy.conf
        subPath: tinyproxy.conf                      
  initContainers:
    setup:
      image: ghcr.io/elfhosted/rclone:1.69.1@sha256:c89e34f0e1032d7ab7f4a927065dfb71856d37ec584bbe4750075f157f9de8ba
      command:
      - /bin/ash
      - -c
      - |
        set -x

        # Create directory structure

        OBSCURED_PASS=$(rclone obscure doesntmatter)
        cp /bootstrap/rclone-debrid-provider.conf /config/
        sed -i "s/REPLACEUSER/$USER/" /config/rclone-debrid-provider.conf
        sed -i "s/REPLACEPASS/$OBSCURED_PASS/" /config/rclone-debrid-provider.conf
        
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /bootstrap
        name: bootstrap
      resources: *default_resources
      securityContext: *default_securitycontext
      envFrom:
      - configMapRef:
          name: elfbot-alldebrid
          optional: true
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9999      
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi
  addons:
    vpn:
      enabled: true
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
      envFrom:
      - configMapRef:
          name: gluetun-config
      env:
        DOT: "off"
        FIREWALL_INPUT_PORTS: "9999,8888" # 9999 is for rclone, 8888 is tinyproxy
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8,192.168.0.0/16,172.16.0.0/20
        DNS_KEEP_NAMESERVER: "on"        
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

rclonepremiumize:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.69.1@sha256:c89e34f0e1032d7ab7f4a927065dfb71856d37ec584bbe4750075f157f9de8ba
  command:
  - /debrid-provider.sh
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-premiumize,premiumize-config"
  podAnnotations:
    kubernetes.io/egress-bandwidth: "128M"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    cache:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      sizeLimit: 1Gi
    config: 
      enabled: "true"
      type: emptyDir
      mountPath: /config
    bootstrap:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: premiumize-config
  initContainers:
    setup:
      image: ghcr.io/elfhosted/rclone:1.69.1@sha256:c89e34f0e1032d7ab7f4a927065dfb71856d37ec584bbe4750075f157f9de8ba
      command:
      - /bin/ash
      - -c
      - |
        set -x

        # Create directory structure

        OBSCURED_PASS=$(rclone obscure "$PASS")
        cp /bootstrap/rclone-debrid-provider.conf /config/
        sed -i "s/REPLACEUSER/$USER/" /config/rclone-debrid-provider.conf
        sed -i "s/REPLACEPASS/$OBSCURED_PASS/" /config/rclone-debrid-provider.conf
        
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /bootstrap
        name: bootstrap
      resources: *default_resources
      securityContext: *default_securitycontext
      envFrom:
      - configMapRef:
          name: elfbot-premiumize
          optional: true
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9999      
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi

rclonetorbox:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.69.1@sha256:c89e34f0e1032d7ab7f4a927065dfb71856d37ec584bbe4750075f157f9de8ba
  command:
  - /debrid-provider.sh
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-torbox,torbox-config"
  podAnnotations:
    kubernetes.io/egress-bandwidth: "128M"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    cache:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      sizeLimit: 1Gi
    # we'll run the obscure command and copy the config into here
    config: 
      enabled: "true"
      type: emptyDir
      mountPath: /config
    bootstrap:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: torbox-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-torbox
          optional: true          
  initContainers:
    setup:
      image: ghcr.io/elfhosted/rclone:1.69.1@sha256:c89e34f0e1032d7ab7f4a927065dfb71856d37ec584bbe4750075f157f9de8ba
      command:
      - /bin/ash
      - -c
      - |
        set -x

        # Create directory structure

        OBSCURED_PASS=$(rclone obscure "$PASS")
        cp /bootstrap/rclone-debrid-provider.conf /config/
        sed -i "s/REPLACEUSER/$USER/" /config/rclone-debrid-provider.conf
        sed -i "s/REPLACEPASS/$OBSCURED_PASS/" /config/rclone-debrid-provider.conf
        
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /bootstrap
        name: bootstrap
      - mountPath: /elfbot
        name: elfbot
      resources: *default_resources
      securityContext: *default_securitycontext
      envFrom:
      - configMapRef:
          name: elfbot-torbox
          optional: true

  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9999
  envFrom:
  - configMapRef:
      name: elfbot-torbox
      optional: true              
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi

decypharr: &app_decypharr
  enabled: false
  image:
    repository: ghcr.io/elfhosted/decypharr
    tag: v0.5.2@sha256:aabf43e851d55a387cb53e9861111d494428cc52de395a095cb602ff8c041e53
  priorityClassName: tenant-normal
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-decypharr,decypharr-example-config"
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
    readOnlyRootFilesystem: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8282
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: decypharr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /app/logs
      subPath: decypharr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs    
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: decypharr-example-config                
  initContainers:
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e
        # If we don't already have an example config, create one
        if [ ! -f /config/config.json ];
        then
          cp /bootstrap/config.json /config/
        fi
      volumeMounts:
      - mountPath: /config/
        name: config
        subPath: decypharr
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext: *default_securitycontext       

blackhole: &app_blackhole
  enabled: false
  image:
    repository: ghcr.io/elfhosted/wests-blackhole-script
    tag: v1.5.1@sha256:3f7a6e09de005b57fec75762422c0880286a8c4d93a9871b3df1e88866fc8dc1
  priorityClassName: tenant-normal
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-blackhole,blackhole-env"
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
    readOnlyRootFilesystem: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  env:
    BLACKHOLE_RADARR_PATH: "radarr"
    BLACKHOLE_SONARR_PATH: "sonarr"
  envFrom:
  - configMapRef:
      name: blackhole-env
  - configMapRef:
      name: elfbot-blackhole
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 100Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: blackhole
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs

blackhole4k:
  <<: *app_blackhole
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: blackhole4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
  env:
    BLACKHOLE_RADARR_PATH: "radarr4k"
    BLACKHOLE_SONARR_PATH: "sonarr4k"
  envFrom:
  - configMapRef:
      name: blackhole-env
  - configMapRef:
      name: elfbot-blackhole
      optional: true   
  - configMapRef:
      name: elfbot-blackhole4k
      optional: true    

blackholetorbox:
  <<: *app_blackhole
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-blackholetorbox,blackholetorbox-env"
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: blackholetorbox
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
  env:
    BLACKHOLE_RADARR_PATH: "radarr"
    BLACKHOLE_SONARR_PATH: "sonarr"
  envFrom:
  - configMapRef:
      name: blackholetorbox-env
  - configMapRef:
      name: elfbot-blackholetorbox
      optional: true
  initContainers:
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x

        # Create directory structure
        mkdir -p /storage/symlinks/blackholetorbox/radarr/completed
        mkdir -p /storage/symlinks/blackholetorbox/radarr/processing
        mkdir -p /storage/symlinks/blackholetorbox/sonarr/completed
        mkdir -p /storage/symlinks/blackholetorbox/sonarr/processing
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      resources: *default_resources
      securityContext: *default_securitycontext

blackholetorbox4k:
  <<: *app_blackhole
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-blackholetorbox,blackholetorbox-env"
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: blackholetorbox4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
  env:
    BLACKHOLE_RADARR_PATH: "radarr4k"
    BLACKHOLE_SONARR_PATH: "sonarr4k"
  envFrom:
  - configMapRef:
      name: blackholetorbox-env
  - configMapRef:
      name: elfbot-blackholetorbox
      optional: true
  - configMapRef:
      name: elfbot-blackholetorbox4k
      optional: true      

channelsdvr:
  enabled: false
  image:
    repository: fancybits/channels-dvr
    tag: latest@sha256:284fed6f4ee5150d41d9a7f247a63e190f6f1c3a4e4bc740f029df6d36955d29
  priorityClassName: tenant-normal
  podAnnotations:
    kubernetes.io/egress-bandwidth: "125M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-channelsdvr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 15m
      memory: 200Mi
    limits:
      cpu: 1
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8089
  persistence:
    <<: *storagemounts
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /channels-dvr
      subPath: channelsdvr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-channelsdvr
          optional: true
  initContainers:
    bootstrap: *bootstrap


immich:
  enabled: false
  image:
    repository: ghcr.io/immich-app/immich-server
    tag: v1.131.3@sha256:7e5b6729b12b5e5cc5d98bcc6f7c27f723fabae4ee77696855808ebd5200bbf8
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-immich,immich-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 15m
      memory: 200Mi
    limits:
      cpu: 500m
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 2283
  persistence:
    <<: *storagemounts
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    tmp: *tmp
    config:
      enabled: true
      type: custom
      subPath: immich
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    upload:
      enabled: true
      type: emptyDir
      mountPath: /usr/src/app/upload
      sizeLimit: 1Gi
    upload-encoded:
      enabled: true
      type: emptyDir
      mountPath: /usr/src/app/upload/encoded-video
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-immich
          optional: true
  envFrom:
  - configMapRef:
      name: immich-env
  - configMapRef:
      name: elfbot-immich
      optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: immich
      - mountPath: /tmp
        name: tmp
  additionalContainers:
    ml:
      image: ghcr.io/immich-app/immich-machine-learning:v1.131.2
      envFrom:
      - configMapRef:
          name: immich-env
      resources:
        requests:
          cpu: 15m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 4Gi
    database:
      image: docker.io/tensorchord/pgvecto-rs:pg14-v0.2.0@sha256:739cdd626151ff1f796dc95a6591b55a714f341c737e27f045019ceabf8e8c52
      env:
        POSTGRES_INITDB_ARGS: '--data-checksums'
        POSTGRES_PASSWORD: immich
        POSTGRES_USER: immich
        POSTGRES_DB: immich
      volumeMounts:
        - name: config
          subPath: immich/database
          mountPath: /var/lib/postgresql/data
      resources:
        requests:
          cpu: 15m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 4Gi
    redis:
      image: docker.io/redis:7.4-alpine@sha256:02419de7eddf55aa5bcf49efb74e88fa8d931b4d77c07eff8a6b2144472b6952
      envFrom:
      - configMapRef:
          name: immich-env
      resources:
        requests:
          cpu: 15m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 4Gi

kubernetesdashboard:

  ## Name of Priority Class of pods
  priorityClassName: "tenant-normal"

  ## Pod resource requests & limits
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 256Mi

  extraArgs:
    - --enable-skip-login
    - --enable-insecure-login
    - --system-banner=Built</A> with ❤️ by <A HREF="https://funkypenguin.co.nz">@funkypenguin</A> and friends (<I><A HREF="https://chat.funkypenguin.co.nz">join us!</A></I>)

  ## Serve application over HTTP without TLS
  ##
  ## Note: If set to true, you may want to add --enable-insecure-login to extraArgs
  protocolHttp: true

  # Global dashboard settings
  settings:
    ## Cluster name that appears in the browser window title if it is set
    clusterName: "ElfHosted"
    # defaultNamespace: "{{ .Release.Namespace }}"
    # namespaceFallbackList: [ "{{ .Release.Namespace }}" ]

    ## Max number of items that can be displayed on each list page
    itemsPerPage: 10
    ## Number of seconds between every auto-refresh of logs
    logsAutoRefreshTimeInterval: 5
    ## Number of seconds between every auto-refresh of every resource. Set 0 to disable
    resourceAutoRefreshTimeInterval: 5
    ## Hide all access denied warnings in the notification panel
    disableAccessDeniedNotifications: true

  ## Metrics Scraper
  ## Container to scrape, store, and retrieve a window of time from the Metrics Server.
  ## refs: https://github.com/kubernetes-sigs/dashboard-metrics-scraper
  metricsScraper:
    ## Wether to enable dashboard-metrics-scraper
    enabled: true
    image:
      repository: kubernetesui/metrics-scraper
      tag: v1.0.9
    resources: {}
    ## SecurityContext especially for the kubernetes dashboard metrics scraper container
    ## If not set, the global containterSecurityContext values will define these values
    # containerSecurityContext:
    #   allowPrivilegeEscalation: false
    #   readOnlyRootFilesystem: true
    #   runAsUser: 1001
    #   runAsGroup: 2001
  #  args:
  #    - --log-level=info
  #    - --logtostderr=true

  # Don't auto-create RBAC for us, we'll do it manually
  rbac:
    create: false

  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: kubernetes-dashboard

# optional but disabled by default to prevent errors


gluetun:
  enabled: false # just to avoid errors
cometproxystreaming:
  enabled: false
mediafusionproxystreaming:
  enabled: false
elfassesment:
  enabled: false

# The hobbit apps
zurghobbit:
  <<: *app_zurg
  podAnnotations: *hobbit_streamer_podAnnotations
  # zurg is the "anchor" which keeps all the other apps on the same node
  affinity: *dedicated_affinity # force zurg to go onto the dedicated nodes
  resources: *hobbit_zurg_resources

zurghalfling:
  <<: *app_zurg
  podAnnotations: *halfling_streamer_podAnnotations
  # zurg is the "anchor" which keeps all the other apps on the same node
  affinity: *dedicated_affinity # force zurg to go onto the dedicated nodes
  resources: *halfling_zurg_resources

zurgnazgul:
  <<: *app_zurg
  podAnnotations: *nazgul_streamer_podAnnotations
  # zurg is the "anchor" which keeps all the other apps on the same node
  affinity: *dedicated_affinity # force zurg to go onto the dedicated nodes
  resources: *nazgul_zurg_resources

plexhobbit:
  <<: *app_plex
  podAnnotations: *hobbit_streamer_podAnnotations
  resources: *hobbit_streamer_resources
  persistence:
    <<: *app_plex_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

jellyfinhobbit:
  <<: *app_jellyfin
  podAnnotations: *hobbit_streamer_podAnnotations
  resources: *hobbit_streamer_resources
  persistence:
    <<: *app_jellyfin_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

embyhobbit:
  <<: *app_emby
  podAnnotations: *hobbit_streamer_podAnnotations
  resources: *hobbit_streamer_resources
  persistence:
    <<: *app_emby_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

plexranger:
  <<: *app_plex
  persistence:
    <<: *app_plex_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g
  podAnnotations: *ranger_streamer_podAnnotations
  resources: *ranger_streamer_resources

plexhalfling:
  <<: *app_plex
  persistence:
    <<: *app_plex_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  
  podAnnotations: *halfling_streamer_podAnnotations
  resources: *halfling_streamer_resources

jellyfinhalfling:
  <<: *app_jellyfin
  podAnnotations: *halfling_streamer_podAnnotations
  resources: *halfling_streamer_resources
  persistence:
    <<: *app_jellyfin_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: jellygfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

embyhalfling:
  <<: *app_emby
  podAnnotations: *halfling_streamer_podAnnotations
  resources: *halfling_streamer_resources
  persistence:
    <<: *app_emby_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

plexnazgul:
  <<: *app_plex
  persistence:
    <<: *app_plex_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  
  podAnnotations: *nazgul_streamer_podAnnotations
  resources: *nazgul_streamer_resources

jellyfinnazgul:
  <<: *app_jellyfin
  podAnnotations: *nazgul_streamer_podAnnotations
  resources: *nazgul_streamer_resources
  persistence:
    <<: *app_jellyfin_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

embynazgul:
  <<: *app_emby
  podAnnotations: *nazgul_streamer_podAnnotations
  resources: *nazgul_streamer_resources
  persistence:
    <<: *app_emby_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

# We use these to set aside resources for dedicated bundles
starter: &app_resource_reserver
  enabled: false
  image:
    repository: ghcr.io/elfhosted/tooling
    tag: focal-20240530@sha256:458d1f3b54e9455b5cdad3c341d6853a6fdd75ac3f1120931ca3c09ac4b588de
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "tooling-scripts" # Reload the deployment every time the yaml config changes
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  # we mount config to force the resource-reserver to run on the same node as the volumes
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config    
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755          
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  command:
  - /bin/bash
  - -c
  - |
    echo "This pod simple reserves contended resources for a tenant"

    sleep infinity
  affinity: *standard_affinity
  service:
    main:
      enabled: false
  probes:
    liveness:
      enabled: false
    startup:
      enabled: false
    readiness:
      enabled: false      
  resources:
    requests: 
      cpu: 250m
      memory: 1Mi
  initContainers:
    update-dns:  &update_dns_on_init
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - /tooling-scripts/update-dns-on-init.sh
      env:
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: ELF_TENANT_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['app.kubernetes.io/instance']
      envFrom:
      - secretRef:
          name: cloudflare-api-token
      volumeMounts:
      - mountPath: /tooling-scripts
        name: tooling-scripts
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
  additionalContainers:
    clean-up-dns:
      <<: *update_dns_on_init
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - /tooling-scripts/clean-up-dns-on-termination.sh      

streamer:
  enabled: false
  <<: *app_resource_reserver

hobbit:
  enabled: false
  <<: *app_resource_reserver

ranger:
  enabled: false
  <<: *app_resource_reserver

halfling:
  enabled: false
  <<: *app_resource_reserver

nazgul:
  enabled: false
  <<: *app_resource_reserver

# This file must end on a single newline
