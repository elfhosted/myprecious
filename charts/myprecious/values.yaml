# This controls whether our automation will auto-release this to stable during the daily maint window
safeToRelease: true

# false by default means the FSN cluster (so no migrations)
location:
  enabled: false
zurgling:
  enabled: false
debridling:
  enabled: false
sidekick:
  enabled: false
decypharrreplacezurg:
  enabled: false

# false by default
volsync:
  enabled: false
  restic_repository:
  restic_password:
  aws_access_key_id:
  aws_secret_access_key:

# Set these to the default if nothing else is set
storageclass:
  rwx:
    name: ceph-filesystem-ssd
    accessMode: ReadWriteMany
    volumeSnapshotClassName: ceph-filesystem
  rwo:
    name: ceph-block-ssd
    accessMode: ReadWriteOnce
    volumeSnapshotClassName: ceph-block

# These control the egress bandwidth of the semi-dedi products
hobbit_streamer_podAnnotations: &hobbit_streamer_podAnnotations
  kubernetes.io/egress-bandwidth: "250M"
ranger_streamer_podAnnotations: &ranger_streamer_podAnnotations
  kubernetes.io/egress-bandwidth: "500M"
halfling_streamer_podAnnotations: &halfling_streamer_podAnnotations
  kubernetes.io/egress-bandwidth: "1000M"
nazgul_streamer_podAnnotations: &nazgul_streamer_podAnnotations
  kubernetes.io/egress-bandwidth: "1000M"

# These control the requests used to "anchor" a stack to a particular dedicated node. The following defaults can be overridden on a per-cluster basis:
hobbit_zurg_resources: &hobbit_zurg_resources
  requests:
    cpu: "1.8"
    memory: 30Mi
  limits:
    cpu: "2"
    memory: 4Gi

ranger_zurg_resources: &ranger_zurg_resources
  requests:
    cpu: "3500m"
    memory: 30Mi
  limits:
    cpu: "4"
    memory: 4Gi

halfling_zurg_resources: &halfling_zurg_resources
  requests:
    cpu: "7"
    memory: 30Mi
  limits:
    cpu: "8"
    memory: 4Gi

nazgul_zurg_resources: &nazgul_zurg_resources
  requests:
    cpu: "7"
    memory: 30Mi
  limits:
    cpu: "16"
    memory: 4Gi


# These allow us to manage RAM usage on streamers
hobbit_streamer_resources: &hobbit_streamer_resources
  requests:
    cpu: "10m"
    memory: 30Mi
  limits:
    cpu: "2"
    memory: 4Gi

ranger_streamer_resources: &ranger_streamer_resources
  requests:
    cpu: 10m
    memory: 30Mi
  limits:
    cpu: 4
    memory: 4Gi

# Giving more than 4 CPU to a streamer is unwise regardless
halfling_streamer_resources: &halfling_streamer_resources
  requests:
    cpu: 10m
    memory: 30Mi
  limits:
    cpu: 4
    memory: 4Gi

nazgul_streamer_resources: &nazgul_streamer_resources
  requests:
    cpu: 10m
    memory: 30Mi
  limits:
    cpu: 4
    memory: 4Gi

# sets the user's base dns domain
dns_domain: elfhosted.com

tooling_image: &tooling_image ghcr.io/elfhosted/tooling:focal-20250404@sha256:72afc01e33e762b560f8846909b1e91ec222259e6749898175e9b2772a6cc3ff

# all RD pods have to exist with zurg - make this soft for now
standard_affinity: &standard_affinity
  podAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app.elfhosted.com/role
          operator: In
          values:
          - nodefinder # use nodefinder in the absence of zurg...
      topologyKey: "kubernetes.io/hostname"

standard_anti_affinity: &standard_anti_affinity
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app.elfhosted.com/role
          operator: In
          values:
          - nodefinder
      topologyKey: "kubernetes.io/hostname"

dedicated_affinity: &dedicated_affinity
  podAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app.elfhosted.com/role
          operator: In
          values:
          - nodefinder # use nodefinder in the absence of zurg...
      topologyKey: "kubernetes.io/hostname"

standard_tolerations: &standard_tolerations

hobbit_tolerations: &hobbit_tolerations

# Set minimal requests so that pods can co-exist with streamers
hobbit_resources: &hobbit_resources
  requests:
    cpu: "1m"
    memory: "16Mi"
  limits:
    cpu: "1"
    memory: 4Gi

ranger_resources: &ranger_resources
  requests:
    cpu: "1m"
    memory: "16Mi"
  limits:
    cpu: "2"
    memory: 8Gi

volumespec_ephemeral_volume_100g: &volumespec_ephemeral_volume_100g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 100Gi

volumespec_ephemeral_volume_50g: &volumespec_ephemeral_volume_50g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 50Gi

# And this makes the media / rclone mounts tidier.
rclonemountrealdebridzurg: &rclonemountrealdebridzurg
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: realdebrid-zurg
  mountPath: /storage/realdebrid-zurg
rclonemountdebridlink: &rclonemountdebridlink
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: debridlink
  mountPath: /storage/debridlink
rclonemountalldebrid: &rclonemountalldebrid
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: alldebrid
  mountPath: /storage/alldebrid
rclonemountpremiumize: &rclonemountpremiumize
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: premiumize
  mountPath: /storage/premiumize
rclonemounttorbox: &rclonemounttorbox
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: torbox
  mountPath: /storage/torbox
rclonemountdebridav: &rclonemountdebridav
  enabled: false 
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: debridav
  mountPath: /storage/debridav    
rclone: &rclone
  enabled: true # everyone gets an rclone mount
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: rclone
  mountPath: /storage/rclone  

# This simplfies the process of adding all the optional mounts to every app
storagemounts: &storagemounts
  rclone: *rclone
  rclonemountrealdebridzurg: *rclonemountrealdebridzurg
  rclonemountdebridlink: *rclonemountdebridlink
  rclonemountalldebrid: *rclonemountalldebrid
  rclonemountpremiumize: *rclonemountpremiumize
  rclonemounttorbox: *rclonemounttorbox
  rclonemountdebridav: *rclonemountdebridav
  tmp: &tmp
    enabled: true
    type: emptyDir
    mountPath: /tmp
  symlinks: &symlinks
    enabled: true
    type: custom
    volumeSpec:
      persistentVolumeClaim:
        claimName: symlinks
    mountPath: /storage/symlinks
  backup: &backup
    enabled: true
    type: custom
    volumeSpec:
      persistentVolumeClaim:
        claimName: backup

# The entire bootstrap sidecar/additionalcontainer
default_resources: &default_resources
  requests:
    cpu: 0m
    memory: 1Mi
    # ephemeral-storage: 50Mi
  limits:
    cpu: 1
    memory: 4Gi # just a safety net against bugs!
    # ephemeral-storage: 2Gi # a safety net against node ephemeral space exhaustion

default_securitycontext: &default_securitycontext
  seccompProfile:
    type: RuntimeDefault
  readOnlyRootFilesystem: true
  allowPrivilegeEscalation: false
  runAsUser: 568
  runAsGroup: 568
  capabilities:
    drop:
    - ALL

speedtest_securitycontext: &speedtest_securitycontext
  seccompProfile:
    type: RuntimeDefault
  readOnlyRootFilesystem: false
  allowPrivilegeEscalation: false
  runAsUser: 101
  runAsGroup: 101
  capabilities:
    drop:
    - ALL

# We use this to provide env not only to bootstrap, but also to the torrent clients which use elfvpn
# it's necessary since the wireguard configs are in S3
bootstrap_env: &bootstrap_env
- name: AWS_ACCESS_KEY_ID
  valueFrom:
    secretKeyRef:
      key: access-key-id
      name: b2-elfhosted-config-ro
- name: AWS_SECRET_ACCESS_KEY
  valueFrom:
    secretKeyRef:
      key: secret-key
      name: b2-elfhosted-config-ro
- name: S3_ENDPOINT_URL
  value: https://s3.us-west-000.backblazeb2.com
- name: K8S_APP_NAME
  valueFrom:
    fieldRef:
      fieldPath: metadata.labels['app.kubernetes.io/name']
- name: ELF_APP_NAME
  valueFrom:
    fieldRef:
      fieldPath: metadata.labels['app.elfhosted.com/name']

migrate_data: &migrate_data
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /bin/bash
  - -c
  - |

    if [[ ! -f /config/.migrated-20241007 ]]
    then
      if [[ ! -z "$(ls -A /migration)" ]]
      then
        echo "Migrating from /migration/..."
        cp -rfpv /migration/* /config/
        touch /config/.migrated-20241007
      fi
    else
      echo "No migration necessary"
    fi

  volumeMounts:
  - mountPath: /config
    name: config
  - mountPath: /migration
    name: migration

  env: *bootstrap_env
  resources: *default_resources
  securityContext: *default_securitycontext

bootstrap: &bootstrap
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /bin/bash
  - -c
  - |
    set -e

    # Allows us to use app.elfhosted.com/name, but fall back to app.kubernetes.io/name if the former doesn't exist
    if [[ -z "$ELF_APP_NAME" ]]; then
      ELF_APP_NAME=$K8S_APP_NAME
    fi

    # look for commands - we match specific names in order of least-destructive
    TIMESTAMP_NOW=$(date +%s)
    if [[ -f /etc/elfbot/pause ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/pause)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 300 ]]; then
        COMMAND=pause
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/backup && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/backup)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 300 ]]; then
        COMMAND=backup
      fi
    fi


    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/reset && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/reset)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 300 ]]; then
        COMMAND=reset
      fi
    fi

    case $COMMAND in

      "pause")
        echo "Recent pause command found, sleeping 5m.."
        sleep 300
        ;;

      "reset")

        # Safety check - if /config/homer exists, we've not mounted /config properly and are about to wipe out EVERYTHING
        if [ -e "/config/homer" ]; then
            echo "The path /config/homer exists. Exiting."
            exit 1
        fi      
        echo "Recent reset command found, resetting"
        rm -rf /config/*
        ;;

      "backup")
        echo "Recent backup command found, backing up to /storage/backup/${ELF_APP_NAME}-${TIMESTAMP}"
        TIMESTAMP=$(printf '%(%Y-%m-%d--%H-%M)T\n' -1)
        cp -rfp /config /storage/backup/$ELF_APP_NAME-$TIMESTAMP
        ;;


    esac

    if [[ ! -f /config/i-am-bootstrapped ]]
    then
      echo "Bootstrapping from goldilocks config..."
      s5cmd sync s3://elfhosted-config/goldilocks/$ELF_APP_NAME/* /config/
      touch /config/i-am-bootstrapped
    fi

  volumeMounts:
  - mountPath: /etc/elfbot
    name: elfbot
  - mountPath: /config
    name: config
  - mountPath: /storage/backup
    name: backup
  - mountPath: /tmp
    name: tmp
  env: *bootstrap_env
  resources: *default_resources
  securityContext: *default_securitycontext

# provide a default
userId: 1

# our VPN loadbalancerIP
torrentLoadBalancerIP: 10.0.42.101

# these are the "exposed" services which allow users to override SSO
# by themselves, they do nothing, but they allow us to selectively disable
# SSO on ingressroutes, or to use non-standard API keys in Homer
radarrexposed:
  enabled: false
  apikey: 041776c8d5f74bf295aa486d9d51c33a
radarr4kexposed:
  enabled: false
  apikey: 7da5d4ba79804527b78a78b68c7a0781
sonarrexposed:
  enabled: false
  apikey: a6f1c7d07fab4be49c5c1cb545f85a76
sonarr4kexposed:
  enabled: false
  apikey: e4f93c115169484bbed19821f7ac8e49
lidarrexposed:
  enabled: false
  apikey: 0e68e28531a249659737513d3102bfe9
readarrexposed:
  enabled: false
  apikey: 74b033ff59964011b8a32c014fdb9b68
readarraudioexposed:
  enabled: false
  apikey: 8496cefe2c6b46ee921e18caddf6a943
prowlarrexposed:
  enabled: false
  apikey: c53bc3bd17c645c3a457e5342a02cd66
bazarrexposed:
  enabled: false
  apikey: 94ab8212a12378fa5333cbf75a3c0390
bazarr4kexposed:
  enabled: false
  apikey: 393bda5f898886a2b87413e6452313af
qbittorrentexposed:
  enabled: false
rdtclientexposed:
  enabled: false
rutorrentexposed:
  enabled: false
sabnzbdexposed:
  enabled: false
  apikey: 8flkbru7ncdps3dzzgk48q2msz41m4on
nzbgetexposed:
  enabled: false
mylarrexposed:
  enabled: false
  apikey: 0f97f6a7f352c63eb43fcb7e53ea9d8f
rivenexposed:
  enabled: false
  apikey: 1nMZNC0Cg6UP7sblvFirUi9Sad4ga84u  
tunarrexposed:
  enabled: false
cometexposed:
  enabled: false  
aiostreamsexposed:
  enabled: false    
davioexposed:
  enabled: false  
jackettioexposed:
  enabled: false  
knightcrawlerexposed:
  enabled: false  
mediafusionexposed:
  enabled: false
stremthruexposed:
  enabled: false    
stremiojackettexposed:
  enabled: false  
youriptvexposed:
  enabled: false
threadfinexposed:
  enabled: false
dispatcharrexposed:
  enabled: false  
zurgexposed:
  enabled: false
elfassessment:
  enabled: false
uptimekumacustomdomain:
  enabled: false
mattermostcustomdomain:
  enabled: false
vaultwardencustomdomain:
  enabled: false
jellyseerrcustomdomain:
  enabled: false
overseerrcustomdomain:
  enabled: false
plexcustomdomain:
  enabled: false
jellyfincustomdomain:
  enabled: false
embycustomdomain:
  enabled: false
flixiocustomdomain:
  enabled: false
pairdropcustomdomain:
  enabled: false
gotosocialcustomdomain:
  enabled: false  
blueskypdscustomdomain:
  enabled: false    
peertubecustomdomain:
  enabled: false     
actualcustomdomain:
  enabled: false        

rutorrentgluetun: &rutorrent
  enabled: false
  automountServiceAccountToken: false
  image:
      repository: ghcr.io/elfhosted/rutorrent
      tag: 5.1.7-7.2@sha256:b5138140095c158f72e386fd7284add7e3b5349a251a5594666bd7a62b9a75a8
  podLabels:
    app.elfhosted.com/name: rutorrent
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,rutorrent-config,rutorrent-gluetun-config,elfbot-rutorrent" # Reload the deployment every time the yaml config changes
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    # runAsUser: 568 # enforced in env vars
    # runAsGroup: 568
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  envFrom:
  - configMapRef:
      name: elfbot-rutorrent
      optional: true
  # we need the injected initcontainer to run as root, so we can't change the pod-level uid/gid
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 568 # s6's fault
    # runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  persistence:
    <<: *storagemounts
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_100g
    config:
      enabled: true
      type: custom
      mountPath: /data/rtorrent/
      subPath: rutorrent
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rutorrent
          optional: true
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    port-range: # Used for dynamic port-forwarding
      enabled: true
      type: emptyDir
      mountPath: /port-range
      sizeLimit: 1Gi
    custom-rtlocal:
      enabled: "true"
      mountPath: "/.rtlocal.rc-elfhosted"
      subPath: ".rtlocal.rc-elfhosted"
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
    custom-rtorrentrc:
      enabled: "true"
      mountPath: "/.rtorrent.rc-elfhosted"
      subPath: ".rtorrent.rc-elfhosted"
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
    custom-s6-init-05:
      enabled: "true"
      mountPath: "/etc/cont-init.d/05-apply-elfhosted-config.sh"
      subPath: "05-apply-elfhosted-config.sh"
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
          defaultMode: 0755
    custom-s6-init-06:
      enabled: "true"
      mountPath: "/etc/cont-init.d/02-wait-for-vpn.sh"
      subPath: "02-wait-for-vpn.sh"
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
          defaultMode: 0755
    custom-s6-init-07:
      enabled: "true"
      mountPath: "/etc/cont-init.d/03-set-inbound-port.sh"
      subPath: "03-set-inbound-port.sh "
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
          defaultMode: 0755
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: false # necessary for probes, but probes aren't working with vpn addon currently
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1200Mi
  env:
    # -- Set the container timezone
    PUID: 568
    GUID: 568
    RUTORRENT_PORT: 8080 # necessary for health checks
    # S6_READ_ONLY_ROOT: 1 # this seems to break rutorrent :(
    WAIT_FOR_VPN: "true"
    PORT_FILE: /data/rtorrent/forwarded-port
    WAN_IP_CMD: 'curl -s ifconfig.me'
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rutorrent
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi
      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext: *default_securitycontext
      resources: *default_resources
      envFrom:
      - configMapRef:
          name: rutorrent-gluetun-config
  addons:
    vpn: &rutorrent_addons_vpn
      enabled: true
      type: gluetun
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      envFrom:
      - configMapRef:
          name: rutorrent-gluetun-config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
        subPath: rutorrent
      config: # We have to set this to null so that we can override with our own config

      # The scripts that get run when the VPN connection opens/closes are defined here.
      # The default scripts will write a string to represent the current connection state to a file.
      # Our qBittorrent image has a feature that can wait for this file to contain the word 'connected' before actually starting the application.
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus
  additionalContainers:
    # Use this to provied proxied access to arrs
    mam-helper:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |
        set -e
        set -x

        echo "Waiting for VPN to be connected..."
        while ! grep -s -q "connected" /shared/vpnstatus; do
            # Also account for gluetun-style http controller
            if (curl -s http://localhost:8042/v1/openvpn/status | grep -q running); then
                break
            fi
            echo "VPN not connected"
            sleep 2
        done
        echo "VPN Connected, processing cookies..."

        # If we have a cookie already, try to use it
        if [[ -f /config/mam/saved.cookies ]]; then
          curl -c /config/mam/saved.cookies -b /config/mam/saved.cookies https://t.myanonamouse.net/json/dynamicSeedbox.php  -o /config/mam/mam_id-curl-output.log
        fi

        # Now whether that worked or not, look for /config/mam/mam_id
        mkdir -p /config/mam
        while [ 1 ]; do
          if [[ -f /config/mam/mam_id ]]; then
            curl -c /config/mam/saved.cookies -b "mam_id=$(cat /config/mam/mam_id)" https://t.myanonamouse.net/json/dynamicSeedbox.php -o /config/mam/mam_id-curl-output.log
            mv /config/mam/mam_id /config/mam/mam_id_processed
          fi
          sleep 1m
        done
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: rutorrent
      - mountPath: /shared
        name: shared
      resources: *default_resources
      securityContext: *default_securitycontext

rutorrentpia:
  <<: *rutorrent
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,rutorrent-config,rutorrent-pia-config,elfbot-rutorrent" # Reload the deployment every time the yaml config changes
  addons:
    vpn:
      <<: *rutorrent_addons_vpn
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      envFrom:  
      - configMapRef:
          name: rutorrent-pia-config
      - configMapRef:
          name: elfbot-rutorrent
          optional: true              
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rutorrent
      - mountPath: /tmp
        name: tmp

qbittorrentgluetun: &qbittorrent
  podLabels:
    app.elfhosted.com/name: qbittorrent
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M"
  enabled: false
  automountServiceAccountToken: false
  image:
    registry: ghcr.io
    repository: elfhosted/qbittorrent
    tag: 5.0.2@sha256:7ff09d6a5ca2267f78161fb46eeafaf5b2af7806288fe7f3d2dfed2521374e3d
  priorityClassName: tenant-bulk
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't seem to work well with entrypoint
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-qbittorrent,qbittorrent-gluetun-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    <<: *storagemounts
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_100g
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: qbittorrent
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-qbittorrent
          optional: true
    elfscripts:
      enabled: "true"
      mountPath: "/elfscripts/"
      type: "custom"
      volumeSpec:
        configMap:
          name: qbittorrent-elfscripts
          defaultMode: 0755
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
      nameOverride: spanky
  env:
    # -- Set the container timezone
    TZ: UTC
    HOME: /config
    XDG_CONFIG_HOME: /config
    XDG_DATA_HOME: /config
    WAIT_FOR_VPN: "true"
  envFrom:
  - configMapRef:
      name: elfbot-qbittorrent
      optional: true
  extraEnvVars:
  - name: PORT_FILE
    valueFrom:
      configMapKeyRef:
        name: qbittorrent-gluetun-config
        key: PORT_FILE
    optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Remove the lockfile if it exists
        if [[ -f /config/qBittorrent/lockfile ]]; then
          rm /config/qBittorrent/lockfile
        fi

        mkdir -p /config/qBittorrent/torrent_files/complete
        mkdir -p /config/qBittorrent/torrent_files/incomplete

        # Enforce 1:1 seeding ratio, and then delete
        sed -i  "s/Session\\\GlobalMaxRatio=.*/Session\\\GlobalMaxRatio=1/" /config/qBittorrent/qBittorrent.conf

        # Permit TCP only
        sed -i  "s/Session\\\BTProtocol=.*/Session\\\BTProtocol=TCP/" /config/qBittorrent/qBittorrent.conf

        # Disable CSRF protection so that Homer can show qBit stats
        sed -i  "s/WebUI\\\CSRFProtection=.*/WebUI\\\CSRFProtection=false/" /config/qBittorrent/qBittorrent.conf

        # Insist on tun0
        sed -i  "s/Session\\\Interface=.*/Session\\\Interface=tun0/" /config/qBittorrent/qBittorrent.conf
        sed -i  "s/Session\\\InterfaceName=.*/Session\\\InterfaceName=tun0/" /config/qBittorrent/qBittorrent.conf

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /shared
        name: shared
      securityContext: *default_securitycontext
      resources: *default_resources
      envFrom:
      - configMapRef:
          name: qbittorrent-gluetun-config
  additionalContainers:
    # Use this to provied proxied access to arrs
    mam-helper:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |
        set -e
        set -x

        echo "Waiting for VPN to be connected..."
        while ! grep -s -q "connected" /shared/vpnstatus; do
            # Also account for gluetun-style http controller
            if (curl -s http://localhost:8042/v1/openvpn/status | grep -q running); then
                break
            fi
            echo "VPN not connected"
            sleep 2
        done
        echo "VPN Connected, processing cookies..."

        # If we have a cookie already, try to use it
        if [[ -f /config/mam/saved.cookies ]]; then
          curl -c /config/mam/saved.cookies -b /config/mam/saved.cookies https://t.myanonamouse.net/json/dynamicSeedbox.php  -o /config/mam/mam_id-curl-output.log
        fi

        # Now whether that worked or not, look for /config/mam/mam_id
        mkdir -p /config/mam
        while [ 1 ]; do
          if [[ -f /config/mam/mam_id ]]; then
            curl -c /config/mam/saved.cookies -b "mam_id=$(cat /config/mam/mam_id)" https://t.myanonamouse.net/json/dynamicSeedbox.php -o /config/mam/mam_id-curl-output.log
            mv /config/mam/mam_id /config/mam/mam_id_processed
          fi
          sleep 1m
        done
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /shared
        name: shared
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
      ephemeral-storage: 50Mi
    limits:
      cpu: 500m
      memory: 2Gi # .2 GB for headroom
      ephemeral-storage: 100Mi # a safety net against node ephemeral space exhaustion
  addons:
    vpn: &qbittorrent_addons_vpn
      enabled: true
      type: gluetun
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      envFrom:
      - configMapRef:
          name: qbittorrent-gluetun-config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
        subPath: qbittorrent
      config: # We have to set this to null so that we can override with our own config

      # The scripts that get run when the VPN connection opens/closes are defined here.
      # The default scripts will write a string to represent the current connection state to a file.
      # Our qBittorrent image has a feature that can wait for this file to contain the word 'connected' before actually starting the application.
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus


# Custom service for pia
qbittorrentpia:
  <<: *qbittorrent
  env:
    PORT_FILE: /config/forwarded-port
    WAIT_FOR_VPN: "true"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-qbittorrent,qbittorrent-pia-config"
  addons:
    vpn:
      <<: *qbittorrent_addons_vpn
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      envFrom:
      - configMapRef:
          name: qbittorrent-pia-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Remove the lockfile if it exists
        if [[ -f /config/qBittorrent/lockfile ]]; then
          rm /config/qBittorrent/lockfile
        fi

        mkdir -p /config/qBittorrent/torrent_files/complete
        mkdir -p /config/qBittorrent/torrent_files/incomplete

        # Enforce 1:1 seeding ratio, and then delete
        sed -i  "s/Session\\\GlobalMaxRatio=.*/Session\\\GlobalMaxRatio=1/" /config/qBittorrent/qBittorrent.conf

        # Permit TCP only
        sed -i  "s/Session\\\BTProtocol=.*/Session\\\BTProtocol=TCP/" /config/qBittorrent/qBittorrent.conf

        # Disable CSRF protection so that Homer can show qBit stats
        sed -i  "s/WebUI\\\CSRFProtection=.*/WebUI\\\CSRFProtection=false/" /config/qBittorrent/qBittorrent.conf

        # Insist on tun0
        sed -i  "s/Session\\\Interface=.*/Session\\\Interface=tun0/" /config/qBittorrent/qBittorrent.conf
        sed -i  "s/Session\\\InterfaceName=.*/Session\\\InterfaceName=tun0/" /config/qBittorrent/qBittorrent.conf

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /shared
        name: shared
      securityContext: *default_securitycontext
      resources: *default_resources

nzbget:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/nzbget
    tag: 25.1@sha256:77e020a307dc76853929e7ed2c47993bfc3a070d5b2205562d1dcb9510c6fedc
  priorityClassName: tenant-bulk
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-nzbget"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: nzbget
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_100g
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-nzbget
          optional: true

  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6789
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: nzbget
      - mountPath: /tmp
        name: tmp

sabnzbd:
  enabled: false
  hostname: sabnzbd # required to prevent whitelisting requirement per https://sabnzbd.org/wiki/extra/hostname-check.html
  podLabels:
    app.elfhosted.com/class: nzb
  image:
    registry: ghcr.io
    repository: elfhosted/sabnzbd
    tag: 4.3.3@sha256:af2ef54052d0d340064997aeb76bb8e612f3b47a8a0fc5c446e821a8bacd80cc
  priorityClassName: tenant-bulk
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-sabnzbd"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: sabnzbd
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_100g
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-sabnzbd
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: sabnzbd
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # fix host_whitelist
        sed -i  's/goldilocks/{{ .Release.Name }}/g' /config/sabnzbd.ini

        # If we've previously backed up a queue, then restore it to /tmp
        files=$(shopt -s nullglob dotglob; echo /config/queue-backup/*)
        if (( ${#files} ))
        then
          cp /config/queue-backup/* /tmp/ -rfp
          rm -rf /config/queue-backup
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: sabnzbd
      - mountPath: /tmp
        name: tmp
      env: *bootstrap_env
      securityContext: *default_securitycontext
      resources: *default_resources
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1500m # if par threads is 1, this leaves 0.5cpu for downloading
      memory: 1500Mi
  additionalContainers:
    backup-queue:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |
        set -e
        IFS=$'\n' # in case of paths with spaces (looking at you, Plex!)

        function backupqueue_on_shutdown {
            echo "Received SIGTERM, waiting 5s for app to shut down..."
            mkdir -p /config/queue-backup
            sleep 5s

            # sync any files < 1MB
            cd /tmp
            find ./ -type f -size -1024k | rsync -avr --files-from=- /tmp /config/queue-backup
        }

        # When we terminate, perform the backup
        trap backupqueue_on_shutdown SIGTERM

        # Hang around doing nothing until terminated
        while true
        do
            echo "Waiting for SIGTERM to backup queue from /tmp"
            sleep infinity
        done
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: sabnzbd
      - mountPath: /tmp
        name: tmp
      env: *bootstrap_env
      securityContext: *default_securitycontext
      resources: *default_resources

  env:
    HOST_WHITELIST_ENTRIES: "{{ .Release.Name }}.sabnzbd.elfhosted.com"
    SABNZBD_UID: 568
    SABNZBD_GID: 568

tautulli:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/tautulli
    tag: 2.15.2@sha256:214a0ef60363bf81722612732eb62830366fc2cbdbd75e39c4acca61aaae6f20
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-tautulli"
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: tautulli
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-tautulli
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8181
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: tautulli
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/sh
      - -c
      - |
        set -x
        set -e

        # Clear out logs older than 24h
        if [ -d "/config/logs" ]; then
            # Find and delete files older than 7 days
            find "/config/logs" -type f -mtime +1 -exec rm -f {} \;
            echo "Files older than 1 day have been removed from /config/logs."
        fi

        # Clear out backups older than 2d
        if [ -d "/config/backups" ]; then
            # Find and delete files older than 2 days
            find "/config/backups" -type f -mtime +2 -exec rm -f {} \;
            echo "Files older than 1 day have been removed from /config/backups."
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: tautulli
      resources: *default_resources
      securityContext: *default_securitycontext

radarr: &app_radarr
  enabled: false
  podLabels:
    app.elfhosted.com/name: radarr
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/radarr
    tag: 5.26.2.10099@sha256:64c278e344051ea00998b2968a653c23c0f6d8ba6c4e72c6e22728d321af3f0a
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-radarr,radarr-env" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: radarr-env
  - configMapRef:
      name: elfbot-radarr
      optional: true       
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: radarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    mediacover:
      enabled: true
      type: custom
      mountPath: /config/MediaCover
      subPath: radarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: mediacovers          
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: radarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: radarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-radarr
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory          
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 7878
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: radarr
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault    
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: radarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/radarr
        mkdir -p /storage/symlinks/movies

        # for database to use 
        mkdir -p /config/postgresql/database

        # if /config/MediaCover exists (on the config volume), purge it, since this is now handled on a dedicated volume
        if [ -d /config/MediaCover ]; then
          rm -rf /config/MediaCover
        fi

      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      - mountPath: /config
        name: config
        subPath: radarr        
      resources: *default_resources
      securityContext: *default_securitycontext
        
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: radarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: radarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: radarr/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 2
          memory: 8Gi     
    database-backup:
      image: ghcr.io/elfhosted/radarr:5.26.2.10099@sha256:64c278e344051ea00998b2968a653c23c0f6d8ba6c4e72c6e22728d321af3f0a
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: radarr-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: radarr/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi
  resources:
    requests:
      cpu: 0m
      memory: 500Mi
    limits:
      cpu: 600m
      memory: 6Gi

radarr4k: &app_radarr4k
  enabled: false
  podLabels:
    app.elfhosted.com/name: radarr4k
    app.elfhosted.com/class: debrid
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/radarr
    tag: 5.26.2.10099@sha256:64c278e344051ea00998b2968a653c23c0f6d8ba6c4e72c6e22728d321af3f0a
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-radarr4k,radarr4k-env" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: radarr4k-env
  - configMapRef:
      name: elfbot-radarr4k
      optional: true       
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: radarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    mediacover:
      enabled: true
      type: custom
      mountPath: /config/MediaCover
      subPath: radarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: mediacovers            
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-radarr4k
          optional: true
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: radarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: radarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory          
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 7878
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config/ -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi
        
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: radarr4k
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault      
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: radarr4k
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/radarr4k
        mkdir -p /storage/symlinks/movies-4k

        # for database to use 
        mkdir -p /config/postgresql/database        

        # if /config/MediaCover exists (on the config volume), purge it, since this is now handled on a dedicated volume
        if [ -d /config/MediaCover ]; then
          rm -rf /config/MediaCover
        fi
        
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      - mountPath: /config
        name: config
        subPath: radarr4k        
      resources: *default_resources
      securityContext: *default_securitycontext    
  resources:
    requests:
      cpu: 0m
      memory: 500Mi
    limits:
      cpu: 2
      memory: 6Gi
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: radarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: radarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: radarr4k/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi   
    database-backup:
      image: ghcr.io/elfhosted/radarr:5.26.2.10099@sha256:64c278e344051ea00998b2968a653c23c0f6d8ba6c4e72c6e22728d321af3f0a
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: radarr4k-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: radarr4k/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi            

ombi:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/ombi
    tag: 4.47.1@sha256:7346137897633f5cb3cd7d26922782f68791201e317bb3051d3f79d25242c54e
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-ombi"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: ombi
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-ombi
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5000
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: ombi
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 3m
      memory: 150Mi
    limits:
      cpu: 2
      memory: 1Gi

scannarr: &app_scannarr
  enabled: false
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/scannarr
    tag: rolling@sha256:a0d7e6f755ef8f9f3cc61d2cd7f540acd81acb39c0431e6cae91030a02a3c7ff
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-scannarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    sonarr-settings:
      enabled: "true"
      mountPath: "/app/settings_sonarr.json"
      subPath: "settings_sonarr.json"
      type: "custom"
      volumeSpec:
        configMap:
          name: scannarr-config
    radarr-settings:
      enabled: "true"
      mountPath: "/app/settings_radarr.json"
      subPath: "settings_radarr.json"
      type: "custom"
      volumeSpec:
        configMap:
          name: scannarr-config
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true
      ports:
        http:
          port: 9898 # doesn't matter this doesn,t actually use ports
  additionalContainers:
    podinfo:
      image: stefanprodan/podinfo # used to run probes from gatus
  resources: *default_resources

scannarr4k:
  <<: *app_scannarr
  enabled: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-scannarr4k"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
    sonarr-settings:
      enabled: "true"
      mountPath: "/app/settings_sonarr.json"
      subPath: "settings_sonarr.json"
      type: "custom"
      volumeSpec:
        configMap:
          name: scannarr4k-config
    radarr-settings:
      enabled: "true"
      mountPath: "/app/settings_radarr.json"
      subPath: "settings_radarr.json"
      type: "custom"
      volumeSpec:
        configMap:
          name: scannarr4k-config

huntarr: &app_huntarr
  enabled: false
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/huntarr
    tag: 8.1.9@sha256:c0c30f43162fcfd42d06f2acc5229380c7219bc1676d1dba87321ed0e13447fc
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-huntarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: huntarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config  
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: huntarr-config   
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-huntarr
          optional: true
    app-frontend-static-data:
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mountPath: /app/frontend/static/data      
  initContainers:  
    aa-remove-json-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Since v8, we don't use .json files anymore.
        # Search for .json files in /config/settings and reset if they're found
        if find /config/settings -type f -name "*.json" | grep -q .; then
            # If any .json files are found, delete everything in /config
            rm -rf /config/*
        fi
      volumeMounts:
      - mountPath: /config/
        name: config
        subPath: huntarr
      - name: example-config
        mountPath: "/bootstrap/"  
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: huntarr
      - mountPath: /tmp
        name: tmp                         
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true
      ports:
        http:
          port: 9705
  resources: *default_resources

bazarr:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/bazarr
    tag: 1.5.2@sha256:c44714dd7504d72e32d4937ebae0d616738574423b3e319a74dddbeb67755c5a
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-bazarr"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: bazarr-config
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: bazarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-bazarr
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6767
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: bazarr
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

bazarr4k:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/bazarr
    tag: 1.5.2@sha256:c44714dd7504d72e32d4937ebae0d616738574423b3e319a74dddbeb67755c5a
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-bazarr4k"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: bazarr4k-config
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: bazarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-bazarr4k
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6767
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: bazarr4k
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

filebrowser:
  hostname: elfhosted
  enabled: true
  podLabels:
    app.elfhosted.com/name: filebrowser
  image:
    repository: ghcr.io/elfhosted/filebrowser
    tag: 2.23.0@sha256:296e3a3d08c5ca07a26350358fa2e58a597a41adb253a65bba27b557f36383e5
  podAnnotations:
    kubernetes.io/egress-bandwidth: "5M" # filebrowser is not for streaming
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  envFrom:
  - configMapRef:
      name: filebrowser-env
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  deploymentStrategy:
    type: Recreate
    rollingUpdate: null
  controller:
    replicas: 1 # not sure we need 2 replicas anymore
    strategy: Recreate
    # rollingUpdate:
    #   unavailable: 1
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,filebrowser-elfbot-script,elfbot-filebrowser" # Reload the deployment every time the rclones change
  # We will use this to alter configmaps to trigger pod restarts
  serviceAccount:
    create: true
    name: filebrowser
  automountServiceAccountToken: true
  persistence:
    <<: *storagemounts  
    backup:
      enabled: true
      type: custom
      mountPath: /storage/backup
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    config:
      enabled: true
      type: custom
      mountPath: /storage/config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /storage/logs
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    elfterm-state: # so auto-provisioning doesn't break
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mountPath: /home/elfie/.local/state
    dummy-storage: # so auto-provisioning doesn't break
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
    elfbot:
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mountPath: /elfbot
    elfbot-script:
      enabled: "true"
      mountPath: "/usr/local/bin/elfbot"
      subPath: "elfbot"
      type: "custom"
      volumeSpec:
        configMap:
          name: filebrowser-elfbot-script
          defaultMode: 0755
    elfbot-script-ucfirst:
      enabled: "true"
      mountPath: "/usr/local/bin/Elfbot" # make it easier for mobile users
      subPath: "elfbot"
      type: "custom"
      volumeSpec:
        configMap:
          name: filebrowser-elfbot-script
          defaultMode: 0755
    recyclarr-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: recyclarr-config
    symlinks: *symlinks
    tmp: *tmp
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080 # this allows us to run as non-root

  ingress:
    main:
      enabled: false
  initContainers:
    setup:
      image: ghcr.io/elfhosted/filebrowser:2.23.0@sha256:296e3a3d08c5ca07a26350358fa2e58a597a41adb253a65bba27b557f36383e5
      # 2.23.0@sha256:1db0f0114a169ea2a877d75c47903a6d01534340421948845d5e298c7ac7ceb4 is the last good version for TFA
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Delete tmp db if necessary
        if [ -f /tmp/filebrowser.db ]
        then
          rm /tmp/filebrowser.db
        fi


        /filebrowser config init \
          --disable-preview-resize \
          --disable-thumbnails \
          --disable-type-detection-by-header \
          --branding.name="{{ .Release.Name }}, by ElfHosted  " \
          --branding.files=/branding \
          --branding.disableExternal \
          --auth.method=noauth \
          --lockPassword \
          --database /tmp/filebrowser.db \
          --root /storage \
          --cache-dir /tmp

        # allow zip, unzip, rar, unrar, ls, pwd, cd, mv
        /filebrowser config set --database /tmp/filebrowser.db --commands zip,unzip,rar,unrar,ls,pwd,cd,mv,cp,ln,find,echo,grep,cat,touch,tar,gzip,rm,tree,du,mlocate,updatedb,locate,elfbot,Elfbot
        # /filebrowser config set --database /tmp/filebrowser.db --shell 'vstat -c'

        # now tell filebrowser about the user (who gets authenticated via the proxy)
        /filebrowser users add 1 bogus --database /tmp/filebrowser.db

      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /storage
        name: dummy-storage
      resources: *default_resources
      securityContext: *default_securitycontext
    copy-recyclarr-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e
        # If we don't already have an example config, create one
        if [ ! -f /config/recyclarr.yaml ];
        then
          cp /bootstrap/recyclarr.yaml /config/
        fi
      volumeMounts:
      - mountPath: /config/
        name: config
        subPath: recyclarr
      - name: recyclarr-config
        mountPath: "/bootstrap/"
      securityContext: *default_securitycontext     
  additionalContainers:
    # this container exists to watch for restarts requested by elfbot, and to use create configmaps to trigger restarts using reloader
    elfbot:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |
        # respond to creation or modification, but not deletion
        inotifywait -m -e create -e modify --format "%f" /elfbot \
          | while read APP
            do
              # if we are force-killing the pod, then don't bother with the configmap
              if (cat /elfbot/$APP | grep -q forcerestart); then
                echo "forcerestart requested, deleting $APP pod with --force.."
                kubectl delete pod -l app.kubernetes.io/name=$APP --force
                kubectl delete pod -l app.elfhosted.com/name=$APP --force
              else

                # put the contents of the file into the configmap which will trigger the restart
                echo command received for ${APP} : [$(cat /elfbot/$APP)]
                # create the configmap if it doesn't exist, since reloader only looks at _changes_ to configmaps
                if ! $(kubectl get configmap -n {{ .Release.Namespace }} elfbot-${APP} 2>&1 >/dev/null); then
                    kubectl create configmap -n {{ .Release.Namespace }} elfbot-${APP} --from-literal=elfbot_last_action=$(date +%s)
                    sleep 10s
                fi

                # If we were passed a key=value string in /etc/elfbot, then split it
                COMMAND=$(cat /elfbot/$APP)

                # We separate key and value with an '=', but sometimes the value may contain __another__ '=' (like Plex preferences)
                sep='='
                case $COMMAND in
                  # If we are separated by an =
                  (*"$sep"*)
                    KEY=${COMMAND%%"$sep"*}
                    VALUE=${COMMAND#*"$sep"}
                    ;;
                  # if not, we are a simple command like "backup"
                  (*)
                    KEY=$COMMAND
                    VALUE=$(date +%s)
                    ;;
                esac


                # patch the configmap with the latest key/value
                kubectl patch configmap -n {{ .Release.Namespace }} elfbot-${APP} -p "{\"data\":{\"${KEY}\":\"${VALUE}\"}}"
              fi
            done
      volumeMounts:
      - mountPath: /elfbot
        name: elfbot
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 0m
      memory: 6Mi
    limits:
      cpu: 1
      memory: 1Gi


uptimekuma:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/uptime-kuma
    tag: 1.23.16@sha256:a8f477d080a481ddb08f66d3b9021ef2842bda942aedeed59ee1839713b63876
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-uptimekuma"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/data/
      subPath: uptimekuma
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-uptimekuba
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: uptimekuma
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 100m
      memory: 1Gi

privatebin:
  enabled: false
  image:
    repository: privatebin/fs
    tag: 1.7.6
  priorityClassName:
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-privatebin"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # crashes privatebin, TBD to determine why, and whether an emptydir /tmpfs might help
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /srv/data
      subPath: privatebin
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-privatebin
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 100m
      memory: 128Mi
  config:
    main:
      discussion: false
      opendiscussion: false
      password: true
      fileupload: true
      burnafterreadingselected: false
      defaultformatter: "plaintext"
      syntaxhighlightingtheme: "sons-of-obsidian"
      sizelimit: 1048576
      template: "bootstrap-dark"
      info: "Hosted with  by ElfHosted "
      languageselection: true
      languagedefault: "en"
      # urlshortener: "https://shortener.example.com/api?link="
      qrcode: false
      icon: "none"
      zerobincompatibility: false
      # httpwarning: true
      compression: "zlib"
    expire:
      default: "1week"
    expire_options:
      5min: 300
      10min: 600
      1hour: 3600
      1day: 86400
      1week: 604800
    formatter_options:
      plaintext: "Plain Text"
      syntaxhighlighting: "Source Code"
      markdown: "Markdown"
    traffic:
      limit: 10
      # exemptedIp: "1.2.3.4,10.10.10/24"

kapowarr:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/kapowarr
    tag: 1.2.0@sha256:469ef44ffbd9514f0fa22facfbc5b35d4ff47713f98ce3a77e6988ab11984858
  priorityClassName:
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-kapowarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # breaks kapowarr
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  probes:
    liveness:
      enabled: false
    startup:
      enabled: false
    readiness:
      enabled: false
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: kapowarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    temp-downloads:
      enabled: true
      type: emptyDir
      mountPath: /app/temp_downloads
      sizeLimit: 50Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-kapowarr
          optional: true
    tmp: *tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5656
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 2Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: kapowarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/comics
        mkdir -p /storage/symlinks/comics

      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      resources: *default_resources
      securityContext: *default_securitycontext


calibreweb:
  enabled: false
  podLabels:
    app.elfhosted.com/name: calibre-web
  priorityClassName: tenant-normal  
  image:
    repository: ghcr.io/elfhosted/calibre-web-automated
    tag: v3.0.4@sha256:5b81a6f32f2947c7ed82b47afda8289e92158f9d19c9ea739fa30de98cecfa8a
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-calibre-web"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
    DOCKER_MODS: linuxserver/mods:universal-calibre
  envFrom:
  - configMapRef:
      name: elfbot-calibre-web
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: calibre-web
      volumeSpec:
        persistentVolumeClaim:
          claimName: config     
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-calibre-web
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 8083
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: calibre-web
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # There are symlinks pre-prepared for these
        mkdir -p /config/calibre-library
        mkdir -p /config/cwa-book-ingest

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: calibre-web
      resources: *default_resources
      securityContext: *default_securitycontext        
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

cwabookdownloader:
  enabled: false
  podLabels:
    app.elfhosted.com/name: cwa-book-downloader
  priorityClassName: tenant-normal  
  image:
    repository: ghcr.io/elfhosted/cwa-downloader
    tag: rolling@sha256:e9af79cc4d95f0fc1a02afe3e48c772fc6763cee25206e3bd55e85972521b390
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-cwa-book-downloader"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    runAsUser: 568
    runAsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
    HOME: /app
  envFrom:
  - configMapRef:
      name: elfbot-cwa-book-downloader
      optional: true
  command:
  - python3 
  - app.py
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /cwa-book-ingest/
      subPath: calibre-web/cwa-book-ingest
      volumeSpec:
        persistentVolumeClaim:
          claimName: config     
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-cwa-book-downloader
          optional: true
    logs:
      enabled: true
      type: custom
      mountPath: /var/log/cwa-book-downloader/
      subPath: cwa-book-downloader
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs   
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 8084
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: calibre-web-book-downloader
      - mountPath: /tmp
        name: tmp      
  additionalContainers:
    cloudflarebypassforscraping:
      image: ghcr.io/elfhosted/cloudflarebypassforscraping@sha256:7bdf614b57f57e47a6cecdaf6048869037123e0731456ed186b5430d6bfbefad
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

lazylibrarian:
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/lazylibrarian
    tag: rolling@sha256:5095b4270cd7132983e6b6b1088a552d20051c2cf0f1f7e265c733c2af899e00
  enabled: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-lazylibrarian"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    runAsUser: 568
    runAsGroup: 568
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: lazylibrarian
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-lazylibrarian
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 5299
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: lazylibrarian
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 96Mi
    limits:
      cpu: 1
      memory: 1Gi

mylar:
  enabled: false
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/mylar3
    tag: 0.8.2@sha256:89fdca5cc80457bde3373a6a7d69631487c41dff1e15d34bf78391eae85df0d9
  env:
    PUID: 568
    PGID: 568
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mylar" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: mylar
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-mylar
          optional: true

  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8090
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: mylar
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

komga:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/komga
    tag: 1.22.0@sha256:dbf05b8f64cc959cfb703174775c232291847e0994d0fde26c39ba0c386ac489
  env:
    KOMGA_CONFIGDIR: /config
    KOMGA_REMEMBERME_KEY: yesplease
    JAVA_TOOL_OPTIONS: -Xmx2g
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-komga" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: komga
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-komga
          optional: true

  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 25600
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: komga
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 2Gi

kavita:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/kavita
    tag: 0.8.6@sha256:98e9dc90caae289c1b7afc4a35ffeaf8eb57acd0bf5684244223443e9f7a6b5c
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-kavita" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /kavita/config
      subPath: kavita
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-kavita
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: kavita
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 5000
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 2
      memory: 1Gi

calibre:
  enabled: false
  # runtimeClassName: kata
  image:
    repository: quay.io/linuxserver.io/calibre
    tag: 8.5.0@sha256:2ff7fbcc54a352b878751be23f1fe651c61ad7548e7e0d3c7b61d117f663b3fc
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with s6
    allowPrivilegeEscalation: false # do we need this too?
    # runAsUser: 568
    # runAsGroup: 568
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-calibre"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: calibre
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-calibre
          optional: true
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
  env:
    PUID: 568
    PGID: 568
    TITLE: Calibre | ElfHosted
    START_DOCKER: false
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 8080
  resources:
    requests:
      cpu: 0m
      memory: 1Gi
    limits:
      cpu: 1
      memory: 4Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: calibre
      - mountPath: /tmp
        name: tmp
  envFrom:
  - configMapRef:
      name: elfbot-calibre
      optional: true

sonarr: &app_sonarr
  enabled: false
  podLabels:
    app.elfhosted.com/name: sonarr
    app.elfhosted.com/class: debrid
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/sonarr
    tag: 4.0.15.2941@sha256:7a6a5969468d002816964ef101a459b53a461e516ff262094c72de3e73f1fff1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-sonarr,sonarr-env" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: sonarr-env
  - configMapRef:
      name: elfbot-sonarr
      optional: true       
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: sonarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    mediacover:
      enabled: true
      type: custom
      mountPath: /config/MediaCover
      subPath: sonarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: mediacovers            
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: sonarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: sonarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-sonarr
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory      
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8989
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: sonarr
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault    
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: sonarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/sonarr
        mkdir -p /storage/symlinks/series

        # for database to use 
        mkdir -p /config/postgresql/database        

        # if /config/MediaCover exists (on the config volume), purge it, since this is now handled on a dedicated volume
        if [ -d /config/MediaCover ]; then
          rm -rf /config/MediaCover
        fi
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      - mountPath: /config
        name: config
        subPath: sonarr
      resources: *default_resources
      securityContext: *default_securitycontext     
  resources:
    requests:
      cpu: 0m
      memory: 600Mi
    limits:
      cpu: 2
      memory: 6Gi
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: sonarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: sonarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: sonarr/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 2Gi         
    database-backup:
      image: ghcr.io/elfhosted/sonarr:4.0.15.2941@sha256:7a6a5969468d002816964ef101a459b53a461e516ff262094c72de3e73f1fff1
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: sonarr-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: sonarr/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi            

sonarr4k: &app_sonarr4k
  enabled: false
  podLabels:
    app.elfhosted.com/name: sonarr4k
    app.elfhosted.com/class: debrid
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/sonarr
    tag: 4.0.15.2941@sha256:7a6a5969468d002816964ef101a459b53a461e516ff262094c72de3e73f1fff1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-sonarr4k,sonarr4k-env" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: sonarr4k-env
  - configMapRef:
      name: elfbot-sonarr4k
      optional: true      
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: sonarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    mediacover:
      enabled: true
      type: custom
      mountPath: /config/MediaCover
      subPath: sonarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: mediacovers            
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: sonarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: sonarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-sonarr4k
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory               
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8989
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: sonarr4k
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault     
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: sonarr4k
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/sonarr4k
        mkdir -p /storage/symlinks/series-4k

        # for database to use 
        mkdir -p /config/postgresql/database        
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      - mountPath: /config
        name: config
        subPath: sonarr4k     
      resources: *default_resources
      securityContext: *default_securitycontext       
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 2
      memory: 6Gi # reduce once sqlite-to-db-migration is done
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: sonarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: sonarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: sonarr4k/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi 
    database-backup:
      image: ghcr.io/elfhosted/sonarr:4.0.15.2941@sha256:7a6a5969468d002816964ef101a459b53a461e516ff262094c72de3e73f1fff1
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: sonarr4k-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: sonarr4k/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi             
  probes: # need a long startup for database migrations
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 3000
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 8989
        timeoutSeconds: 1

resiliosync:
  service:
    main:
      enabled: false
  command:
  - rslsync
  - --config
  - /sync.conf
  - --nodaemon
  enabled: false
  priorityClassName: tenant-bulk
  image:
    repository: ghcr.io/elfhosted/resilio-sync
    tag: 3.0.3.1065-1@sha256:544fe39e56b2beef67a0266f155a144ddcce20d5742a71b6113c7f1e4d9b6c28
  env:
    PUID: 568
    GUID: 568
    S6_READ_ONLY_ROOT: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # another s6 containeir!
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-resiliosync"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: resiliosync
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    setup-config:
      enabled: "true"
      mountPath: "/sync.conf"
      subPath: "sync.conf"
      type: "custom"
      volumeSpec:
        configMap:
          name: resiliosync-config
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-resiliosync
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: resiliosync
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi

prowlarr: &app_prowlarr
  enabled: false
  podLabels:
    app.elfhosted.com/name: prowlarr
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/prowlarr-develop
    tag: 2.0.0.5094@sha256:a84a164ab157071d756ebf6736a7e3185e2dfac121fed99ae157921c5c382b02
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-prowlarr,prowlarr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: prowlarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: prowlarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: prowlarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-prowlarr
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory      
  envFrom:
  - configMapRef:
      name: prowlarr-env  
  - configMapRef:
      name: elfbot-prowlarr
      optional: true                   
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9696
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: prowlarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/sh
      - -c
      - |
        set -x
        set -e
        # Set auth to external
        sed -i  "s|<AuthenticationMethod>None</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml

        # for database to use 
        mkdir -p /config/postgresql/database

        # Clear out logs older than 24h
        if [ -d "/config/logs" ]; then
            # Find and delete files older than 7 days
            find "/config/logs" -type f -mtime +1 -exec rm -f {} \;
            echo "Files older than 1 day have been removed from /config/logs."
        fi

        # Get custom indexers
        mkdir -p /config/Definitions/Custom

        curl https://raw.githubusercontent.com/elfhosted/prowlarr-indexers/main/Custom/elfcomet.yml > /config/Definitions/Custom/elfcomet.yml

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: prowlarr
      resources: *default_resources
      securityContext: *default_securitycontext
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: prowlarr
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault       
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 6Gi
  env:
    S6_READ_ONLY_ROOT: 1
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: postgres
        - name: POSTGRES_DB
          value: prowlarr
        - name: POSTGRES_USER
          value: prowlarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: prowlarr/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi     
    database-backup:
      image: ghcr.io/elfhosted/prowlarr-develop:2.0.0.5094@sha256:a84a164ab157071d756ebf6736a7e3185e2dfac121fed99ae157921c5c382b02 
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: prowlarr-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: prowlarr/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi
  probes: # need a long startup for database migrations, until we can bootstrap an entire database
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 3000
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 9696
        timeoutSeconds: 1  
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 300
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 9696
        timeoutSeconds: 1  
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 300
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 9696
        timeoutSeconds: 1                      


nzbhydra:
  enabled: false
  podLabels:
    app.elfhosted.com/name: nzbhydra
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/nzbhydra2
    tag: 7.15.1@sha256:afd2a8841b43629289c4f8df75531c3a48f8b8a52f763694b12f4b9c21052dc8
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-nzbhydra"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: nzbhydra
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: nzbhydra
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-nzbhydra
          optional: true          
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5076
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: nzbhydra
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

lidarr:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/lidarr-develop
    tag: 2.13.0.4664@sha256:f35f078ba42887de525392bc08cc413bd158d743b3ac0164c1c6c0c094166cc5  
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-lidarr" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: lidarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    s6:
      enabled: true
      type: emptyDir
      mountPath: /var/run/s6
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-lidarr
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory              
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8686
  envFrom:
  - configMapRef:
      name: lidarr-env
  - configMapRef:
      name: elfbot-lidarr
      optional: true             
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: lidarr
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault     
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: lidarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for database to use 
        mkdir -p /config/postgresql/database

        # Set auth to external
        sed -i  "s|<AuthenticationMethod>None</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml
        # Clean up wasteful temporary mediacover storage (Radarr will just re-download these)

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: lidarr
      resources: *default_resources
      securityContext: *default_securitycontext
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: lidarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: lidarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: lidarr/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi     
    database-backup:
      image: ghcr.io/elfhosted/lidarr-develop:2.13.0.4664@sha256:f35f078ba42887de525392bc08cc413bd158d743b3ac0164c1c6c0c094166cc5  
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: lidarr-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: lidarr/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi      
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 1Gi

navidrome:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/navidrome
    tag: 0.56.1@sha256:02a73ab42d64b5b5b29f5f0b05ce6d15149f4e6c162661d3b33958e821046019
  priorityClassName: tenant-streaming
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-navidrome"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  env:
    ND_MUSICFOLDER: /tmp
    ND_DATAFOLDER: /config
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: navidrome
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-navidrome
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: navidrome
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 4533
  resources:
    requests:
      cpu: 0m
      memory: 32Mi
    limits:
      cpu: 2
      memory: 1Gi

readarr:
  enabled: false
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/readarr-develop
    tag: 0.4.18.2805@sha256:ed51f754b6baeb58d86b4fae4872dbcc8e9799c01788a9023c3d7704ae549333    
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-readarr" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: readarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: readarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    tmp-readarr-backup:
      enabled: true
      type: emptyDir
      mountPath: /tmp/readarr_backup
      sizeLimit: 32Mi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-readarr
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: readarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup        
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8787
  envFrom:
  - configMapRef:
      name: readarr-env
  - configMapRef:
      name: elfbot-readarr
      optional: true           
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: readarr
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault     
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: readarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for database to use 
        mkdir -p /config/postgresql/database

        # Set auth to external
        sed -i  "s|<AuthenticationMethod>None</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml
        sed -i  "s|<AuthenticationMethod>Basic</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: readarr
      resources: *default_resources
      securityContext: *default_securitycontext
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: readarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: readarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: readarr/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi     
    database-backup:
      image: ghcr.io/elfhosted/readarr-develop:0.4.18.2805@sha256:ed51f754b6baeb58d86b4fae4872dbcc8e9799c01788a9023c3d7704ae549333   
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: readarr-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: readarr/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi        
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 2
      memory: 16Gi
  probes: # need a long startup for database migrations
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 3000
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 8787
        timeoutSeconds: 1      

readarraudio:
  enabled: false
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/readarr-develop
    tag: 0.4.18.2805@sha256:ed51f754b6baeb58d86b4fae4872dbcc8e9799c01788a9023c3d7704ae549333   
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: readarr-env
  - configMapRef:
      name: elfbot-readarr
      optional: true    
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-readarraudio" # Reload the deployment every time the rclones change
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: readarraudio
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: readarraudio
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    tmp-readarr-backup:
      enabled: true
      type: emptyDir
      mountPath: /tmp/readarr_backup
      sizeLimit: 32Mi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-readarraudio
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory          
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: readarraudio
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup        
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8787
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: readarraudio
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault     
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: readarraudio
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for database to use 
        mkdir -p /config/postgresql/database

        # Set auth to external
        sed -i  "s|<AuthenticationMethod>None</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml
        sed -i  "s|<AuthenticationMethod>Basic</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: readarraudio
      resources: *default_resources
      securityContext: *default_securitycontext
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: readarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: readarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: readarr/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi     
    database-backup:
      image: ghcr.io/elfhosted/readarr-develop:0.4.18.2805@sha256:ed51f754b6baeb58d86b4fae4872dbcc8e9799c01788a9023c3d7704ae549333   
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: readarr-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: readarr/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi         
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 16Gi
  probes: # need a long startup for database migrations
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 3000
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 8787
        timeoutSeconds: 1      

plex: &app_plex
  enabled: false
  priorityClassName: tenant-streaming
  podLabels:
    app.elfhosted.com/name: plex
    app.elfhosted.com/class: debrid
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M" # tested with _kilos in Discord on a 97Mbit remux
    container.apparmor.security.beta.kubernetes.io/transcode-killer: unconfined    
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    runAsUser: 568
    runAsGroup: 568    
    # fsGroupChangePolicy: "Always"
    seccompProfile:
      type: Unconfined
    supplementalGroups:
    - 993
    - 44
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-zurg,elfbot-plex,elfbot-decypharr,elfbot-debridav,elfbot-imagemaid,plex-config,imagemaid-env,plex-tinyproxy-conf" # Reload the deployment every time the rclones change
  image:
    registry: ghcr.io
    repository: elfhosted/plex
    tag: 1.41.8.9834-071366d65@sha256:dfaa43dc80599b36032386821517627b42e914b5716ca10e79d994dd3a534872
  persistence: &app_plex_persistence
    <<: *storagemounts
    backup: &backup
      enabled: true
      type: custom
      mountPath: /backup
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/Library/Application Support/Plex Media Server/Logs
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-1g    
    phototranscoder:
      enabled: true
      mountPath: /config/Library/Application Support/Plex Media Server/Cache/PhotoTranscoder
      type: emptyDir
      sizeLimit: 50Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex
          optional: true
    render-device: &streamer_render_device
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri"
      mountPath: "/dev/dri"
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory     
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 32400
  envFrom:
  - configMapRef:
      name: plex-config
  - secretRef:
      name: gatus-smtp-config      
  - configMapRef:
      name: elfbot-plex
      optional: true
  resources:
    requests:
      cpu: "100m"
      memory: 1Gi
    limits:
      cpu: "2" # 1.5 works, but results in buffering when playback starts, see https://github.com/elfhosted/charts/issues/501
      memory: 4Gi
  initContainers:
    restart-with-zurg:
      image: ghcr.io/elfhosted/zurg-rc:2025.06.26.0032-nightly@sha256:31877ca1e4195918cfbedcfb62c5e6e06d499b532e6a2414b3b316d3d072e8d5
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when zurg is updated"
    restart-with-decypharr:
      image: ghcr.io/elfhosted/decypharr:v1.0.4@sha256:907e38dcc531f75976dfb2840651897a84da27091414eca91fd5bac011ed1e55
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when decypharr is updated"
    restart-with-debridav:
      image: ghcr.io/elfhosted/debridav:0.10.1@sha256:67748eff91c1c318827284bb31a9f39c2ce10b22e29e58ec4f65659152fd8cfe
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when debridav is updated"        
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex
      - mountPath: /tmp
        name: tmp
    dbrepair:
      image: ghcr.io/elfhosted/plex:1.41.8.9834-071366d65@sha256:dfaa43dc80599b36032386821517627b42e914b5716ca10e79d994dd3a534872
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        if [ $REPAIR_DB_ON_START == "true" ]; then
          # Repair the database if it exists
          if [ -f /config/Library/Application\ Support/Plex\ Media\ Server/Plug-in\ Support/Databases/com.plexapp.plugins.library.db ]; then
            echo "Repairing Plex database"
            /usr/local/bin/DBRepair.sh --databases /config/Library/Application\ Support/Plex\ Media\ Server/Plug-in\ Support/Databases/ --sqlite "/usr/lib/plexmediaserver/Plex SQLite" automatic
          else
            echo "No Plex database found, skipping repair"
          fi
        else
          echo "REPAIR_DB_ON_START is not set to true, skipping database repair"
        fi
      envFrom:
      - configMapRef:
          name: elfbot-plex
          optional: true        
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: plex
      - mountPath: /phototranscoder
        name: phototranscoder
      # can't use default resources because the ephemeral limit kicks out /phototranscoder later
      # resources: *default_resources
      securityContext: *default_securitycontext       
    setup-warp: &setup_warp
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      env:
        CHANGE_CONTAINER_PORTS: "True"
        HTTP_PORT: "3002"
        HTTPS_PORT: "3003"
      securityContext: *speedtest_securitycontext
    transcode-killer:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        python3 /scripts/transcode-killer.py        
      volumeMounts:
      - mountPath: /var/log/
        name: logs
        subPath: plex/transcode-killer
      - mountPath: /dev/dri
        name: render-device
      resources: *default_resources
      securityContext:
        capabilities:
          add:
            - SYS_ADMIN
            - SYS_PTRACE
        seccompProfile:
          type: Unconfined
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "32400,3000,8888,3001,3002"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /web/index.html
          port: 32400
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /web/index.html
          port: 32400
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /web/index.html
          port: 32400
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10


plexrequests:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/wests-blackhole-script
    tag: v1.5.1@sha256:3f7a6e09de005b57fec75762422c0880286a8c4d93a9871b3df1e88866fc8dc1
  command: [ "/request.sh" ]
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plexrequests,plexrequests-env"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568 # need this so that the bootstrap can run
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: plexrequests-env
  - configMapRef:
      name: elfbot-plexrequests
      optional: true    
  persistence:
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plexrequests
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    app-cache:
      mountPath: /app/cache
      enabled: true
      type: emptyDir
      volumeSpec:
        medium: Memory                      
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 32501
  resources:
    requests:
      cpu: 0m
      memory: 10Mi
    limits:
      cpu: 0.5
      memory: 1Gi
  additionalContainers:
    auth:
      image: ghcr.io/elfhosted/wests-blackhole-script:v1.5.1@sha256:71c8321c12ec2a643aca899c38f144fb31c4ba302612e139499ca966d633a316
      command: [ "/auth.sh" ]
      envFrom:
      - configMapRef:
          name: plexrequests-env
      - configMapRef:
          name: elfbot-plexrequests
          optional: true    
      volumeMounts:
      - name: config
        mountPath: /config
        subPath: plexrequests     
    watchlist:
      image: ghcr.io/elfhosted/wests-blackhole-script:v1.5.1@sha256:71c8321c12ec2a643aca899c38f144fb31c4ba302612e139499ca966d633a316
      command: [ "/watchlist.sh" ]
      envFrom:
      - configMapRef:
          name: plexrequests-env
      - configMapRef:
          name: elfbot-plexrequests
          optional: true
      volumeMounts:
      - name: config
        mountPath: /config
        subPath: plexrequests             

jellyfin: &app_jellyfin
  hostname: elfhosted
  image:
    repository: ghcr.io/elfhosted/jellyfin-dev
    tag: 10.10.7@sha256:7ad104992e7908aa85b27cb4360886858caf406bc54abdc452b5b69dcce99b17
  enabled: false
  podLabels:
    app.elfhosted.com/name: jellyfin
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M" # tested with _kilos in Discord on a 97Mbit remux
  priorityClassName: tenant-streaming
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    runAsUser: 568
    runAsGroup: 568    
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jellyfin,jellyfin-env,elfbot-zurg,elfbot-decypharr,elfbot-debridav" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence: &app_jellyfin_persistence
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: "/config/log/"
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs             
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-1g  
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jellyfin
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  initContainers:
    restart-with-zurg:
      image: ghcr.io/elfhosted/zurg-rc:2025.06.26.0032-nightly@sha256:31877ca1e4195918cfbedcfb62c5e6e06d499b532e6a2414b3b316d3d072e8d5
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when zurg is updated"
    restart-with-decypharr:
      image: ghcr.io/elfhosted/decypharr:v1.0.4@sha256:907e38dcc531f75976dfb2840651897a84da27091414eca91fd5bac011ed1e55
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when decypharr is updated"
    restart-with-debridav:
      image: ghcr.io/elfhosted/debridav:0.10.1@sha256:67748eff91c1c318827284bb31a9f39c2ce10b22e29e58ec4f65659152fd8cfe
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when debridav is updated"   
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jellyfin
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: false # necessary for probes
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
  resources:
    requests:
      cpu: "50m"
      memory: 1Gi
    limits:
      cpu: 2
      memory: 4Gi
  envFrom:
  - configMapRef:
      name: jellyfin-env
  - configMapRef:
      name: elfbot-jellyfin
      optional: true          
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      securityContext: *speedtest_securitycontext
    jellyfixer:
      image: quay.io/xsteadfastx/jellyfixer:latest
      env:
        JELLYFIXER_INTERNAL_URL: http://jellyfin:8096
        JELLYFIXER_EXTERNAL_URL: https://{{ .Release.Name }}-jellyfin.elfhosted.com
      

jellyfinranger:
  <<: *app_jellyfin
  podLabels:
    app.elfhosted.com/name: jellyfin
    app.elfhosted.com/class: dedicated
  podAnnotations:
    kubernetes.io/egress-bandwidth: "500M"
  enabled: false
  automountServiceAccountToken: false
  resources: *ranger_streamer_resources
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-jellyfin,elfbot-all"
  persistence:
    <<: *app_jellyfin_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

emby: &app_emby
  hostname: elfhosted
  image:
    registry: ghcr.io
    repository: elfhosted/emby
    tag: 4.9.1.3@sha256:6a49f56c7293f903813b30c7ca3923a98d69621f0d86e06796c76b0e95c53ac6
  enabled: false
  priorityClassName: tenant-streaming
  podLabels:
    app.elfhosted.com/class: debrid
    app.elfhosted.com/name: emby
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M" # tested with _kilos in Discord on a 97Mbit remux
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem:
    privileged: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-emby,emby-env,elfbot-zurg,elfbot-decypharr,elfbot-debridav" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  envFrom:
  - configMapRef:
      name: emby-env
  - configMapRef:
      name: elfbot-emby
      optional: true      
  persistence: &app_emby_persistence
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: "/config/log/"
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs                
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-1g  
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-emby
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  initContainers:
    restart-with-zurg:
      image: ghcr.io/elfhosted/zurg-rc:2025.06.26.0032-nightly@sha256:31877ca1e4195918cfbedcfb62c5e6e06d499b532e6a2414b3b316d3d072e8d5
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when zurg is updated"
    restart-with-decypharr:
      image: ghcr.io/elfhosted/decypharr:v1.0.4@sha256:907e38dcc531f75976dfb2840651897a84da27091414eca91fd5bac011ed1e55
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when decypharr is updated"
    restart-with-debridav:
      image: ghcr.io/elfhosted/debridav:0.10.1@sha256:67748eff91c1c318827284bb31a9f39c2ce10b22e29e58ec4f65659152fd8cfe
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when debridav is updated"   
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: emby
      - mountPath: /tmp
        name: tmp
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      securityContext: *speedtest_securitycontext
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: false # necessary for probes
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
  resources:
    requests:
      cpu: "50m"
      memory: 1Gi
    limits:
      cpu: 2
      memory: 4Gi

embyranger:
  <<: *app_emby
  podLabels:
    app.elfhosted.com/name: emby
    app.elfhosted.com/class: dedicated
  podAnnotations:
    kubernetes.io/egress-bandwidth: "500M"
  enabled: false
  automountServiceAccountToken: false
  resources: *ranger_streamer_resources
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-emby,elfbot-all"
  persistence:
    <<: *app_emby_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

homer:
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    runAsNonRoot: false
    runAsUser: 568
    runAsGroup: 568
  podSecurityContext:
    runAsNonRoot: false
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "Always"
  automountServiceAccountToken: false
  image:
    repository: ghcr.io/elfhosted/tooling
    tag: focal-20230605@sha256:6088a1e9fc0ce83aec9910af0899661c23b5f2025428d7da631b9b9390241b6c
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
  podLabels:
    app.elfhosted.com/role: nodefinder # let this be an anchor for replicationdestinations
  persistence:
    <<: *storagemounts
    logs:
      enabled: true
      type: custom
      mountPath: /logs
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    config:
      enabled: true
      type: custom
      mountPath: /config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config          
    backup:
      enabled: true
      type: custom
      mountPath: /backup
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    config-yml:
      enabled: "true"
      subPath: "config.yml"
      type: "custom"
      volumeSpec:
        configMap:
          name: homer-config
    custom-css:
      enabled: "true"
      subPath: "custom-css"
      type: "custom"
      volumeSpec:
        configMap:
          name: homer-config
    gatus-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: gatus-config
    disk-usage:
      enabled: "true"
      mountPath: "/usr/local/bin/disk_usage.sh"
      subPath: "disk_usage.sh"
      type: "custom"
      volumeSpec:
        configMap:
          name: homer-config
    message:
      enabled: true
      type: emptyDir
      mountPath: /www/assets/message
  command:
  - /bin/bash
  - /usr/local/bin/disk_usage.sh
  additionalContainers:
    ui:
      image: ghcr.io/elfhosted/homer:v25.05.2@sha256:60772bd0292281282161f10aa2bea105bba344158937f1c0022c6986ef5aedc3
      imagePullPolicy: IfNotPresent
      volumeMounts:
      - mountPath: /www/assets/config.yml
        name: config-yml
        subPath: "config.yml"
      - mountPath: /www/assets/custom.css
        name: custom-css
        subPath: "custom.css"
      - mountPath: /www/assets/message
        name: message
      - mountPath: /www/assets/backgrounds
        name: config
        subPath: homer/backgrounds
        readOnly: true
      resources: *default_resources
      securityContext: *default_securitycontext
  configmap:
    config:
      # -- Store homer configuration as a ConfigMap, but don't specify any config, since we'll supply our own
      enabled: false
  controller:
    replicas: 1
    strategy: RollingUpdate
    rollingUpdate:
      unavailable: 1
    annotations:
      configmap.reloader.stakater.com/reload: "homer-config, elfbot-homer" # Reload the deployment every time the yaml config changes
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 200m
      memory: 1Gi

traefikforwardauth:
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  whitelist: admin@elfhosted.com
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  controller:
    replicas: 1
    annotations:
      configmap.reloader.stakater.com/reload: traefik-forward-auth-config
    strategy: RollingUpdate
  image:
    repository: ghcr.io/elfhosted/traefik-forward-auth
    pullPolicy: IfNotPresent
    tag: 3.1.0@sha256:19cd990fae90c544100676bc049f944becc8c454639e57d20f6f48e27de90776

  middleware:
    # middleware.enabled -- Enable to deploy a preconfigured middleware
    enabled: false

  envFrom:
  - configMapRef:
      name: traefik-forward-auth-config

  ingress:
    main:
      enabled: false

  service:
    main:
      enabled: true # necessary for probes

  resources:
    requests:
      cpu: 0m
      memory: 6Mi
    limits:
      cpu: 1
      memory: 32Mi

gatus:
  image:
    repository: ghcr.io/elfhosted/gatus
    tag: 5.18.1@sha256:67487d56837e84b2ef5fea3ab91ec06f923341e2edd07f3a18e815cb8bc4524b
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 20Mi
    limits:
      cpu: 1
      memory: 128Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  controller:
    # strategy: RollingUpdate
    annotations:
      configmap.reloader.stakater.com/reload: "gatus-config"
  env:
    GATUS_CONFIG_PATH: /config/config.yaml
    SMTP_FROM: 'health@elfhosted.com'
    SMTP_PORT: 587
  persistence:
    gatus-config:
      enabled: "true"
      mountPath: /config
      type: "custom"
      volumeSpec:
        configMap:
          name: gatus-config
    config:
      enabled: true
      type: custom
      mountPath: /data/
      subPath: gatus
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  envFrom:
  - secretRef:
      name: gatus-smtp-config
  configmap:
    config:
      # -- Store homer configuration as a ConfigMap, but don't specify any config, since we'll supply our own
      enabled: false

gotify:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/gotify
    tag: 2.5.0@sha256:f5c89bb3ccbf857bca816e4550b46c442cfb6c0eae0f081975ba5c5099779c3f
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-gotify"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  env:
    GOTIFY_SERVER_PORT: 8080
  resources:
    requests:
      cpu: 0m
      memory: 32Mi
    limits:
      cpu: 1
      memory: 64Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/data/
      subPath: gotify
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-gotify
          optional: true
    tmp: *tmp # Avoids issues with readOnlyRootFilesystem
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: gotify
      - mountPath: /tmp
        name: tmp

flaresolverr: &app_flaresolverr
  enabled: false
  podLabels:
    app.elfhosted.com/name: flaresolverr
  image:
    registry: ghcr.io
    repository: elfhosted/byparr
    tag: v1.2.1@sha256:38f47cca6a8300afbb0f704e7574188e8b836b59a9359ecb9820c4b2366954e1
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # makes node unhappy
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-flaresolverr"
  # persistence:
  #   config:
  #     enabled: true
  #     type: custom
  #     mountPath: /app/screenshots
  #     volumeSpec:
  #       persistentVolumeClaim:
  #         claimName: config
  #     subPath: byparr/sreenshots
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 1000
    # runAsGroup: 1000
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 600m
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8191

seafile:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/seafile
    tag: 10.0.1@sha256:810be7e7c5a6ae2c73cf21d2ec48226cbb1afbc86ead3a59738c3eef593cb9f6
  priorityClassName: tenant-bulk
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't seem to work with seafile, no output from container either
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 568 # has to run as root, see https://github.com/haiwen/seafile-docker/issues/86
    # runAsGroup: 568
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-seafile"
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1024m
      memory: 512Mi
  env:
    # -- Set the container timezone
    TIME_ZONE: Etc/UTC
    # -- The hostname of your database
    DB_HOST: "{{ .Release.Name }}-seafile-mysql"
    # -- The root password for mysql (used for initial setup)
    DB_ROOT_PASSWD: wLu5UUuT@3Zu33eT
    # -- The initial admin user's password
    SEAFILE_ADMIN_PASSWORD: changeme
    # -- The hostname for the server (set to your ingress hostname)
    SEAFILE_SERVER_HOSTNAME: "{{ .Release.Name }}-seafile.elfhosted.com"
    SEAFILE_SERVER_LETSENCRYPT: false
    FORCE_HTTPS_IN_CONF: true
    NON_ROOT: true # yes, and with our custom image, this runs the seafile/seahub components as user 568
  envFrom:
  - configMapRef:
      name: seafile-config
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # debug whether this gets us probes
  memcached:
    nameOverride: seafile-memcached
    enabled: true
  mysql:
    nameOverride: seafile-mysql
    enabled: true
    architecture: standalone
    commonAnnotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-seafile"

    auth:
      rootPassword: "wLu5UUuT@3Zu33eT"
      database: "seafile"
      username: "seafile"
      password: "nXCXSmqU4TMk3okD"

    primary:
      readinessProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      livenessProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      startupProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      persistence:
        enabled: true
        existingClaim: config
        subPath: seafile/database
      resources:
        requests:
          cpu: 5m
          memory: 1Gi
        limits:
          cpu: 2
          memory: 1024Mi
      containerSecurityContext:
        enabled: true
        seccompProfile:
          type: RuntimeDefault
        runAsUser: 568
        runAsGroup: 568
      podSecurityContext:
        enabled: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
      extraVolumeMounts:
      - mountPath: /opt/bitnami/mysql/tmp/
        name: tmp
      extraVolumes:
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi
      - name: backup-database-script
        configMap:
          name: seafile-backup

  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /shared/seafile
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
      subPath: seafile/data
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-seafile
          optional: true

tunarr:
  enabled: false
  image:
    registry: ghcr.io
    repository: chrisbenincasa/tunarr
    tag: 0.20.4-vaapi
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    privileged: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-tunarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podAnnotations:
    kubernetes.io/egress-bandwidth: "128M"  
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 1
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /root/.local
      subPath: tunarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-tunarr
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    media: # in case users use /tmp
      enabled: true
      mountPath: /streams
      type: emptyDir
      sizeLimit: 50Gi
    cache: # in case users use /tmp
      enabled: true
      mountPath: /root/.cache/pkg
      type: emptyDir
      sizeLimit: 10Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: tunarr
      - mountPath: /tmp
        name: tmp

ersatztv:
  enabled: false
  image:
    registry: docker.io
    repository: jasongdove/ersatztv
    tag: develop-vaapi@sha256:04bfca875a7da7c7ba15bca1e9c79a3b6b26d4ead20a5d0ae22ad037700b03d5
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    privileged: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-erzatztv"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 1
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8409
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /root/.local/share/ersatztv
      subPath: ersatztv
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    transcode:
      enabled: true
      type: custom
      mountPath: /root/.local/share/etv-transcode
      volumeSpec: *volumespec_ephemeral_volume_50g          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-tunarr
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: ersatztv
      - mountPath: /tmp
        name: tmp

threadfin:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/threadfin
    tag: 1.2.34@sha256:945c7ccdbc0a56473d4f8d31c41e81c96a693b09cd15fd9cb984704fdafe8b49
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-threadfin"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 1
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 34400
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /home/threadfin/conf/
      subPath: threadfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-tunarr
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: threadfin
      - mountPath: /tmp
        name: tmp

thelounge:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/thelounge
    tag: "4.4.3@sha256:74ae8d9fc36d5a8396bb70cfaa58d222730fa69d2f71c3e2ec3ae010f2a0b264"
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-thelounge"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because the node modules in /app try to create files
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  env:
    TZ: UTC
    THELOUNGE_HOME: /config/thelounge # avoids attempts to chown /config
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 40Mi
    limits:
      cpu: 100m
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9000
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: thelounge
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-thelounge
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: thelounge
      - mountPath: /tmp
        name: tmp
    create-user:
      image: ghcr.io/elfhosted/thelounge:4.4.3@sha256:74ae8d9fc36d5a8396bb70cfaa58d222730fa69d2f71c3e2ec3ae010f2a0b264
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e


        # If we don't already have a config, create one
        if [ ! -f /config/thelounge/config.json ];
        then
          mkdir -p /config/thelounge
          cp /config-bootstrap/* /config/thelounge/ -R
        fi

        # If we don't already have a user, create one
        if [ ! -f /config/thelounge/users/${USERNAME}.json ];
        then
          thelounge add ${USERNAME} --password ${PASSWORD}
        fi
      volumeMounts:
      - mountPath: /config
        subPath: thelounge
        name: config
      env:
      - name: THELOUNGE_HOME
        value: /config/thelounge # avoids attempts to chown /config
      - name: USERNAME
        valueFrom:
          configMapKeyRef:
            name: elfhosted-user-config
            key: USERNAME
      - name: PASSWORD
        value: ireadthedocs
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true

symlinkcleaner:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/symlink-cleaner
    tag: v1.4.7@sha256:2b980b95269aa534c46bc735a613efe1934af948d602e281958ac2891006acb6
  imagePullSecrets:
  - name: ghcr-io-elfhosted
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-symlink-cleaner,symlink-cleaner-example-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because the node modules in /app try to create files
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 40Mi
    limits:
      cpu: 100m
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5000
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 5000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 5000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 45
        httpGet:
          path: /health
          port: 5000
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 5
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/config
      subPath: symlink-cleaner
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /app/logs
      subPath: symlink-cleaner
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-symlink-cleaner
          optional: true          
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: symlink-cleaner-example-config          
  envFrom:
  - configMapRef:
      name: symlink-cleaner-env
  - configMapRef:
      name: elfbot-symlink-cleaner
      optional: true    
  initContainers:  
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: symlink-cleaner
      - mountPath: /tmp
        name: tmp      
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e
        # If we don't already have an example config, create one
        if [ ! -f /config/config.json ];
        then
          cp /bootstrap/config.json /config/
        fi
      volumeMounts:
      - mountPath: /config/
        name: config
        subPath: symlink-cleaner
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext: *default_securitycontext  

discovarr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/discovarr
    tag: v1.2.0@sha256:fd33f143f23635460f4ebdad07ba665bd7eb67e27ea7a3189db1d148dd5169e8
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: discovarr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-discovarr,discovarr-config"
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: discovarr-env
  - configMapRef:
      name: elfbot-discovarr
      optional: true  
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: discovarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-discovarr
          optional: true
    tmp: *tmp
    backup: *backup
    cache:
      mountPath: /cache
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi 
    static:
      mountPath: /app/server/static
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi          
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: discovarr
      - mountPath: /tmp
        name: tmp
    setup:
      image:  ghcr.io/elfhosted/discovarr:v1.2.0@sha256:fd33f143f23635460f4ebdad07ba665bd7eb67e27ea7a3189db1d148dd5169e8
      command:
      - /bin/sh
      - -c
      - |
        set -x
        set -e

        # Directory where your built frontend assets are located (copied from client/dist)
        # This path is inside the Docker container.
        STATIC_ASSETS_DIR="/static"

        cp /app/server/static/* /static/ -rf
        echo "INFO: Replacing placeholder '${PLACEHOLDER}' with '${VITE_DISCOVARR_URL}' in JS and HTML files..."

        # Find all .js and .html files in the static assets directory and its subdirectories
        # and replace the placeholder.
        # Using '#' as a delimiter for sed to avoid issues with slashes ('/') in the URL.
        # The -print0 and xargs -0 pattern handles filenames with spaces or special characters.
        find "${STATIC_ASSETS_DIR}" -type f \( -name "*.js" -o -name "*.html" \) -print0 | \
          xargs -0 sed -i "s#__API_ENDPOINT__#${VITE_DISCOVARR_URL}#g"
      volumeMounts:
      - mountPath: /static
        name: static
      envFrom:
      - configMapRef:
          name: discovarr-env
      - configMapRef:
          name: elfbot-discovarr
          optional: true          
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 0m
      memory: 175Mi
    limits:
      cpu: 500m
      memory: 2Gi

overseerr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/overseerr
    tag: 1.34.0@sha256:d9cfaf805e8512684cec4b5c897e372d898eeff522cef0801f3245cb3d00caf6
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: overseerr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-overseerr,overseerr-config"
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: overseerr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-overseerr
          optional: true
    tmp: *tmp
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
    overseerr-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: overseerr-config
          optional: true 
    run:
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi                   
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: overseerr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        # run the setup script from the configmap, so that we can make templated changes
        bash /bootstrap/setup.sh
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: overseerr
      - name: overseerr-config
        mountPath: "/bootstrap/"
    # We do this so that we can override the /app/jellyseer/public path with our own, allowing the user to customize the branding
    copy-branding:
      image: ghcr.io/elfhosted/overseerr:1.34.0@sha256:d9cfaf805e8512684cec4b5c897e372d898eeff522cef0801f3245cb3d00caf6
      command:
        - /bin/bash
        - -c
        - |
          mkdir -p /config/branding
          cp --no-clobber -rf /app/overseerr/public/logo_* /config/branding
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: overseerr
      resources: *default_resources
      securityContext: *default_securitycontext
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5055
  resources:
    requests:
      cpu: 0m
      memory: 175Mi
    limits:
      cpu: 2
      memory: 2Gi
  additionalContainers:
    branding:
      image: nginxinc/nginx-unprivileged
      volumeMounts:
      - mountPath: /usr/share/nginx/html
        name: config
        subPath: overseerr/branding
        readOnly: true
      - mountPath: /tmp
        name: tmp
      - mountPath: /run
        name: run
      resources: *default_resources
      securityContext: *default_securitycontext

jellyseerr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/jellyseerr
    tag: 2.7.0@sha256:a337e2e2fe5d1b75e32ac83a6550c700890fbe2240f5cec311c0c57fc70768e7
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: jellyseerr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jellyseerr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: jellyseerr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jellyseerr
          optional: true
    tmp: *tmp
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
    jellyseerr-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: jellyseerr-config
          optional: true
    run:
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi            
  envFrom:
  - configMapRef:
      name: jellyseerr-env
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jellyseerr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        # run the setup script from the configmap, so that we can make templated changes
        bash /bootstrap/setup.sh
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: jellyseerr
      - name: jellyseerr-config
        mountPath: "/bootstrap/"        
    # We do this so that we can override the /app/jellyseer/public path with our own, allowing the user to customize the branding
    copy-branding:
      image: ghcr.io/elfhosted/jellyseerr:2.7.0@sha256:a337e2e2fe5d1b75e32ac83a6550c700890fbe2240f5cec311c0c57fc70768e7
      command:
        - /bin/ash
        - -c
        - |
          mkdir -p /config/branding
          cp --no-clobber -rf /app/public/logo_* /config/branding/
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: jellyseerr
      resources: *default_resources
      securityContext: *default_securitycontext
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5055
  resources:
    requests:
      cpu: 0m
      memory: 160Mi
    limits:
      cpu: 2
      memory: 1Gi
  additionalContainers:
    branding:
      image: nginxinc/nginx-unprivileged
      volumeMounts:
      - mountPath: /usr/share/nginx/html
        name: config
        subPath: jellyseerr/branding
        readOnly: true
      - mountPath: /tmp
        name: tmp
      - mountPath: /run
        name: run
      resources: *default_resources
      securityContext: *default_securitycontext

jellystat:
  enabled: false
  podLabels:
    app.elfhosted.com/name: jellystat
  image:
    repository: ghcr.io/elfhosted/jellystat
    tag: 1.1.6@sha256:39bb3980570b6dd56fac1becbb52cde40f821d6ac73eaef97dd9c3d9ec9d97bd
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jellystat,jellystat-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with ilikedanger currently
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: jellystat-env
  - configMapRef:
      name: elfbot-jellystat
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 20Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: jellystat/config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    backup:
      enabled: true
      type: custom
      mountPath: /app/backup-data
      subPath: jellystat
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jellystat
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jellystat
      - mountPath: /tmp
        name: tmp
    setup-postgres:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/database
        chown elfie:elfie /config/database -R

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: jellystat
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault    
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: jellystat
        - name: POSTGRES_DB
          value: jellystat
        - name: POSTGRES_USER
          value: jellystat
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: jellystat/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 1Gi

booklore:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/booklore
    tag: v0.29.0@sha256:72b023b6cf2d4c42b32380e917ebadc071b7a6b48ea34a83f3106a0379963a06
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-booklore,booklore-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    allowPrivilegeEscalation: false
    runAsNonRoot: false # because we need the permission-fixer to run as root
    capabilities:
      drop:
      - ALL        
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: booklore-env
  - configMapRef:
      name: elfbot-booklore
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 20Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6060    
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/data
      subPath: booklore
      volumeSpec:
        persistentVolumeClaim:
          claimName: config        
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-booklore
          optional: true
  command: 
  - java 
  - -jar 
  - /app/app.jar     
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: booklore
      - mountPath: /tmp
        name: tmp
    setup-mariadb:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/database
        chown elfie:elfie /config/database -R

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: booklore
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault            
  additionalContainers:
    frontend:
      image: ghcr.io/elfhosted/booklore:v0.29.0@sha256:72b023b6cf2d4c42b32380e917ebadc071b7a6b48ea34a83f3106a0379963a06
      command: 
      - /usr/sbin/nginx 
      - -g 
      - "daemon off;"
      securityContext:
        readOnlyRootFilesystem: false
        runAsUser: 0 # nginx insists on running as root
    database:
      image: mariadb:11 
      env:
        - name: MYSQL_ROOT_PASSWORD
          value: booklore
        - name: MYSQL_DATABASE
          value: booklore
        - name: MYSQL_USER
          value: booklore
        - name: MYSQL_PASSWORD
          value: booklore
      volumeMounts:
      - mountPath: /var/lib/mysql
        name: config
        subPath: booklore/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 1Gi

peertube:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/peertube-cli
    tag: v7.2.1@sha256:c331f0c3945217c51e907b0f2ce5e1ee23522bfb06f854cddfcd3b2a32854b5b
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-peertube,peertube-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    privileged: true # needed for transcoding
    runAsNonRoot: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    supplementalGroups:
    - 993      
    allowPrivilegeEscalation: false
    runAsNonRoot: false
    capabilities:
      drop:
      - ALL        
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 20Mi
    # no limits (this is only usable with a nazgul)
    # limits:
    #   cpu: 1
    #   memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9000
    probes:
      liveness:
        custom: true
        enabled: true
        spec:
          failureThreshold: 5
          httpGet:
            path: /api/v1/ping
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 10
      readiness:
        custom: true
        enabled: true
        spec:
          failureThreshold: 5
          httpGet:
            path: /api/v1/ping
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 10
      startup:
        custom: true
        enabled: true
        spec:
          failureThreshold: 5
          httpGet:
            path: /api/v1/ping
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 10          
  persistence:
    <<: *storagemounts
    logs:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    config:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
      mountPath: /home/elfie
      subPath: peertube/cli
    backup:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-peertube
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory
    data-tmp:
      enabled: "true"
      type: emptyDir      
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: peertube
      - mountPath: /tmp
        name: tmp
    setup-postgres:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/database
        chown elfie:elfie /config/database -R

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: peertube
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault    
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: peertube
        - name: POSTGRES_DB
          value: peertube
        - name: POSTGRES_USER
          value: peertube
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: peertube/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 1Gi
    redis:
      image: docker.io/redis:7.4-alpine@sha256:ee9e8748ace004102a267f7b8265dab2c618317df22507b89d16a8add7154273
      resources:
        requests:
          cpu: 15m
          memory: 200Mi
        limits:
          cpu: 100m
          memory: 4Gi
      volumeMounts:
      - mountPath: /data
        name: config
        subPath: peertube/redis
    ui:
      image: ghcr.io/elfhosted/peertube-nonroot:v7.2.1@sha256:a46b7da39faed804e7ba7e3e253eec133967336da0c91fcde372d76ef9aa29e2
      volumeMounts:
      - mountPath: /data
        name: config
        subPath: peertube/app
      - mountPath: /data/logs
        name: logs
        subPath: peertube
      - mountPath: /data/tmp
        name: data-tmp
      - mountPath: /dev/dri/renderD128
        name: render-device
      envFrom:
      - configMapRef:
          name: peertube-env
      - configMapRef:
          name: elfbot-peertube
          optional: true        
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
        privileged: true # needed for transcoding
        runAsNonRoot: true
        
audiobookshelf:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/audiobookshelf
    tag: v2.25.1@sha256:c3ee892604484658fdbd3c6a37972a7d41a0566e4f97ea058b6b66587a2c7242
  priorityClassName: tenant-streaming
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: audiobookshelf
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-audiobookshelf,elfbot-all"
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: audiobookshelf
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    # never used, just satisfies startup scripts
    metadata:
      enabled: true
      type: emptyDir
      mountPath: /metadata
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-audiobookshelf
          optional: true
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  env:
    METADATA_PATH: /config/metadata
    SOURCE: ElfHosted
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: audiobookshelf
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 2
      memory: 1Gi

storyteller:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/storyteller
    tag: v1.4.0-ctc.7@sha256:1081edd3ec29b3ef32df8887e5e388f7686ce288bde4c7b9cff8e448e12a26ae
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-storyteller,elfbot-all"
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data/
      subPath: storyteller
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    # never used, just satisfies startup scripts
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-storyteller
          optional: true
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
    cache: 
      enabled: true
      type: emptyDir
      mountPath: /app/.next/standalone/web/.next/cache         
    whisper-builds:
      enabled: true
      type: emptyDir
      mountPath: /app/.next/standalone/web/whisper-builds
    home-elfie:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie
  envFrom:
  - configMapRef:
      name: storyteller-env
  - secretRef:
      name: storyteller-env      
  - configMapRef:
      name: elfbot-storyteller
      optional: true          
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: storyteller
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8001
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 12Gi


audiobookrequest:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/audiobookrequest
    tag: 1.4.8@sha256:21043d365b7964b51c2e4017eae11c2998997cdc722f65ff82b4e6a761ab7d41
  priorityClassName: tenant-streaming
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-audiobookrequest,elfbot-all"
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: audiobookrequest
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-audiobookrequest
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: audiobookrequest
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 2
      memory: 1Gi

audiobookbayautomated:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/audiobookbay-automated
    tag: rolling@sha256:1c44d86d4f1b6bc93368bc3f5132ea1a061df2a6db8b013ba6cb4ab81377cd6b
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-audiobookbay-automated"
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: audiobookbay-automated
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-audiobookbay-automated
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: audiobookbay-automated
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5078
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 2
      memory: 1Gi      
  envFrom:
  - configMapRef:
      name: audiobookbay-automated-env      

openbooks:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/openbooks
    tag: 4.5.0@sha256:c17d8e86d55e35cd1edb20fe1e1ff95a8e93cbef4c571ead2bfeb07276845477
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  persistence:
    <<: *storagemounts
  command:
  - /bin/bash
  - -c
  - |
    set -x
    set -e
    sleep 5s
    USER=$(tr -dc A-Za-z0-9 </dev/urandom | head -c 13 ; echo '')
    ./openbooks server \
      --dir ${DATA_DIR-/tmp} \
      --port 8000 \
      --name $USER \
      --tls=false \
      --persist \
      --server irc.irchighway.net:6661 \
      --no-browser-downloads \
      --debug
  envFrom:
  - configMapRef:
      name: elfbot-openbooks
      optional: true
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-openbooks"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 2
      memory: 1Gi

vaultwarden:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/vaultwarden
    tag: 1.34.1@sha256:1637be9c0a684e74c890dfba9c5d4243e123f7127536a299d62bb2a6c0e105c3
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-vaultwarden"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: elfbot-vaultwarden
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: vaultwarden
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-vaultwarden
          optional: true
    tmp: *tmp
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: vaultwarden
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 1
      memory: 1Gi


notifiarr:
  enabled: false
  hostname: elfhosted
  image:
    repository: ghcr.io/elfhosted/notifiarr
    tag: 0.8.3@sha256:16d9f57524a249fd6181b9282a0c2c66e24caf417b692199d4dec2681784fc4c
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because the node modules in /app try to create files
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-notifiarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 2
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5454
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: notifiarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: notifiarr-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-notifiarr
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: notifiarr
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [ ! -f /config/notifiarr.conf ];
        then
          cp /bootstrap/notifiarr.conf /config/
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: notifiarr
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true

shoko:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/shokoserver
    tag: v5.1.0@sha256:a386f39bcfd7c1e8f9c38b4171c8e59162e3e1b2c579728465fd4ea52978a69e
  env:
    PUID: 568
    PGID: 568
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-shoko"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # again, s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /home/shoko/.shoko/
      subPath: shoko
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-shoko
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: shoko
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8111
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi

filebot:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/filebot-node
    tag: 0.4.8@sha256:2f35281e7f6d4a1566dfe6427dae83480c76d1cebe1e5d6d2379064db1846ba7
  env:
    PUID: 568
    PGID: 568
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-filebot"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # again, s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: filebot
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-filebot
          optional: true

    tmp: # to avoid errors about storing java prefs
      enabled: true
      type: emptyDir
      mountPath: /home/seedy
      sizeLimit: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: filebot
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5452
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 2
      memory: 1Gi

rpdb:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rpdb
    tag: 0.3.3@sha256:fc651aeb123bedbcb92eeda3227b51005540b3c350bf15640c7928f5e8fd1d4b
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,storage-changed,elfbot-rpdb"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /.config
      subPath: rpdb
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rpdb
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rpdb
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8750
  resources:
    requests:
      cpu: 0m
      memory: 40Mi
    limits:
      cpu: 1
      memory: 1Gi

kometa: &app_kometa
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/kometa
    tag: v2.2.0@sha256:d0d56d58de2ef623d7167e1dbf4901aa7bc2dc5ca4ec7206dbf1de5b2906d7d1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: kometa
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-kometa"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: elfbot-kometa
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: kometa
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs/
      subPath: kometa
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-kometa
          optional: true
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: kometa-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: kometa
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [ ! -f /config/config.yml ];
        then
          cp /bootstrap/config.yml /config/
        fi

        # Create directories we need by default
        mkdir -p /config/kometa/assets
        mkdir -p /config/kometa/logs
        mkdir -p /config/kometa/metadata
        mkdir -p /config/kometa/missing
        mkdir -p /config/kometa/overlays

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: kometa
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 4Gi

imagemaid:
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/imagemaid
    tag: v1.1.1@sha256:574796393e4366e19b602f530d61d22835b15d3367253c9225587a692e3e060b
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: imagemaid
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-imagemaid"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: imagemaid-env
  - configMapRef:
      name: elfbot-imagemaid
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-imagemaid
          optional: true
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: kometa-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: imagemaid
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 4Gi  

cinesync:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/cinesync
    tag: v2.4.1@sha256:2357ef1feb815962b21cd993931757d483ea12b8a274155b5dfb90a59772db58
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: cinesync
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-cinesync,cinesync-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: cinesync-env
  - secretRef:
      name: cinesync-env      
  - configMapRef:
      name: elfbot-cinesync
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/db
      subPath: cinesync
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-cinesync
          optional: true  
    logs:
      enabled: true
      type: custom
      mountPath: /app/logs
      subPath: cinesync
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs              
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: cinesync
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x

        # Create directory structure if it doesn't exist yet
        mkdir -p /storage/symlinks/movies
        mkdir -p /storage/symlinks/movies-4k
        mkdir -p /storage/symlinks/movies-anime
        mkdir -p /storage/symlinks/series
        mkdir -p /storage/symlinks/series-4k
        mkdir -p /storage/symlinks/series-anime
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      resources: *default_resources
      securityContext: *default_securitycontext        
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 2Gi 



arrtools:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/arrtools
    tag: rolling@sha256:91a4b1133de4212f86fe3f3b2a5deaf2f622cf4216d70aeb735aa1c562133c46
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  podLabels:
    app.elfhosted.com/name: arrtools
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-arrtools"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: arrtools
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: arrtools-config          
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 2Gi 
  initContainers:
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [[ ! -f /config/confighd.ini ]];
        then
          cp /bootstrap/confighd.ini /config/
        fi
        if [[ ! -f /config/config4k.ini ]];
        then
          cp /bootstrap/config4k.ini /config/
        fi        
        cp /bootstrap/config-placeholder.ini /config/
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: arrtools
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true      

seerrbridge:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/seerrbridge
    tag: v0.7.2@sha256:4d13d92bb342d4fbdf68177e76aa14dca4841863465dd7875a82ba5c096fd44a
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # needs for temporary files I think
  podLabels:
    app.elfhosted.com/name: seerrbridge
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-seerrbridge,seerrbridge-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
    backup: *backup
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory      
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-seerrbridge
          optional: true                
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: seerrbridge
      volumeSpec:
        persistentVolumeClaim:
          claimName: config  
    logs:
      enabled: true
      type: custom
      mountPath: /app/logs
      subPath: seerrbridge
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs     
    npm: # for bridgeboard
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
    chromedriver: # needs to download chromedriver on start
      enabled: true
      mountPath: /app/seerr/chromedriver
      type: emptyDir
      sizeLimit: 1Gi           
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: seerrbridge-env
  - secretRef:
      name: seerrbridge-env      
  - configMapRef:
      name: elfbot-seerrbridge
      optional: true
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 2Gi  
  additionalContainers:
    bridgeboard:
      image: ghcr.io/elfhosted/seerrbridge-bridgeboard:v0.7.2@sha256:65956292cddb8ea3e57142c570cfb5de4bbb47fb62c8ba4adba7b4e293f16f59
      env:
        - name: SEERRBRIDGE_URL
          value: "http://localhost:8777"
        - name: SEERRBRIDGE_LOG_PATH
          value: /app/logs/seerbridge.log
      volumeMounts:
      - mountPath: /app/data
        name: config
        subPath: seerrbridge
      - mountPath: /app/logs
        name: logs
        subPath: seerrbridge
      - mountPath: /.npm
        name: npm
  initContainers:
    setup-warp: *setup_warp
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: seerrbridge
      - mountPath: /tmp
        name: tmp          
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "3001,8777,3777"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared               

listsync:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/listsync
    tag: 0.5.9@sha256:13a18a1f76947a4f2ca3d065d188ba570d0f633733abe1ea2cbce09664109f86
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # needs to create its own .venv
  podLabels:
    app.elfhosted.com/name: listsync
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-listsync,listsync-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    backup: *backup
    tmp: *tmp     
    config:
      enabled: true
      type: custom
      mountPath: /usr/src/app/data
      subPath: listsync
      volumeSpec:
        persistentVolumeClaim:
          claimName: config  
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-listsync
          optional: true    
    logs:
      enabled: true
      type: custom
      mountPath: /logs
      subPath: listsync
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs                         
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: listsync-env
  - configMapRef:
      name: elfbot-listsync
      optional: true
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 2Gi  
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: listsync
      - mountPath: /tmp
        name: tmp  

plextraktsync:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/plextraktsync
    tag: 0.34.11@sha256:c40ebc681d35655fb21bc477d6b8281875d0322afa9271446d9ca9e63e7fdcf7
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: plextraktsync
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plextraktsync"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: elfbot-plextraktsync
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /home/elfie/.config/PlexTraktSync
      subPath: plextraktsync
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plextraktsync
          optional: true
    state: # plextraktsync needs this
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mountPath: /home/elfie/.local/state
    cache: # plextraktsync needs this
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mountPath: /home/elfie/.cache      
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plextraktsync
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 100m # no way this should be using so much resources
      memory: 1Gi

plexytrack:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/plexytrack
    tag: v0.3.2@sha256:ee05371a8f374efba9627adfcd80a23be98ad4feb08d3bedaa9d84b5139458d8
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plexytrack"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5030
  envFrom:
  - configMapRef:
      name: plexytrack-env 
  - configMapRef:
      name: elfbot-pleyxtrack
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plexytrack
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plexytrack
          optional: true    
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plexytrack
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 100m # no way this should be using so much resources
      memory: 1Gi

letterboxdtraktsync:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/letterboxd-trakt-sync
    tag: v1.0.1@sha256:368112e9ef5eda2934b67f676368da4e478265af1b37770343670a3d0c341e4c
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-letterboxd-trakt-sync"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  env:
    RUN_ON_START: "true"
  envFrom:
  - configMapRef:
      name: elfbot-letterboxd-trakt-sync
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: letterboxd-trakt-sync
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-letterboxd-trakt-sync
          optional: true   
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: letterboxd-trakt-sync
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 100m # no way this should be using so much resources
      memory: 1Gi

decluttarr: &app_decluttarr
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/decluttarr
    tag: rolling@sha256:98b9098333e02d89874c53a303e2c353982fab2df9315464950fc81401dd2f96
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: decluttarr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-decluttarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: elfbot-decluttarr
      optional: true
  - configMapRef:
      name: zenv-decluttarr
      optional: true      
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi

rdebridui:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rdebrid-ui
    tag: rolling@sha256:2c9885f1918097d8b762e0e60fcee5e12f147c10d79f13decbe4f3b73d24e4ad
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: rdebrid-ui
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rdebrid-ui"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  envFrom:
  - configMapRef:
      name: elfbot-rdebrid-ui
      optional: true  
  - configMapRef:
      name: zenv-rdebrid-ui
  - secretRef:
      name: zenv-rdebrid-ui      
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi

suggestarr:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/suggestarr
    tag: v1.0.20@sha256:f567f7416f2e2092d54e68037e0886908ce583126281c9673c92bde0969bf1c5
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with app :(
  podLabels:
    app.elfhosted.com/name: suggestarr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-suggestarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  env:
    TZ: UTC
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /app/config/
      subPath: suggestarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /var/log
      subPath: suggestarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-suggestarr
          optional: true          
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5000
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: suggestarr
      - mountPath: /tmp
        name: tmp      

decluttarr4k: 
  <<: *app_decluttarr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-decluttarr4k"  
  envFrom:
  - configMapRef:
      name: elfbot-decluttarr4k
      optional: true
  - configMapRef:
      name: zenv-decluttarr4k
      optional: true      


rcloneui:
  enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.70.1@sha256:67cbf11bdec4f38821ffc66fd526b7f040d023b7c68f200f1ce070fee99b8eb6
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rcloneui,elfhosted-user-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    privileged: true
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    backup: *backup
    cache: 
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
    mount:
      enabled: true
      type: emptyDir
      mountPath: /mount
      sizeLimit: 1Gi 
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: rclone
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    rclone-remote-storage:
      enabled: "true"
      subPath: "rclone-remote-storage"
      type: "custom"
      volumeSpec:
        configMap:
          name: rclonefm-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rclonebrowser
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rclone
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # add local remote if it doesn't exist
        grep -q '/storage' /config/rclone.conf || cat /rclone-remote-storage >> /config/rclone.conf

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: rclone
      - mountPath: /tmp # need this for cating into a file
        name: tmp
      - mountPath: /rclone-remote-storage
        subPath: rclone-remote-storage
        name: rclone-remote-storage
      resources: *default_resources
      securityContext: *default_securitycontext
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5572
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 1
      memory: 1Gi

rclonefm:
  enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.70.1@sha256:67cbf11bdec4f38821ffc66fd526b7f040d023b7c68f200f1ce070fee99b8eb6
  command:
  - /rclonefm.sh
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rclonefm,rclonefm-config"
  podAnnotations:
    kubernetes.io/egress-bandwidth: "40M"
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    cache:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      sizeLimit: 1Gi
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: rclone
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    rclonefm-config:
      enabled: "true"
      mountPath: /var/lib/rclonefm/js/settings.js
      subPath: "settings.js"
      type: "custom"
      volumeSpec:
        configMap:
          name: rclonefm-config
    rclone-remote-storage:
      enabled: "true"
      subPath: "rclone-remote-storage"
      type: "custom"
      volumeSpec:
        configMap:
          name: rclonefm-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rclonefm
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rclone
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # add local remote if it doesn't exist
        grep -q '/storage' /config/rclone.conf || cat /rclone-remote-storage >> /config/rclone.conf

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: rclone
      - mountPath: /tmp # need this for cating into a file
        name: tmp
      - mountPath: /rclone-remote-storage
        subPath: rclone-remote-storage
        name: rclone-remote-storage
      resources: *default_resources
      securityContext: *default_securitycontext
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5573
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi

webdav: &webdav
  enabled:
    false
  podLabels:
    app.elfhosted.com/name: webdav
  podAnnotations:
    kubernetes.io/egress-bandwidth: "40M"
  priorityClassName: tenant-normal
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.70.1@sha256:67cbf11bdec4f38821ffc66fd526b7f040d023b7c68f200f1ce070fee99b8eb6
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-webdav-plus,elfbot-webdav"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  command:
  - /webdav.sh
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: webdav-config
  - configMapRef:
      name: elfbot-webdav
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /storage/config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-webdav-plus
          optional: true
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  service:
    main:
      enabled: false # necessary for probes
      ports:
        http:
          port: 5574
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi

webdavplus:
  enabled: false
  <<: *webdav
  podLabels:
    app.elfhosted.com/name: webdav-plus
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M"
  envFrom:
  - configMapRef:
      name: webdav-plus-config

jfa:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/jfa-go
    tag: v0.5.1@sha256:ad4d6052d9fb27c0a7d972ecd4916aa0e51eb303a515e93a9a5e8a3ee80b1dd8
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jfa"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: jfa
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jfa
          optional: true
    tmp:
      enabled: true
      type: emptyDir
      mountPath: /tmp
      sizeLimit: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jfa
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8056
  resources:
    requests:
      cpu: 0m
      memory: 150Mi
    limits:
      cpu: 2
      memory: 1Gi

mattermost:
  enabled: false
  # Default values for mattermost-team-edition.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.
  image:
    repository: mattermost/mattermost-team-edition
    tag: 10.9.2@sha256:673342e3ca774d74c4fe1fbaf41212c49707c363e5e42ad7018935ec02d484bb
    imagePullPolicy: IfNotPresent
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mattermost"

  initContainerImage:
    repository: appropriate/curl
    tag: latest
    imagePullPolicy: IfNotPresent

  extraInitContainers: []

  ## Deployment Strategy
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  deploymentStrategy:
    type: Recreate
    rollingUpdate: null

  ## How many old ReplicaSets for Mattermost Deployment you want to retain
  revisionHistoryLimit: 1

  ## Enable persistence using Persistent Volume Claims
  ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
  ## ref: https://docs.gitlab.com/ee/install/requirements.html#storage
  ##
  persistence:
    ## This volume persists generated data from users, like images, attachments...
    ##
    data:
      enabled: true
      size: 10Gi
      ## If defined, volume.beta.kubernetes.io/storage-class: <storageClass>
      ## Default: volume.alpha.kubernetes.io/storage-class: default
      ##
      # storageClass:
      accessMode: ReadWriteOnce
      existingClaim: "config"
      subPath: mattermost/data
    plugins:
      enabled: false # these just end up under data anyway

  service:
    type: ClusterIP
    externalPort: 8065
    internalPort: 8065
    annotations: {}
    # loadBalancerIP:
    loadBalancerSourceRanges: []

  ingress:
    enabled: false
    path: /
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # certmanager.k8s.io/issuer:  your-issuer
      # nginx.ingress.kubernetes.io/proxy-body-size: 50m
      # nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
      # nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
      # nginx.ingress.kubernetes.io/proxy-buffering: "on"
      # nginx.ingress.kubernetes.io/configuration-snippet: |
      #   proxy_cache mattermost_cache;
      #   proxy_cache_revalidate on;
      #   proxy_cache_min_uses 2;
      #   proxy_cache_use_stale timeout;
      #   proxy_cache_lock on;
      #### To use the nginx cache you will need to set an http-snippet in the ingress-nginx configmap
      #### http-snippet: |
      ####     proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=mattermost_cache:10m max_size=3g inactive=120m use_temp_path=off;
    hosts:
      - mattermost.example.com
    tls:
      # - secretName: mattermost.example.com-tls
      #   hosts:
      #     - mattermost.example.com

  route:
    enabled: false

  ## If use this please disable the mysql chart by setting mysql.enable to false
  externalDB:
    enabled: true

    ## postgres or mysql
    externalDriverType: "mysql"

    ## postgres:  "<USERNAME>:<PASSWORD>@<HOST>:5432/<DATABASE_NAME>?sslmode=disable&connect_timeout=10"
    ## mysql:     "<USERNAME>:<PASSWORD>@tcp(<HOST>:3306)/<DATABASE_NAME>?charset=utf8mb4,utf8&readTimeout=30s&writeTimeout=30s"
    externalConnectionString: "mattermost:IUzI1NiJ9.eyJhdWQiOiIwMDk1MTkyYjhjZWIyYjVhNDQwMT@tcp(mattermost-mysql:3306)/mattermost?charset=utf8mb4,utf8&readTimeout=30s&writeTimeout=30s"

  mysql:
    nameOverride: mattermost-mysql
    enabled: true
    architecture: standalone
    # nameOverride: mattermost-mariadb

    commonAnnotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mattermost"

    auth:
      rootPassword: "3uAaJYGJLR3d2qbBM2FsSThJ"
      database: "mattermost"
      username: "mattermost"
      password: "IUzI1NiJ9.eyJhdWQiOiIwMDk1MTkyYjhjZWIyYjVhNDQwMT"

    primary:
      readinessProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      livenessProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      startupProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      persistence:
        enabled: true
        existingClaim: config
        subPath: mattermost/database
      resources:
        requests:
          cpu: 5m
          memory: 512Mi
        limits:
          cpu: 2
          memory: 1024Mi
      containerSecurityContext:
        enabled: true
        seccompProfile:
          type: RuntimeDefault
        runAsUser: 568
        runAsGroup: 568
      podSecurityContext:
        enabled: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
      extraVolumeMounts:
      - mountPath: /opt/bitnami/mysql/tmp/
        name: tmp
      extraVolumes:
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi
      - name: backup-database-script
        configMap:
          name: mattermost-backup
      - name: confighdd
        persistentVolumeClaim:
          claimName: config
          subPath: mattermost/database
      sidecars:
        - name: backup-database
          image: *tooling_image
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: 3uAaJYGJLR3d2qbBM2FsSThJ
            - name: MYSQL_DATABASE
              value: mattermost
          command:
          - /usr/bin/dumb-init
          - /bin/bash
          - -c
          - |

            sleep 2m # give mysql time to start up
            while true
            do
              now=$(date +"%s_%Y-%m-%d")
              /usr/bin/mysqldump --opt -h mattermost-mysql -u root -p${MYSQL_ROOT_PASSWORD} ${MYSQL_DATABASE} > "/backup/${now}_${MYSQL_DATABASE}.sql"
              sleep 1d
            done

  ## Additional pod annotations
  extraPodAnnotations: {}

  ## Additional env vars
  extraEnvVars: []
    # This is an example of extra env vars when using with the deployment with GitLab Helm Charts
    # - name: POSTGRES_PASSWORD_GITLAB
    #   valueFrom:
    #     secretKeyRef:
    #       # NOTE: Needs to be manually created
    #       # kubectl create secret generic gitlab-postgresql-password --namespace <NAMESPACE> --from-literal postgres-password=<PASSWORD>
    #       name: gitlab-postgresql-password
    #       key: postgres-password
    # - name: POSTGRES_USER_GITLAB
    #   value: gitlab
    # - name: POSTGRES_HOST_GITLAB
    #   value: gitlab-postgresql
    # - name: POSTGRES_PORT_GITLAB
    #   value: "5432"
    # - name: POSTGRES_DB_NAME_MATTERMOST
    #   value: mm5
    # - name: MM_SQLSETTINGS_DRIVERNAME
    #   value: "postgres"
    # - name: MM_SQLSETTINGS_DATASOURCE
    #   value: postgres://$(POSTGRES_USER_GITLAB):$(POSTGRES_PASSWORD_GITLAB)@$(POSTGRES_HOST_GITLAB):$(POSTGRES_PORT_GITLAB)/$(POSTGRES_DB_NAME_MATTERMOST)?sslmode=disable&connect_timeout=10

  ## Additional init containers
  # extraInitContainers: []
    # This is an example of extra Init Container when using with the deployment with GitLab Helm Charts
    # - name: bootstrap-database
    #   image: "postgres:9.6-alpine"
    #   imagePullPolicy: IfNotPresent
    #   env:
    #     - name: POSTGRES_PASSWORD_GITLAB
    #       valueFrom:
    #         secretKeyRef:
    #           name: gitlab-postgresql-password
    #           key: postgres-password
    #     - name: POSTGRES_USER_GITLAB
    #       value: gitlab
    #     - name: POSTGRES_HOST_GITLAB
    #       value: gitlab-postgresql
    #     - name: POSTGRES_PORT_GITLAB
    #       value: "5432"
    #     - name: POSTGRES_DB_NAME_MATTERMOST
    #       value: mm5
    #   command:
    #     - sh
    #     - "-c"
    #     - |
    #       if PGPASSWORD=$POSTGRES_PASSWORD_GITLAB psql -h $POSTGRES_HOST_GITLAB -p $POSTGRES_PORT_GITLAB -U $POSTGRES_USER_GITLAB -lqt | cut -d \| -f 1 | grep -qw $POSTGRES_DB_NAME_MATTERMOST; then
    #       echo "database already exist, exiting initContainer"
    #       exit 0
    #       else
    #       echo "Database does not exist. creating...."
    #       PGPASSWORD=$POSTGRES_PASSWORD_GITLAB createdb -h $POSTGRES_HOST_GITLAB -p $POSTGRES_PORT_GITLAB -U $POSTGRES_USER_GITLAB $POSTGRES_DB_NAME_MATTERMOST
    #       echo "Done"
    #       fi

  # Add additional volumes and mounts, for example to add SAML keys in the app or other files the app server may need to access
  extraVolumes: []
    # - hostPath:
    #     path: /var/log
    #   name: varlog
  extraVolumeMounts:
    - name: mattermost-data
      mountPath: mattermost/mattermost/logs
      subPath: mattermost/logs
      readOnly: true

  ## Node selector
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
  nodeSelector: {}

  ## Affinity
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}

  ## Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  ## Pod Security Context
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  securityContext:
    fsGroup: 568
    runAsGroup: 568
    runAsUser: 568
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL

  serviceAccount:
    create: false
    name:
    annotations: {}

  ## Configuration
  ## The config here will be injected as environment variables in the deployment
  ## Please refer to https://docs.mattermost.com/administration/config-settings.html#configuration-in-database for more information
  ## You can add any config here, but need to respect the format: MM_<GROUPSECTION>_<SETTING>. ie: MM_SERVICESETTINGS_ENABLECOMMANDS: false
  config:
    MM_PLUGINSETTINGS_CLIENTDIRECTORY: "./client/plugins"

syncthing:
  enabled: false
  hostname: elfhosted
  priorityClassName: tenant-bulk
  image:
    repository: ghcr.io/elfhosted/syncthing
    tag: 1.29.6@sha256:d7d222a79ebec9b60eb7ae01a0ac3099db5ae41bac00b79abe13ac803c44f15e
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-syncthing"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: syncthing
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-syncthing
          optional: true

  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8384
  resources:
    requests:
      cpu: 0m
      memory: 70Mi
    limits:
      cpu: 1
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: syncthing
      - mountPath: /tmp
        name: tmp
    setup:
      image: ghcr.io/elfhosted/syncthing:1.29.6@sha256:d7d222a79ebec9b60eb7ae01a0ac3099db5ae41bac00b79abe13ac803c44f15e
      imagePullPolicy: IfNotPresent
      envFrom:
      - configMapRef:
          name: elfhosted-user-config
      command:
      - /bin/ash
      - -c
      - |
        set -x
        set -e

        # Generate a new config if necessary
        if [ ! -f /config/config.xml ]
        then
          # We are generating a new config
          syncthing generate --config=/config
        fi

        # Apply the port every time (incase the user changes it and reboots)
        # sed -i  "s/<listenAddress>tcp.*/<listenAddress>tcp:\/\/0.0.0.0:${PORT_SYNCTHING}<\/listenAddress>/" /config/config.xml
        # sed -i  "s/<listenAddress>quic.*/<listenAddress>quic:\/\/0.0.0.0:${PORT_SYNCTHING}<\/listenAddress>/" /config/config.xml

        # # And if it's defaulted...
        # sed -i  "s/<<listenAddress>default<\/listenAddress>/<listenAddress>tcp:\/\/0.0.0.0:${PORT_SYNCTHING}<\/listenAddress>\n\t<listenAddress>quic:\/\/0.0.0.0:${PORT_SYNCTHING}<\/listenAddress>/" /config/config.xml

        # Ignore the fact that we have no password set
        # grep '<insecureAdminAccess>true</insecureAdminAccess>' /config/config.xml || sed -i  "s/<\/gui>/<insecureAdminAccess>true<\/insecureAdminAccess>\n\t<\/gui>/" /config/config.xml

        # Avoid foolish use of capital letters in default sync folder
        # sed -i  "s/\/storage\/elfstorage\/Sync/\/storage\/elfstorage\/syncthing/" /config/config.xml

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: syncthing
      resources: *default_resources
      securityContext: *default_securitycontext

rdtclient: &app_rdtclient
  enabled: false
  hostname: elfhosted
  priorityClassName: tenant-bulk
  podLabels:
    app.elfhosted.com/class: debrid
  image:
    repository: ghcr.io/elfhosted/rdtclient
    tag: v2.0.113@sha256:e274c86872f823296d063cc04eaec10410bf5ea4590ccb1660ebe51339803464
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rdtclient"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data/db
      subPath: rdtclient
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rdtclient
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6500
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
      ephemeral-storage: 50Mi
    limits:
      cpu: 100m
      memory: 2Gi
      ephemeral-storage: 100Mi # a safety net against node ephemeral space exhaustion
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rdtclient
      - mountPath: /tmp
        name: tmp

miniflux:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/miniflux
    tag: 2.2.10
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-miniflux,miniflux-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1500m # if par threads is 1, this leaves 0.5cpu for downloading
      memory: 1Gi
  envFrom:
  - configMapRef:
      name: miniflux-config
  postgresql:
    enabled: true
    nameOverride: miniflux-postgresql
    commonAnnotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-miniflux"
    auth:
      username: miniflux
      password: miniflux
      database: miniflux
      postgresPassword: miniflux
    primary:
      affinity: *standard_affinity
      tolerations: *standard_tolerations
      persistence:
        enabled: true
        existingClaim: config
        subPath: miniflux/database
      resources:
        requests:
          cpu: 5m
          memory: 128Mi
        limits:
          cpu: 2
          memory: 1024Mi
      containerSecurityContext:
        enabled: true
        seccompProfile:
          type: RuntimeDefault
        runAsUser: 568
        runAsGroup: 568
      podSecurityContext:
        enabled: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
      extraVolumeMounts:
      - mountPath: /opt/bitnami/postgresql/conf/
        name: conf
      - mountPath: /opt/bitnami/postgresql/tmp/
        name: tmp
      extraVolumes:
      - name: conf
        emptyDir:
          sizeLimit: 1Gi
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi
      sidecars:
        - name: backup-database
          image: *tooling_image
          env:
            - name: POSTGRES_PASSWORD
              value: miniflux
            - name: POSTGRES_DATABASE
              value: miniflux
            - name: POSTGRES_USER
              value: miniflux
          command:
          - /usr/bin/dumb-init
          - /bin/bash
          - -c
          - |

            set +e # for debug
            sleep 2m # give postgres time to start up
            while true
            do
              now=$(date +"%s_%Y-%m-%d")
              PGPASSWORD=$POSTGRES_PASSWORD pg_dump -U $POSTGRES_USER -h localhost -d $POSTGRES_DATABASE -F c -f /backup/${now}_${POSTGRES_DATABASE}.psql
              sleep 1d
            done

joplinserver:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/geek-cookbook/joplin-server
    tag: v2.14.2@sha256:b4f52bffce08541dd54e823b78bbfa18e091d53064ed507f69fe9e1ca92b719b
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-joplinserver,joplinserver-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # breaks migrations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1500m # if par threads is 1, this leaves 0.5cpu for downloading
      memory: 1Gi
  envFrom:
  - configMapRef:
      name: joplinserver-config
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: joplinserver/data
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  postgresql:
    enabled: true
    nameOverride: joplinserver-postgresql
    commonAnnotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-joplinserver"
    auth:
      username: joplinserver
      password: joplinserver
      database: joplinserver
      postgresPassword: joplinserver
    primary:
      affinity: *standard_affinity
      tolerations: *standard_tolerations
      persistence:
        enabled: true
        existingClaim: config
        subPath: joplinserver/database
      resources:
        requests:
          cpu: 5m
          memory: 128Mi
        limits:
          cpu: 2
          memory: 1024Mi
      containerSecurityContext:
        enabled: true
        seccompProfile:
          type: RuntimeDefault
        runAsUser: 568
        runAsGroup: 568
      podSecurityContext:
        enabled: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
      extraVolumeMounts:
      - mountPath: /opt/bitnami/postgresql/conf/
        name: conf
      - mountPath: /opt/bitnami/postgresql/tmp/
        name: tmp
      extraVolumes:
      - name: conf
        emptyDir:
          sizeLimit: 1Gi
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi
      sidecars:
        - name: backup-database
          image: *tooling_image
          env:
            - name: POSTGRES_PASSWORD
              value: joplinserver
            - name: POSTGRES_DATABASE
              value: joplinserver
            - name: POSTGRES_USER
              value: joplin
          command:
          - /usr/bin/dumb-init
          - /bin/bash
          - -c
          - |

            set +e # for debug
            sleep 2m # give postgres time to start up
            while true
            do
              now=$(date +"%s_%Y-%m-%d")
              PGPASSWORD=$POSTGRES_PASSWORD pg_dump -U $POSTGRES_USER -h localhost -d $POSTGRES_DATABASE -F c -f /backup/${now}_${POSTGRES_DATABASE}.psql
              sleep 1d
            done

homepage:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/gethomepage/homepage
    tag: v1.3.2
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-homepage,homepage-config,homepage-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
    privileged: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 568
    # runAsGroup: 568
    fsGroup: 568 # need this so that the bootstrap can run
    fsGroupChangePolicy: "OnRootMismatch"
  serviceAccount:
    create: true
    name: homepage
  automountServiceAccountToken: true
  env:
    PUID: 568
    PGID: 568
  envFrom:
  - configMapRef:
      name: homepage-env
  - configMapRef:
      name: elfbot-homepage
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/config
      subPath: homepage
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    config-default:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: homepage-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-homepage
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 250m # deliberately hobble the CPU in favor of GPU transcoding
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: homepage
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /app/config/user-change-these/
        touch /app/config/user-change-these/JELLYFIN_KEY
        touch /app/config/user-change-these/PLEX_KEY
        touch /app/config/user-change-these/EMBY_KEY
        touch /app/config/user-change-these/NAVIDROME_USER
        touch /app/config/user-change-these/NAVIDROME_TOKEN
        touch /app/config/user-change-these/NAVIDROME_SALT
        touch /app/config/user-change-these/CALIBREWEB_USERNAME
        touch /app/config/user-change-these/CALIBREWEB_PASSWORD
        touch /app/config/user-change-these/KOMGA_USERNAME
        touch /app/config/user-change-these/KOMGA_PASSWORD
        touch /app/config/user-change-these/KAVITA_USERNAME
        touch /app/config/user-change-these/KAVITA_PASSWORD
        touch /app/config/user-change-these/AUDIOBOOKSHELF_KEY
        touch /app/config/user-change-these/OMBI_KEY
        touch /app/config/user-change-these/OVERSEERR_KEY
        touch /app/config/user-change-these/JELLYSEERR_KEY
        touch /app/config/user-change-these/TAUTULLI_KEY
        touch /app/config/user-change-these/tunarr_USERNAME
        touch /app/config/user-change-these/tunarr_PASSWORD
        touch /app/config/user-change-these/MINIFLUX_KEY
        touch /app/config/user-change-these/UPTIMEKUMA_SLUG
        touch /app/config/user-change-these/GOTIFY_KEY

        # If we don't already have an example config, create one
        if [ ! -f /app/config/dont-overwrite-me ];
        then
          cp /bootstrap/* /app/config/
        fi
      volumeMounts:
      - mountPath: /app/config
        name: config
        subPath: homepage
      - name: config-default
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true


wallabag:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: stefanprodan/podinfo
    tag: latest
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-wallabag,wallabag-config"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
    privileged: false
  # runtimeClassName: kata
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568 # for the mounted volumes
  persistence:
    config:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 0m
      memory: 32Mi
    limits:
      cpu: 100m
      memory: 1Gi
  additionalContainers:
    ui:
      image: ghcr.io/elfhosted/wallabag:2.6.12@sha256:3398b2af57d0e51727ebaf2d674c71566718040c685e01a26db532bd4848ebcb
      volumeMounts:
      - mountPath: /var/www/wallabag/data
        name: config
        subPath: wallabag/data
      - mountPath: /var/www/wallabag/images
        name: config
        subPath: wallabag/images
      envFrom:
      - configMapRef:
          name: elfbot-wallbag
          optional: true
      - configMapRef:
          name: wallabag-config
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        allowPrivilegeEscalation: false
      resources:
        requests:
          cpu: 0m
          memory: 100Mi
        limits:
          cpu: 500m
          memory: 200Mi

gotosocial:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/gotosocial
    tag: 0.19.0@sha256:538111e5b96b5dd7bf6142bcaf1de1aa86180d5d3d09e20d76425cc38036cb9e
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-gotosocial"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work with s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 1
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  env:
    S6_READ_ONLY_ROOT: "true"
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: gotosocial
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: gotosocial-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-gotosocial
          optional: true
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: gotosocial
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [[ ! -f /config/config.yaml ]];
        then
          cp /bootstrap/config.yaml /config/
        fi

        # Setup the fish env
        mkdir -p /config/.config/fish
        cp /bootstrap/config.fish /config/.config/fish
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: gotosocial
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /livez
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /readyz
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 60 # allow 10 min of failures to start up
        httpGet:
          path: /readyz
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10  

blueskypds:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/bluesky-pds
    tag: 0.4.154@sha256:a42a32014499d44e9a4050e95cf7bfd80ab5fc6063ed69a07434bdd8dffd33e5
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-bluesky-pds"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work with s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 1
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  env:
    S6_READ_ONLY_ROOT: "true"
  envFrom:
  - configMapRef:
      name: bluesky-pds-env
  - configMapRef:
      name: elfbot-bluesky-pds
      optional: true    
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: bluesky-pds
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-bluesky-pds
          optional: true
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: bluesky-pds
      - mountPath: /tmp
        name: tmp
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /xrpc/_health
          port: 3000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /xrpc/_health
          port: 3000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 60 # allow 10 min of failures to start up
        httpGet:
          path: /xrpc/_health
          port: 3000
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10  

riven: &app_riven
  enabled: false
  podLabels:
    app.elfhosted.com/name: riven
  image:
    repository: ghcr.io/elfhosted/riven
    tag: v0.21.21@sha256:bf1a9cab579d5eaac0ed72e20584b139547db7c38753a03b3756151af5e0ac13
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-riven,riven-env,riven-setup"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with ilikedanger currently
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: riven-env
  - configMapRef:
      name: elfbot-riven
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 20Mi
    limits:
      cpu: 2
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /riven/data
      subPath: riven
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /riven/data/logs
      subPath: riven
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-riven
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory
    setup:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: riven-setup        
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: riven
      - mountPath: /tmp
        name: tmp
    setup-postgres:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/postgresql/database
        mkdir -p /config/postgresql/backups
        chown elfie:elfie /config -R

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: riven
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        # run the setup script from the configmap, so that we can make templated changes
        bash /setup/setup.sh
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      - mountPath: /config
        name: config
        subPath: riven
      - name: setup
        mountPath: "/setup/"              
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: postgres
        - name: POSTGRES_DB
          value: riven
        - name: POSTGRES_USER
          value: postgres
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: riven/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi   

rivenvpn: 
  enabled: false
  <<: *app_riven
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-riven,riven-env,riven-setup,gluetun-config"  
  addons:
    vpn:
      enabled: true
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      envFrom:
      - configMapRef:
          name: gluetun-config
      env:
        DOT: "off"
        FIREWALL_INPUT_PORTS: "3001,8080" # 3001 is ttyd, 8080 is the backend
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"        
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

rivenfrontend: &app_rivenfrontend
  podLabels:
    app.elfhosted.com/name: riven-frontend
  image:
    repository: ghcr.io/elfhosted/riven-frontend
    tag: v0.20.0@sha256:c4d792868a8113a2f57f504fe371cbcd8c52172d734c812c366bff352672998e
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rivenfrontend,riven-frontend-env,riven-frontend-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with ilikedanger currently
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    config:
      enabled: "true"
      type: "custom"
      mountPath: /riven/config
      volumeSpec:
        configMap:
          name: riven-frontend-config          
  envFrom:
  - configMapRef:
      name: riven-frontend-env
  - configMapRef:
      name: elfbot-rivenfrontend
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 20Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000

# So that we can toggle it on with rivenvpn
rivenfrontendvpn:
  enabled: false
  <<: *app_rivenfrontend

airdcpp: &app_airdcpp
  enabled: false
  image:
    repository: ghcr.io/geek-cookbook/airdcpp
    tag: 2.9.0@sha256:d9f6e597bcfc38946d0c4cafce775a559e7b8cf7c66397c9c506cb695ea01205
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: airdcpp
  podAnnotations:
    kubernetes.io/egress-bandwidth: "100M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-airdcpp"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 2
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5600
  env:
    WAIT_FOR_VPN: "true"
    PORT_FILE: /.airdcpp/forwarded-port
  probes:
    liveness:
      enabled: false
    startup:
      enabled: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /.airdcpp/
      subPath: airdcpp
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-airdcpp
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: airdcpp
      - mountPath: /tmp
        name: tmp
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      envFrom:
      - configMapRef:
          name: airdcpp-pia-config
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
        subPath: airdcpp
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus

airdcpppia:
  enabled: false
  <<: *app_airdcpp

airdcppgluetun:
  enabled: false
  <<: *app_airdcpp
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      envFrom:
      - configMapRef:
          name: airdcpp-gluetun-config
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
        subPath: airdcpp
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus

jackett:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/jackett
    tag: 0.22.2064@sha256:6c1151a527c82eb3e9bba4b60f008d56bda8632c8a74041b1aaa363beafd426b
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jackett"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9117
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: jackett
      volumeSpec:
        persistentVolumeClaim:
          claimName: config        
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jackett
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jackett
      - mountPath: /tmp
        name: tmp

wizarr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/wizarr
    tag: 2025.6.5@sha256:5d472f2f00b947f56a5278eda1bb30b39e66e241e5dffcb956b1c2dd91f8b590
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-wizarr"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # now wizarr wants to write data to /data/.venv
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5690
  envFrom:
  - configMapRef:
      name: wizarr-env
  - configMapRef:
      name: elfbot-wizarr
      optional: true            
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /data/database
      subPath: wizarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config        
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-wizarr
          optional: true
    wizarr-steps:
      enabled: true
      type: emptyDir
      mountPath: /data/wizard_steps          
    wizarr-steps-plex:
      enabled: "true"
      type: "custom"
      mountPath: /opt/default_wizard_steps/plex
      volumeSpec:
        configMap:
          name: wizarr-steps-plex
    wizarr-steps-jellyfin:
      enabled: "true"
      type: "custom"
      mountPath: /opt/default_wizard_steps/jellyfin
      volumeSpec:
        configMap:
          name: wizarr-steps-jellyfin
    cache:
      enabled: true
      type: emptyDir
      mountPath: /.cache     
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: wizarr
      - mountPath: /tmp
        name: tmp        

flixio:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/stremio-web
    tag: rolling@sha256:de20030d591000b871519899ccbcead9e958a8e1eb5e98d05b65e033286a7e2a
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: flixio
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-flixio"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with iprom's patching trick
    privileged: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: flixio-env
  - configMapRef:
      name: elfbot-flixio
      optional: true  
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080

flixioapi:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/flixio-api
    tag: rolling@sha256:0e30427d9c311f806610951cfc8d8a71c1ba6b65c3ea1a93b829ed75e37138c9
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: flixio-api
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-flixio-api"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    runAsUser: 568
    runAsGroup: 568
  automountServiceAccountToken: false
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-flixio-api
          optional: true
    config:
      enabled: true
      type: custom
      mountPath: /app/data
      subPath: flixio-api
      volumeSpec:
        persistentVolumeClaim:
          claimName: config   
  envFrom:
  - configMapRef:
      name: flixio-api-env
  - configMapRef:
      name: elfbot-flixio-api
      optional: true           
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080

stremiojackett:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/stremio-jackett
    tag: v4.2.6@sha256:e835d4a2579a474394dce8d19175d3e4a89bbdbd307422b9ce6d3452950ee1b2
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-stremio-jackett"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: stremio-jackett-env
  resources:
    requests:
      cpu: 10m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  persistence:
    pm2:
      enabled: true
      type: emptyDir
      mountPath: /.pm2
      sizeLimit: 1Gi
    npm:
      enabled: true
      type: emptyDir
      mountPath: /.npm
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-stremio-jackett
          optional: true

pairdrop:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/pairdrop
    tag: v1.11.2@sha256:d4e52400813a6af412433a801645cd59e7ad27e3e73540af698e085125eead8c
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: pairdrop
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-pairdrop"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  # envFrom:
  # - configMapRef:
  #     name: pairdrop-env
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-pairdrop
          optional: true

actual:
  enabled: false
  image:
    repository: ghcr.io/actualbudget/actual-server
    tag: 25.6.1-alpine
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: actual
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-actual"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5006
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-actual
          optional: true
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: actual
      volumeSpec:
        persistentVolumeClaim:
          claimName: config          

petio:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/petio
    tag: v0.5.5@sha256:a28b7ffb5b1b04a8ad798112604c410a9a09f8882e9bf98b9f24e8f41571f505
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-petio"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 7777
  persistence:
    backup: *backup
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-petio
          optional: true
    config:
      enabled: true
      type: custom
      mountPath: /app/api/config/
      subPath: petio/config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    tmp: *tmp
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: petio
      - mountPath: /tmp
        name: tmp
  additionalContainers:
    mongodb:
      image: mongodb/mongodb-community-server:8.0.10-ubi8
      volumeMounts:
        - name: config
          subPath: petio/mongodb
          mountPath: /data/db/
        - name: tmp
          mountPath: /tmp
      securityContext: *default_securitycontext

nightscout:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/nightscout
    tag: 15.0.3@sha256:9fb5de043a782232738fc7fc185a0f4d9655de7ae3177fed5b3c13fe51840ee3
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-nightscout,nightscout-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: nightscout-env
  - configMapRef:
      name: elfbot-nightscout
      optional: true  
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 1337
  persistence:
    tmp: *tmp
    config:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  additionalContainers:
    mongodb:
      image: mongodb/mongodb-community-server:8.0.10-ubi8
      volumeMounts:
        - name: config
          subPath: nightscout/mongodb
          mountPath: /data/db/
        - name: tmp
          mountPath: /tmp
      securityContext: *default_securitycontext

pgadmin:
  enabled: false
  image:
    repository: dpage/pgadmin4
    tag: "9.4"
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-pgadmin"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    capabilities:
      add:
      - NET_BIND_SERVICE
      drop:
      - ALL
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: pgadmin-env
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 80
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: pgadmin
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    tmp: *tmp

redisinsight:
  enabled: false
  image:
    repository: redislabs/redisinsight
    tag: v2@sha256:7fef8b7ecf2e8597037f906fc69863345dd846d36577210569396f7917333355
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-redisinsight"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    capabilities:
      add:
      - IPC_LOCK
      drop:
      - ALL
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5540
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: redisinsight
      volumeSpec:
        persistentVolumeClaim:
          claimName: config

mongoexpress:
  enabled: false
  image:
    repository: mongo-express
    tag: 1.0.2-18
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mongoexpress"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8081
  envFrom:
  - configMapRef:
      name: elfbot-mongoexpress
      optional: true

comet:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/comet
    tag: v2.10.0@sha256:e59c6edf6a8a33313a8b785505b5caf3d516f573346a9bfd3d95a966eb8c3086
  priorityClassName: tenant-normal
  podAnnotations:
    kubernetes.io/egress-bandwidth: "64M"
  podLabels:
      app.elfhosted.com/name: comet
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-comet,comet-env"
      secret.reloader.stakater.com/reload: "comet-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568    
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
    config:
      enabled: true
      type: custom
      mountPath: /app/data
      subPath: comet
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    cache:
      enabled: true
      type: emptyDir
      mountPath: /.cache
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-comet
          optional: true
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory          
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 2
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  envFrom:
  - configMapRef:
      name: comet-env
  - secretRef:
      name: comet-env
  - configMapRef:
      name: elfbot-comet
      optional: true
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      env:
        CHANGE_CONTAINER_PORTS: "True"
        HTTP_PORT: "3002"
        HTTPS_PORT: "3003"        
      securityContext: *speedtest_securitycontext

jackettio:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/jackettio
    tag: v1.7.1@sha256:6bff317e8c7eb286cf53a8e1740a9b06092e2d3536f80168c728bfeb04a54289
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jackettio,jackettio-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: jackettio
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory              
  resources:
    requests:
      cpu: 10m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 4000
  envFrom:
  - configMapRef:
      name: jackettio-env
  - configMapRef:
      name: elfbot-jackettio
      optional: true
  initContainers:      
    setup-warp: *setup_warp 
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "4000"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared

stremthru:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/stremthru
    tag: 0.79.3@sha256:6bc9e4e075e574d1311e7ef2e4daddd9cf7128cebd8850399d740184e8b4fe43
  priorityClassName: tenant-normal
  podAnnotations:
    kubernetes.io/egress-bandwidth: "64M"  
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-stremthru,stremthru-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: stremthru
      volumeSpec:
        persistentVolumeClaim:
          claimName: config      
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755           
    # shared:
    #   enabled: true
    #   mountPath: /shared
    #   type: emptyDir
    #   volumeSpec:
    #     medium: Memory                    
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  envFrom:
  - configMapRef:
      name: stremthru-env
  - configMapRef:
      name: elfbot-stremthru
      optional: true
  - secretRef:
      name: stremthru-env      
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      env:
        IPTABLES_BACKEND: nft
        KILLSWITCH: "true"
        LOCAL_NETWORK: 10.0.0.0/8
        LOC: de-frankfurt
        PORT_FORWARDING: "0"
        PORT_PERSIST: "1"
        NFTABLES: "1"
        VPNDNS: "0"
      envFrom:
      - secretRef:
          name: stremthru-vpn
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      env:
        CHANGE_CONTAINER_PORTS: "True"
        HTTP_PORT: "3002"
        HTTPS_PORT: "3003"    
      securityContext: *speedtest_securitycontext  

davio:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/davio
    tag: v1.0.4@sha256:3508249d413b6b55bb2860358bbb92dd8ccd760969fadb0473540ddb74218523
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-davio,davio-env"
      secret.reloader.stakater.com/reload: "davio-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    config:
      enabled: true
      mountPath: /config
      type: emptyDir
      volumeSpec:
        medium: Memory      
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 4000
  envFrom:
  - configMapRef:
      name: davio-env
  - secretRef:
      name: davio-env
  - configMapRef:
      name: elfbot-davio
      optional: true

mediafusion:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/mediafusion
    tag: 4.3.33@sha256:f29a06e3ce4649fa26f8573ae1f7a3da0ba1fecc4604d5186df8a49576a83869
  podLabels:
      app.elfhosted.com/name: mediafusion
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mediafusion,mediafusion-env"
      secret.reloader.stakater.com/reload: "mediafusion-env,mediafusion-vpn"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_anti_affinity 
  tolerations: *standard_tolerations      
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    tinyproxy-conf:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tinyproxy-conf 
  resources:
    requests:
      cpu: 1m
      memory: 150Mi
    limits:
      cpu: 100m
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  # Attempmt to reduce load spike on restarts
  command: ["gunicorn", "api.main:app", "-w", "1", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8000", "--timeout", "120", "--max-requests", "2500", "--max-requests-jitter", "500"]          
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8000
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10           
  envFrom:
  - configMapRef:
      name: mediafusion-env
  - configMapRef:
      name: elfbot-mediafusion
      optional: true
  - secretRef:
      name: mediafusion-env

piaproxy:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: thrnz/docker-wireguard-pia
    tag: latest
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "piaproxy-conf"
      secret.reloader.stakater.com/reload: "piaproxy-vpn"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations    
  env:
    IPTABLES_BACKEND: nft
    KILLSWITCH: "true"
    LOCAL_NETWORK: 10.0.0.0/8
    LOC: de-frankfurt
    PORT_FORWARDING: "0"
    PORT_PERSIST: "1"
    NFTABLES: "1"
    VPNDNS: "0"
  envFrom:
  - secretRef:
      name: piaproxy-vpn
  securityContext:
    privileged: true
    runAsUser: 0
    capabilities:
      add:
        - NET_ADMIN
        - SYS_MODULE
  persistence:
    tinyproxy-conf:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tinyproxy-conf 
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8888
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        exec:
          command:
          - /bin/bash
          - -c
          - curl -x http://localhost:8888 -s https://www.cloudflare.com/cdn-cgi/trace | grep www.cloudflare.com
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
  additionalContainers:
    tinyproxy:
      image: ghcr.io/elfhosted/tinyproxy:v1.4.3@sha256:262bbdc0e468ee97c049203becc52b9ad7bf4c21405d58c82766d1aebb2e27e5
      volumeMounts:
      - mountPath: /etc/tinyproxy/tinyproxy.conf
        name: tinyproxy-conf
        subPath: tinyproxy.conf
      - mountPath: /shared
        name: shared
      # env:
      #   WAIT_FOR_VPN: "true"

vpnproxy:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/gluetun
    tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-vpnproxy"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations    
  securityContext:
    privileged: true
    runAsUser: 0
    capabilities:
      add:
        - NET_ADMIN
        - SYS_MODULE
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory          
  resources:
    requests:
      cpu: 0m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8888
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        exec:
          command:
          - /bin/ash
          - -c
          - curl -x http://localhost:8888 -s https://www.cloudflare.com/cdn-cgi/trace | grep www.cloudflare.com
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10            
  env:
    VPN_TYPE: wireguard
    VPN_SERVICE_PROVIDER: custom
    VPN_ENDPOINT_PORT: "2408"
    HTTPPROXY: "on"
    FIREWALL_INPUT_PORTS: "8888"
    DOT: "off"
  initContainers:      
    setup-warp: *setup_warp

tinyproxy:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/tinyproxy
    tag: v1.4.3@sha256:262bbdc0e468ee97c049203becc52b9ad7bf4c21405d58c82766d1aebb2e27e5
  podLabels:
      app.elfhosted.com/name: tinyproxy
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "tinyproxy-conf"
      secret.reloader.stakater.com/reload: "tinyproxy-vpn"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    tinyproxy-conf:
      enabled: "true"
      type: "custom"
      mountPath: /etc/tinyproxy/tinyproxy.conf
      subPath: tinyproxy.conf  
      volumeSpec:
        configMap:
          name: tinyproxy-conf 
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory          
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8888
  env:
    WAIT_FOR_VPN: "true"    
  initContainers:      
    setup-warp: *setup_warp
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "8888"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared

mediaflowproxy:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/mediaflow-proxy
    tag: 2.1.6@sha256:69dcbcb26a09e88ebccaa93c6e167ef5a76d28019da0ac2243122fcd3582b0d8
  podLabels:
      app.elfhosted.com/name: mediaflow-proxy
  priorityClassName: tenant-normal
  podAnnotations:
    kubernetes.io/egress-bandwidth: "64M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mediaflow-proxy,mediaflow-proxy-env"
      secret.reloader.stakater.com/reload: "mediaflowproxy-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      env:
        CHANGE_CONTAINER_PORTS: "True"
        HTTP_PORT: "3002"
        HTTPS_PORT: "3003"    
      securityContext: *speedtest_securitycontext
  resources:
    requests:
      cpu: 100m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8888
  envFrom:
  - configMapRef:
      name: mediaflow-proxy-env
  - configMapRef:
      name: elfbot-mediaflow-proxy
      optional: true   

youriptv:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/youriptv-nobranding
    tag: rolling@sha256:b5b7cb9858e5d6b2edd636f6167ef1e78198deb0badea677256a50613b43211e
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-youriptv"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PORT: 3649
  persistence:
    tmp: *tmp
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3649

aiostreams:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/aiostreams
    tag: v2.4.1@sha256:1121f96ab05d24a622707c91f9aa6a2ca9095e3242d8c4c84aaaa0479c7fc282
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-aiostreams,aiostreams-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
    backup: *backup    
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: aiostreams
      volumeSpec:
        persistentVolumeClaim:
          claimName: config  
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-aiostreams
          optional: true            
  resources:
    requests:
      cpu: 0m
      memory: 50Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  envFrom:
  - configMapRef:
      name: aiostreams-env   
  - configMapRef:
      name: elfbot-aiostreams
      optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: aiostreams
      - mountPath: /tmp
        name: tmp

aiostreamsv1:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/aiostreams-v1
    tag: rolling
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-aiostreamsv1,aiostreamsv1-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  affinity: *standard_affinity
  tolerations: *standard_tolerations     
  resources:
    requests:
      cpu: 0m
      memory: 50Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  envFrom:
  - configMapRef:
      name: aiostreams-env   
  - configMapRef:
      name: elfbot-aiostreams
      optional: true

autopulse:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/autopulse
    tag: v1.3.2@sha256:2f6e2e0ea0648a33b9a26688853851a2963c8c3b7f1b325e896b46333e2e8fc2
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-autopulse,autopulse-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 568
    # runAsGroup: 568
    # fsGroup: 568
    # fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
    backup: *backup    
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: autopulse
      volumeSpec:
        persistentVolumeClaim:
          claimName: config  
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-autopulse
          optional: true     
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi                 
  resources:
    requests:
      cpu: 0m
      memory: 50Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 2875
  envFrom:
  - configMapRef:
      name: autopulse-env   
  - configMapRef:
      name: elfbot-autopulse
      optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: autopulse
      - mountPath: /tmp
        name: tmp
  additionalContainers:
    ui:
      image: ghcr.io/elfhosted/autopulse-ui:v1.3.2@sha256:53a1f11001f0eab374c65f08c19a917e3f9ac4bd480561aab91efbd83ef99f6f
      envFrom:
      - configMapRef:
          name: autopulse-env

nuviostreams:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/nuviostreams-private
    tag: rolling@sha256:b41dfd2c1e211917dd6cf72daf9df0b65c4d6f9aa1eebee0bf16c2a29bcd30ad
  priorityClassName: tenant-normal
  podAnnotations:
    kubernetes.io/egress-bandwidth: "64M" 
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-nuviostreams,nuviostreams-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  affinity: *standard_affinity
  tolerations: *standard_tolerations  
  persistence:
    tmp: *tmp
    shared:
      enabled: true
      type: emptyDir
      volumeSpec:
        medium: Memory     
  resources:
    requests:
      cpu: 0m
      memory: 50Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 7777
  envFrom:
  - configMapRef:
      name: nuviostreams-env   
  - secretRef:
      name: nuviostreams-env         
  - configMapRef:
      name: elfbot-nuviostreams
      optional: true
  additionalContainers:
    proxy:
      image: ghcr.io/elfhosted/simple-proxy:rolling@sha256:a2ea32012ca4a04fdb185a032ac95fd76133acafb8dc75a52fa18c9bbc6d703e
  initContainers:
    setup-warp: *setup_warp
  addons:
    vpn:
      enabled: true
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "7777,3000"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared

webstreamr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/webstreamr
    tag: v0.32.4@sha256:adf052f4ad25e309c240461cb0e2f31e338928bdc78137d1dd792a27ebafdc6f
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,webstreamr-env,elfbot-webstreamr"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp:
      enabled: true
      type: emptyDir
      mountPath: /tmp
  resources:
    requests:
      cpu: 10m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 51546
  envFrom:
  - configMapRef:
      name: webstreamr-env
  - secretRef:
      name: webstreamr-env      
  - configMapRef:
      name: elfbot-webstreamr
      optional: true
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      env:
        IPTABLES_BACKEND: nft
        KILLSWITCH: "true"
        LOCAL_NETWORK: 10.0.0.0/8
        LOC: de-frankfurt
        PORT_FORWARDING: "0"
        PORT_PERSIST: "1"
        NFTABLES: "1"
        VPNDNS: "0"
      envFrom:
      - secretRef:
          name: webstreamr-vpn
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

recyclarr:
  enabled: false
  image:
    repository: ghcr.io/recyclarr/recyclarr
    tag: latest@sha256:759540877f95453eca8a26c1a93593e783a7a824c324fbd57523deffb67f48e1
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-recyclarr"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9898
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: recyclarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: recyclarr-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-recyclarr
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: recyclarr
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [ ! -f /config/recyclarr.yaml ];
        then
          cp /bootstrap/recyclarr.yaml /config/
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: recyclarr
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
  additionalContainers:
    podinfo:
      image: stefanprodan/podinfo # used to run probes from gatus
    sync:
      image: ghcr.io/recyclarr/recyclarr:latest@sha256:759540877f95453eca8a26c1a93593e783a7a824c324fbd57523deffb67f48e1
      command:
      - /bin/bash
      - -c
      - |
        recyclarr sync
        sleep infinity
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: recyclarr
      envFrom:
      - configMapRef:
          name: recyclarr-env

knightcrawler: &app_knightcrawler
  enabled: false
  image:
    repository: ghcr.io/elfhosted/knightcrawler-addon
    tag: v2.0.28@sha256:0985eb3036a940fc751d78eb224f6791a78d1bd64b2e921983a503b60cd9fcc0
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: knightcrawler
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-knightcrawler,elfbot-torrentio,knightcrawler-env"
      secret.reloader.stakater.com/reload: "knightcrawler-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 100m
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 7000
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-knightcrawler
          optional: true
    npm:
      enabled: true
      type: emptyDir
      mountPath: /.npm
    pm2:
      enabled: true
      mountPath: /.pm2
      type: emptyDir
  envFrom:
  - configMapRef:
      name: knightcrawler-env
  - secretRef:
      name: knightcrawler-env

zurg: &app_zurg
  enabled: false
  # Avoid search expansion DNS queries against coredns for every RD API request 
  dnsConfig:
    options:
      - name: ndots
        value: "2"
  podLabels:
    app.elfhosted.com/name: zurg
  image:
    repository: ghcr.io/elfhosted/zurg-rc
    tag: 2025.06.26.0032-nightly@sha256:31877ca1e4195918cfbedcfb62c5e6e06d499b532e6a2414b3b316d3d072e8d5
  imagePullSecrets:
  - name: ghcr-io-elfhosted    
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-zurg,zurg-env,gluetun-config,elfbot-plex,elfbot-emby,elfbot-jellyfin"
    strategy: Recreate
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 32Mi
    limits:
      cpu: 1
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9999
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: zurg
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: zurg
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs          
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: zurg-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-zurg
          optional: true
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  envFrom:
  - configMapRef:
      name: zurg-env # this is here so we can use env vars to detect whether to enable warp
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: zurg
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # We need a /config/logs folder
        mkdir -p /config/logs

        # If we don't already have an example config, create one
        if [[ ! -f /config/config.yml ]];
        then
          cp /bootstrap/config.yml /config/
        fi

        # If we don't already have an example plex_update, create one
        if [[ ! -f /config/plex_update.sh ]];
        then
          cp /bootstrap/plex_update.sh /config/
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: zurg
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        # run the setup script from the configmap, so that we can make templated changes
        bash /bootstrap/setup.sh
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: zurg
      - name: example-config
        mountPath: "/bootstrap/"
    # restart-with-plex:
    #   image: ghcr.io/elfhosted/plex:1.41.8.9834-071366d65@sha256:695ac4a1b79b82c2907e3c112a6bd8b1208a0b1b2e63ca4e9b7e06270651b13f
    #   command:
    #   - /bin/bash
    #   - -c
    #   - |
    #     echo "This pod only exists to cause a restart when plex is updated"  
    # restart-with-jellyfin:
    #   image: ghcr.io/elfhosted/jellyfin-dev:10.10.7@sha256:7ad104992e7908aa85b27cb4360886858caf406bc54abdc452b5b69dcce99b17
    #   command:
    #   - /bin/bash
    #   - -c
    #   - |
    #     echo "This pod only exists to cause a restart when jellyfin is updated"  
    # restart-with-emby:
    #   image: ghcr.io/elfhosted/emby:4.9.1.1@sha256:e81f21831c751bc9365ab9544d6500c0dc132b3cb5b2dc96e05e837ada470863
    #   command:
    #   - /bin/bash
    #   - -c
    #   - |
    #     echo "This pod only exists to cause a restart when emby is updated"                        
  addons:
    vpn: &zurg_addons_vpn
      enabled: false # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      envFrom:
      - configMapRef:
          name: gluetun-config
          optional: true
      - configMapRef:
          name: zurg-env # this is here so we can use env vars to detect whether to enable warp
      env:
        DOT: "off"
        FIREWALL_INPUT_PORTS: "9999" # 9999 is for zurg
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus
  probes:
    startup:
      spec:
        initialDelaySeconds: 0
        timeoutSeconds: 1
        ## This means it has a maximum of 5*120=720 seconds to start up before it fails
        periodSeconds: 5
        failureThreshold: 120

zurggluetun:
  <<: *app_zurg
  enabled: false
  podLabels:
    app.elfhosted.com/name: zurg
    app.elfhosted.com/class: debrid
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-zurg,zurg-gluetun-config,zurg-env"
  service:
    main:
      nameOverride: zurg
      enabled: true # necessary for probes, but probes aren't working with vpn addon currently
  env:
    WAIT_FOR_VPN: "true"
  addons:
    vpn:
      enabled: true
      <<: *zurg_addons_vpn
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      envFrom:
      - configMapRef:
          name: gluetun-config

zurgranger:
  <<: *app_zurg
  podLabels:
    app.elfhosted.com/name: zurg
    app.elfhosted.com/class: dedicated
  podAnnotations:
    kubernetes.io/egress-bandwidth: "500M"
  enabled: false
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-zurg"
  affinity: *dedicated_affinity # force zurg to go onto the dedicated nodes
  resources: *ranger_zurg_resources

plexdebrid: &app_plexdebrid
  enabled: false
  # podLabels:
  #   app.elfhosted.com/name: plexdebrid
  image:
    repository: ghcr.io/elfhosted/plex-debrid
    tag: rolling@sha256:0c0251d2aef532ba62660b719b7e37e72b4eb262ca18e21d68b9509d305e12a1
  podLabels:
    app.elfhosted.com/name: plex-debrid    
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex-debrid"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because of s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
  resources:
    requests:
      cpu: 2m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 3Gi
  ingress:
    main:
      enabled: false
  envFrom:
  - secretRef:
      name: plex-debrid-env
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plex-debrid
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex-debrid
      - mountPath: /tmp
        name: tmp

clidebrid: &app_clidebrid
  enabled: false
  podLabels:
    app.elfhosted.com/name: cli-debrid
  image:
    repository: ghcr.io/elfhosted/cli_debrid-dev
    tag: v0.6.75@sha256:0530e66e4aaf7669b9e53d66abceb1bab1fb6b6e7e28edfa1fcb17a6c7bf3115 
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-cli-debrid,cli-debrid-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work because of s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
  envFrom:
  - configMapRef:
      name: cli-debrid-env
  - configMapRef:
      name: elfbot-cli-debrid
      optional: true        
  resources:
    requests:
      cpu: 2m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 3Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5000
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /user/
      subPath: cli-debrid
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /user/logs
      subPath: cli-debrid
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-cli-debrid
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: cli-debrid
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x

        # Create directory structure if it doesn't exist yet
        mkdir -p /storage/symlinks/movies
        mkdir -p /storage/symlinks/movies-4k
        mkdir -p /storage/symlinks/movies-anime
        mkdir -p /storage/symlinks/series
        mkdir -p /storage/symlinks/series-4k
        mkdir -p /storage/symlinks/series-anime
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      resources: *default_resources
      securityContext: *default_securitycontext           

puter:
  enabled: false
  podLabels:
    app.elfhosted.com/name: puter
  image:
    repository: ghcr.io/elfhosted/puter
    tag: v2.5.1@sha256:4566493190067df16091134e9fa845ffa21f6e517874eb146cb4bca9c4177ca9
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-puter"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work because of s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
  resources:
    requests:
      cpu: 2m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 3Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 4100
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: puter
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-puter
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: puter
      - mountPath: /tmp
        name: tmp

plexdebriddebridlink: 
  <<: *app_plexdebrid  
  enabled: false
  podLabels:
    app.elfhosted.com/name: plex-debrid-debridlink  
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex-debrid-debridlink"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plex-debrid-debridlink
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid-debridlink
          optional: true          
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex-debrid-debridlink
      - mountPath: /tmp
        name: tmp

codeserver:
  enabled: false
  # runtimeClassName: kata
  image:
    repository: ghcr.io/elfhosted/codeserver
    tag: 4.101.2@sha256:b98aea58266285ac8fcd1e86182b4a9962a34109f343dba4c2251da7c01a6c0f
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-codeserver"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because of s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    # runAsUser: 568
    # runAsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 15m
      memory: 200Mi
    limits:
      cpu: 2
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config # no subpath, codeserver wants to see all
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid
          optional: true
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: codeserver-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: codeserver
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        mkdir -p /config/.config/code-server/
        if [ ! -f /config/.config/code-server/config.yaml ];
        then
          cp /bootstrap/config.yaml /config/.config/code-server/
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: codeserver
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true

doplarr: &app_doplarr
  enabled: false
  image:
    repository: ghcr.io/elfhosted/doplarr
    tag: v3.6.3@sha256:7703328fc7f9f4190606ba9f95e867a64db79bd06c19e923a83ad6f939f89097
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-doplarr,doplarr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: elfbot-doplarr
      optional: true

profilarr:
  enabled: false
  hostAliases:
  - ip: "127.0.0.1"
    hostnames:  
    - "backend" 
  image:
    repository: ghcr.io/elfhosted/profilarr-frontend
    tag: v1.0.1@sha256:1f2165783bc2030da7ca84f8b2aa4d709a24d51f05cd1de33cb3b4a438d399d0
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-profilarr,profilarr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  envFrom:
  - configMapRef:
      name: elfbot-profilarr
      optional: true
  persistence:
    tmp: *tmp
    logs:
      enabled: true
      type: custom
      mountPath: /app/logs
      subPath: profilarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    npm:
      enabled: true
      type: emptyDir
      mountPath: /.npm
      sizeLimit: 1Gi          
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: profilarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-profilarr
          optional: true   
    app:
      enabled: "true"
      type: emptyDir
      mountPath: /app
      volumeSpec:
        medium: Memory           
  additionalContainers:
    backend:
      image: ghcr.io/elfhosted/profilarr-backend:v1.0.1@sha256:b1f776ae78836917d2808bd4ab3a0311be58c2627b109e529e052c678c74e8c3     
      volumeMounts:
      - name: config
        mountPath: /config
        subPath: profilarr
  initContainers:
    setup:
      image: ghcr.io/elfhosted/profilarr-frontend:v1.0.1@sha256:1f2165783bc2030da7ca84f8b2aa4d709a24d51f05cd1de33cb3b4a438d399d0
      command:
      - /bin/bash
      - -c
      - |
        # copy the image's build-cache directory into the emptyDir
        cp /app/* /tmp -rfp
      volumeMounts:
      - mountPath: /tmp
        name: app
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: profilarr
      - mountPath: /tmp
        name: tmp

pulsarr: &app_pulsarr
  enabled: false
  image:
    repository: ghcr.io/elfhosted/pulsarr
    tag: v0.3.19@sha256:f797dded59648ecae1e1e333e7c09a5c6a700953f034e654695b3bb219d27168
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-pulsarr,pulsarr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  persistence:
    backup: *backup
    tmp: *tmp
    logs:
      enabled: true
      type: custom
      mountPath: /app/data/logs
      subPath: pulsarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs    
    config:
      enabled: true
      type: custom
      mountPath: /app/data
      subPath: pulsarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-pulsarr
          optional: true  
    app:
      enabled: "true"
      type: emptyDir
      mountPath: /app
      volumeSpec:
        medium: Memory                
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3003
  envFrom:
  - configMapRef:
      name: pulsarr-env
  - configMapRef:
      name: elfbot-pulsarr
      optional: true      
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: pulsarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: ghcr.io/elfhosted/pulsarr:v0.3.19@sha256:f797dded59648ecae1e1e333e7c09a5c6a700953f034e654695b3bb219d27168
      command:
      - /bin/ash
      - -c
      - |
        # copy the image's build-cache directory into the emptyDir
        cp /app/* /tmp -rfp
      volumeMounts:
      - mountPath: /tmp
        name: app
  additionalContainers:
    apprise-api:
      image: ghcr.io/elfhosted/apprise-api:v1.2.0@sha256:ed6e963a2cdd1bdef63c51e82ca7b8bd2549f285f2e0510ea78d6f0adac40588
      volumeMounts:
      - name: config
        mountPath: /config
        subPath: pulsarr/apprise
      env:
        PUID: 568
        PGID: 568
        APPRISE_STATEFUL_MODE: simple
        APPRISE_WORKER_COUNT: "1"

dispatcharr:
  enabled: false
  image:
    repository: ghcr.io/dispatcharr/dispatcharr
    tag: 0.6.0@sha256:1902842f73deecf6958bf1b408f72f99ca08f1b0a9bdfdedc5e4194d5ee2a2be
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-dispatcharr,dispatcharr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 993      
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: dispatcharr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-dispatcharr
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"       
  command:
  - gunicorn 
  - --workers=4
  - --worker-class=gevent 
  - --timeout=300 
  - --bind 0.0.0.0:9191
  - dispatcharr.wsgi:application   
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9191
  envFrom:
  - configMapRef:
      name: dispatcharr-env
  - configMapRef:
      name: elfbot-dispatcharr
      optional: true      
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: dispatcharr
      - mountPath: /tmp
        name: tmp

requestrr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/requestrr
    tag: v2.1.6@sha256:d9daf341af2608f8351ae4cfdb6f685c1ea675e18b88860d0bfbc6343202402b
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-requestrr,requestrr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /app/config
      subPath: requestrr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-requestrr
          optional: true
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 4545
  envFrom:
  - configMapRef:
      name: elfbot-requestrr
      optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: requestrr
      - mountPath: /tmp
        name: tmp

debridav: &app_debridav
  enabled: false
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: debridav  
  image:
    repository: ghcr.io/elfhosted/debridav
    tag: 0.10.1@sha256:67748eff91c1c318827284bb31a9f39c2ce10b22e29e58ec4f65659152fd8cfe
  imagePullSecrets:
  - name: ghcr-io-elfhosted    
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-debridav,debridav-env,elfbot-plex,elfbot-jellyfin,elfbot-emby"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    tmp: *tmp
    backup: *backup
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory    
    config:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-debridav
          optional: true    
    import:
      enabled: true
      type: emptyDir
      mountPath: /import
      sizeLimit: 1Gi                   
  envFrom:
  - configMapRef:
      name: debridav-env
  - configMapRef:
      name: elfbot-debridav
      optional: true      
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080      
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /actuator/health/liveness
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /actuator/health/readiness
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10         
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 2Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: debridav
      - mountPath: /tmp
        name: tmp  
    # Only trigger this when moving to database
    setup-postgres:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/database
        mkdir -p /config/backups
        chown elfie:elfie /config/database -R
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: debridav
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
    restart-with-plex:
      image: ghcr.io/elfhosted/plex:1.41.8.9834-071366d65@sha256:dfaa43dc80599b36032386821517627b42e914b5716ca10e79d994dd3a534872
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when plex is updated"  
    restart-with-jellyfin:
      image: ghcr.io/elfhosted/jellyfin-dev:10.10.7@sha256:7ad104992e7908aa85b27cb4360886858caf406bc54abdc452b5b69dcce99b17
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when jellyfin is updated"  
    restart-with-emby:
      image: ghcr.io/elfhosted/emby:4.9.1.3@sha256:6a49f56c7293f903813b30c7ca3923a98d69621f0d86e06796c76b0e95c53ac6
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when emby is updated"   
  additionalContainers:
    make-folders:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |

        # Define the folders to check/create
        FOLDERS=("/storage/debridav/movies" "/storage/debridav/movies-4k" "/storage/debridav/series" "/storage/debridav/series-4k")

        # Function to check and create folders
        create_folders() {
          local all_created=true
          
          for folder in "${FOLDERS[@]}"; do
            if [ ! -d "$folder" ]; then
              echo "Folder $folder does not exist. Creating..."
              
              # sleep so that we have time to avoid a race on local vs dbfolders
              sleep 20s
              mkdir -p "$folder"
              
              # Check if creation was successful
              if [ ! -d "$folder" ]; then
                echo "Error creating $folder. Will retry in 10 seconds."
                all_created=false
              else
                echo "Successfully created $folder."
              fi
            else
              echo "Folder $folder already exists."
            fi
          done
          
          return $([ "$all_created" = true ] && echo 0 || echo 1)
        }

        # Main loop to create folders
        while true; do
          if create_folders; then
            echo "All required folders exist or were successfully created."
            break
          else
            echo "Waiting 10 seconds before retrying..."
            sleep 10
          fi
        done

        echo "Success! All required folders are now available."
        echo "Script will now sleep indefinitely."

        # Sleep forever
        while true; do
          sleep 3600  # Sleep for an hour at a time (to be a bit nicer to the system than an infinite tight loop)
        done
      volumeMounts:
      - name: rclonemountdebridav
        mountPath: /storage/debridav
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310
      env:
        - name: POSTGRES_PASSWORD
          value: debridav
        - name: POSTGRES_DB
          value: debridav
        - name: POSTGRES_USER
          value: debridav
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: debridav/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi
    fakearr:
      image: ghcr.io/elfhosted/fakearr:rolling@sha256:9734625ab6bc3f669d5ed5d8d58c6acd45000c068acfc36807c5c97f03531bbf
      envFrom:
      - configMapRef:
          name: debridav-env      
      - configMapRef:      
          name: elfbot-debridav
          optional: true  
      volumeMounts:
      - name: tmp
        mountPath: /tmp    
          
rclonedebridlink:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/davdebrid
    tag: v1.2.2@sha256:6571776a022a36889d1cc8392440ad58fa2654535e82b9741f1c287e045c7fd0
  priorityClassName: tenant-normal
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podLabels:
    app.elfhosted.com/name: debridlink
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-debridlink,debridlink-env"
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
    readOnlyRootFilesystem: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: debridlink
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  envFrom:
  - configMapRef:
      name: debridlink-env
  - configMapRef:
      name: elfbot-debridlink
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 100Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080


rclonealldebrid:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.70.1@sha256:67cbf11bdec4f38821ffc66fd526b7f040d023b7c68f200f1ce070fee99b8eb6
  command:
  - /debrid-provider.sh
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-alldebrid,alldebrid-config,gluetun-config"
  podAnnotations:
    kubernetes.io/egress-bandwidth: "128M"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    cache:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      sizeLimit: 1Gi
    config: 
      enabled: "true"
      type: emptyDir
      mountPath: /config
    bootstrap:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: alldebrid-config
    tinyproxy-conf:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: alldebrid-tinyproxy-conf 
  additionalContainers:
    # Use this to provied proxied access to mediaflowproxy
    tinyproxy:
      image: docker.io/kalaksi/tinyproxy
      volumeMounts:
      - name: tinyproxy-conf
        mountPath: /etc/tinyproxy/tinyproxy.conf
        subPath: tinyproxy.conf                      
  initContainers:
    setup:
      image: ghcr.io/elfhosted/rclone:1.70.1@sha256:67cbf11bdec4f38821ffc66fd526b7f040d023b7c68f200f1ce070fee99b8eb6
      command:
      - /bin/ash
      - -c
      - |
        set -x

        # Create directory structure

        OBSCURED_PASS=$(rclone obscure doesntmatter)
        cp /bootstrap/rclone-debrid-provider.conf /config/
        sed -i "s/REPLACEUSER/$USER/" /config/rclone-debrid-provider.conf
        sed -i "s/REPLACEPASS/$OBSCURED_PASS/" /config/rclone-debrid-provider.conf
        
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /bootstrap
        name: bootstrap
      resources: *default_resources
      securityContext: *default_securitycontext
      envFrom:
      - configMapRef:
          name: elfbot-alldebrid
          optional: true
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9999      
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi
  addons:
    vpn:
      enabled: true
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      envFrom:
      - configMapRef:
          name: gluetun-config
      env:
        DOT: "off"
        FIREWALL_INPUT_PORTS: "9999,8888" # 9999 is for rclone, 8888 is tinyproxy
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8,192.168.0.0/16,172.16.0.0/20
        DNS_KEEP_NAMESERVER: "on"        
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

rclonepremiumize:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.70.1@sha256:67cbf11bdec4f38821ffc66fd526b7f040d023b7c68f200f1ce070fee99b8eb6
  command:
  - /debrid-provider.sh
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-premiumize,premiumize-config"
  podAnnotations:
    kubernetes.io/egress-bandwidth: "128M"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    cache:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      sizeLimit: 1Gi
    config: 
      enabled: "true"
      type: emptyDir
      mountPath: /config
    bootstrap:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: premiumize-config
  initContainers:
    setup:
      image: ghcr.io/elfhosted/rclone:1.70.1@sha256:67cbf11bdec4f38821ffc66fd526b7f040d023b7c68f200f1ce070fee99b8eb6
      command:
      - /bin/ash
      - -c
      - |
        set -x

        # Create directory structure

        OBSCURED_PASS=$(rclone obscure "$PASS")
        cp /bootstrap/rclone-debrid-provider.conf /config/
        sed -i "s/REPLACEUSER/$USER/" /config/rclone-debrid-provider.conf
        sed -i "s/REPLACEPASS/$OBSCURED_PASS/" /config/rclone-debrid-provider.conf
        
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /bootstrap
        name: bootstrap
      resources: *default_resources
      securityContext: *default_securitycontext
      envFrom:
      - configMapRef:
          name: elfbot-premiumize
          optional: true
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9999      
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi

rclonetorbox:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.70.1@sha256:67cbf11bdec4f38821ffc66fd526b7f040d023b7c68f200f1ce070fee99b8eb6
  command:
  - /debrid-provider.sh
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-torbox,torbox-config"
  podAnnotations:
    kubernetes.io/egress-bandwidth: "128M"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    cache:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      sizeLimit: 1Gi
    # we'll run the obscure command and copy the config into here
    config: 
      enabled: "true"
      type: emptyDir
      mountPath: /config
    bootstrap:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: torbox-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-torbox
          optional: true          
  initContainers:
    setup:
      image: ghcr.io/elfhosted/rclone:1.70.1@sha256:67cbf11bdec4f38821ffc66fd526b7f040d023b7c68f200f1ce070fee99b8eb6
      command:
      - /bin/ash
      - -c
      - |
        set -x

        # Create directory structure

        OBSCURED_PASS=$(rclone obscure "$PASS")
        cp /bootstrap/rclone-debrid-provider.conf /config/
        sed -i "s/REPLACEUSER/$USER/" /config/rclone-debrid-provider.conf
        sed -i "s/REPLACEPASS/$OBSCURED_PASS/" /config/rclone-debrid-provider.conf
        
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /bootstrap
        name: bootstrap
      - mountPath: /elfbot
        name: elfbot
      resources: *default_resources
      securityContext: *default_securitycontext
      envFrom:
      - configMapRef:
          name: elfbot-torbox
          optional: true

  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9999
  envFrom:
  - configMapRef:
      name: elfbot-torbox
      optional: true              
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi


decypharr: &app_decypharr
  enabled: false
  image:
    repository: ghcr.io/elfhosted/decypharr
    tag: v1.0.4@sha256:907e38dcc531f75976dfb2840651897a84da27091414eca91fd5bac011ed1e55
  priorityClassName: tenant-normal
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  # Avoid search expansion DNS queries against coredns for every RD API request
  dnsConfig:
    options:
      - name: ndots
        value: "2"  
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-decypharr,decypharr-example-config,elfbot-plex,elfbot-emby,elfbot-jellyfin"
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
    readOnlyRootFilesystem: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 1
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8282
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: decypharr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: decypharr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs     
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-decypharr
          optional: true                      
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: decypharr
      - mountPath: /tmp
        name: tmp  
    # restart-with-plex:
    #   image: ghcr.io/elfhosted/plex:1.41.8.9834-071366d65@sha256:695ac4a1b79b82c2907e3c112a6bd8b1208a0b1b2e63ca4e9b7e06270651b13f
    #   command:
    #   - /bin/bash
    #   - -c
    #   - |
    #     echo "This pod only exists to cause a restart when plex is updated"  
    # restart-with-jellyfin:
    #   image: ghcr.io/elfhosted/jellyfin-dev:10.10.7@sha256:7ad104992e7908aa85b27cb4360886858caf406bc54abdc452b5b69dcce99b17
    #   command:
    #   - /bin/bash
    #   - -c
    #   - |
    #     echo "This pod only exists to cause a restart when jellyfin is updated"  
    # restart-with-emby:
    #   image: ghcr.io/elfhosted/emby:4.9.1.1@sha256:e81f21831c751bc9365ab9544d6500c0dc132b3cb5b2dc96e05e837ada470863
    #   command:
    #   - /bin/bash
    #   - -c
    #   - |
    #     echo "This pod only exists to cause a restart when emby is updated"        

blackhole: &app_blackhole
  enabled: false
  image:
    repository: ghcr.io/elfhosted/wests-blackhole-script
    tag: v1.5.1@sha256:3f7a6e09de005b57fec75762422c0880286a8c4d93a9871b3df1e88866fc8dc1
  priorityClassName: tenant-normal
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-blackhole,blackhole-env"
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
    readOnlyRootFilesystem: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  env:
    BLACKHOLE_RADARR_PATH: "radarr"
    BLACKHOLE_SONARR_PATH: "sonarr"
  envFrom:
  - configMapRef:
      name: blackhole-env
  - configMapRef:
      name: elfbot-blackhole
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 100Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: blackhole
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs

blackhole4k:
  <<: *app_blackhole
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: blackhole4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
  env:
    BLACKHOLE_RADARR_PATH: "radarr4k"
    BLACKHOLE_SONARR_PATH: "sonarr4k"
  envFrom:
  - configMapRef:
      name: blackhole-env
  - configMapRef:
      name: elfbot-blackhole
      optional: true   
  - configMapRef:
      name: elfbot-blackhole4k
      optional: true    

blackholetorbox:
  <<: *app_blackhole
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-blackholetorbox,blackholetorbox-env"
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: blackholetorbox
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
  env:
    BLACKHOLE_RADARR_PATH: "radarr"
    BLACKHOLE_SONARR_PATH: "sonarr"
  envFrom:
  - configMapRef:
      name: blackholetorbox-env
  - configMapRef:
      name: elfbot-blackholetorbox
      optional: true
  initContainers:
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x

        # Create directory structure
        mkdir -p /storage/symlinks/blackholetorbox/radarr/completed
        mkdir -p /storage/symlinks/blackholetorbox/radarr/processing
        mkdir -p /storage/symlinks/blackholetorbox/sonarr/completed
        mkdir -p /storage/symlinks/blackholetorbox/sonarr/processing
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      resources: *default_resources
      securityContext: *default_securitycontext

blackholetorbox4k:
  <<: *app_blackhole
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-blackholetorbox,blackholetorbox-env"
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: blackholetorbox4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
  env:
    BLACKHOLE_RADARR_PATH: "radarr4k"
    BLACKHOLE_SONARR_PATH: "sonarr4k"
  envFrom:
  - configMapRef:
      name: blackholetorbox-env
  - configMapRef:
      name: elfbot-blackholetorbox
      optional: true
  - configMapRef:
      name: elfbot-blackholetorbox4k
      optional: true      

channelsdvr:
  enabled: false
  image:
    repository: fancybits/channels-dvr
    tag: latest@sha256:284fed6f4ee5150d41d9a7f247a63e190f6f1c3a4e4bc740f029df6d36955d29
  priorityClassName: tenant-normal
  podAnnotations:
    kubernetes.io/egress-bandwidth: "125M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-channelsdvr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 15m
      memory: 200Mi
    limits:
      cpu: 1
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8089
  persistence:
    <<: *storagemounts
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /channels-dvr
      subPath: channelsdvr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-channelsdvr
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: channelsdvr
      - mountPath: /tmp
        name: tmp

wger:
  enabled: false
  image:
    repository: wger/server
    tag: latest@sha256:e477eb556bac4dc6715fefd1aaab0e297ecd15b4913d585436e808bb1ea5f3a0
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-wger,wger-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 15m
      memory: 200Mi
    limits:
      cpu: 1
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      subPath: immich
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-immich
          optional: true
  envFrom:
  - configMapRef:
      name: wger-env
  - configMapRef:
      name: elfbot-wger
      optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: wger
      - mountPath: /tmp
        name: tmp
  additionalContainers:
    # ml:
    #   image: ghcr.io/immich-app/immich-machine-learning:v1.131.2
    #   envFrom:
    #   - configMapRef:
    #       name: immich-env
    #   resources:
    #     requests:
    #       cpu: 15m
    #       memory: 200Mi
    #     limits:
    #       cpu: 500m
    #       memory: 4Gi
    # ngnix:
    #   image: nginxinc/nginx-unprivileged
    #   volumeMounts:
    #   - mountPath: /usr/share/nginx/html
    #     name: config
    #     subPath: overseerr/branding
    #     readOnly: true
    #   - mountPath: /tmp
    #     name: tmp
    #   - mountPath: /run
    #     name: run
    #   resources: *default_resources
    #   securityContext: *default_securitycontext    
    database:
      image: postgres:17-alpine
      env:
        POSTGRES_PASSWORD: wger
        POSTGRES_USER: wger
        POSTGRES_DB: wger
      volumeMounts:
        - mountPath: /var/lib/postgresql/data
          name: config
          subPath: wger/postgresql/database
      resources:
        requests:
          cpu: 15m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 4Gi
    redis:
      image: docker.io/redis:7.4-alpine@sha256:ee9e8748ace004102a267f7b8265dab2c618317df22507b89d16a8add7154273
      envFrom:
      - configMapRef:
          name: wger-env
      volumeMounts:
        - name: config
          subPath: wger/redis
          mountPath: /data         
      resources:
        requests:
          cpu: 15m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 4Gi
    celery-worker:
      image: wger/server:latest@sha256:e477eb556bac4dc6715fefd1aaab0e297ecd15b4913d585436e808bb1ea5f3a0
      command:
      - /start-worker
      volumeMounts:
        - name: config
          subPath: wger/media
          mountPath: /home/wger/media
    celery-beat:
      image: wger/server:latest@sha256:e477eb556bac4dc6715fefd1aaab0e297ecd15b4913d585436e808bb1ea5f3a0
      command:
      - /start-beat      
      volumeMounts:
        - name: config
          subPath: wger/beat
          mountPath: /home/wger/beat

immich:
  enabled: false
  image:
    repository: ghcr.io/immich-app/immich-server
    tag: v1.135.3@sha256:df5bbf4e29eff4688063a005708f8b96f13073200b4a7378f7661568459b31e9
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-immich,immich-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 15m
      memory: 200Mi
    limits:
      cpu: 500m
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 2283
  persistence:
    <<: *storagemounts
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    tmp: *tmp
    config:
      enabled: true
      type: custom
      subPath: immich
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    upload:
      enabled: true
      type: emptyDir
      mountPath: /usr/src/app/upload
      sizeLimit: 1Gi
    upload-encoded:
      enabled: true
      type: emptyDir
      mountPath: /usr/src/app/upload/encoded-video
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-immich
          optional: true
  envFrom:
  - configMapRef:
      name: immich-env
  - configMapRef:
      name: elfbot-immich
      optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: immich
      - mountPath: /tmp
        name: tmp
  additionalContainers:
    ml:
      image: ghcr.io/immich-app/immich-machine-learning:v1.131.2
      envFrom:
      - configMapRef:
          name: immich-env
      resources:
        requests:
          cpu: 15m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 4Gi
    database:
      image: ghcr.io/immich-app/postgres:14-vectorchord0.3.0-pgvectors0.2.0
      env:
        POSTGRES_INITDB_ARGS: '--data-checksums'
        POSTGRES_PASSWORD: immich
        POSTGRES_USER: immich
        POSTGRES_DB: immich
      volumeMounts:
        - name: config
          subPath: immich/database
          mountPath: /var/lib/postgresql/data
      resources:
        requests:
          cpu: 15m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 4Gi
    redis:
      image: docker.io/redis:7.4-alpine@sha256:ee9e8748ace004102a267f7b8265dab2c618317df22507b89d16a8add7154273
      envFrom:
      - configMapRef:
          name: immich-env
      resources:
        requests:
          cpu: 15m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 4Gi

kubernetesdashboard:

  ## Name of Priority Class of pods
  priorityClassName: "tenant-normal"

  ## Pod resource requests & limits
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 256Mi

  extraArgs:
    - --enable-skip-login
    - --enable-insecure-login
    - --system-banner=Built</A> with  by <A HREF="https://funkypenguin.co.nz">@funkypenguin</A> and friends (<I><A HREF="https://chat.funkypenguin.co.nz">join us!</A></I>)

  ## Serve application over HTTP without TLS
  ##
  ## Note: If set to true, you may want to add --enable-insecure-login to extraArgs
  protocolHttp: true

  # Global dashboard settings
  settings:
    ## Cluster name that appears in the browser window title if it is set
    clusterName: "ElfHosted"
    # defaultNamespace: "{{ .Release.Namespace }}"
    # namespaceFallbackList: [ "{{ .Release.Namespace }}" ]

    ## Max number of items that can be displayed on each list page
    itemsPerPage: 10
    ## Number of seconds between every auto-refresh of logs
    logsAutoRefreshTimeInterval: 5
    ## Number of seconds between every auto-refresh of every resource. Set 0 to disable
    resourceAutoRefreshTimeInterval: 5
    ## Hide all access denied warnings in the notification panel
    disableAccessDeniedNotifications: true

  ## Metrics Scraper
  ## Container to scrape, store, and retrieve a window of time from the Metrics Server.
  ## refs: https://github.com/kubernetes-sigs/dashboard-metrics-scraper
  metricsScraper:
    ## Wether to enable dashboard-metrics-scraper
    enabled: true
    image:
      repository: kubernetesui/metrics-scraper
      tag: v1.0.9
    resources: {}
    ## SecurityContext especially for the kubernetes dashboard metrics scraper container
    ## If not set, the global containterSecurityContext values will define these values
    # containerSecurityContext:
    #   allowPrivilegeEscalation: false
    #   readOnlyRootFilesystem: true
    #   runAsUser: 1001
    #   runAsGroup: 2001
  #  args:
  #    - --log-level=info
  #    - --logtostderr=true

  # Don't auto-create RBAC for us, we'll do it manually
  rbac:
    create: false

  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: kubernetes-dashboard

# optional but disabled by default to prevent errors


gluetun:
  enabled: false # just to avoid errors
cometproxystreaming:
  enabled: false
mediafusionproxystreaming:
  enabled: false
elfassesment:
  enabled: false

plexhobbit:
  <<: *app_plex
  podAnnotations: *hobbit_streamer_podAnnotations
  resources: *hobbit_streamer_resources
  persistence:
    <<: *app_plex_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

jellyfinhobbit:
  <<: *app_jellyfin
  podAnnotations: *hobbit_streamer_podAnnotations
  resources: *hobbit_streamer_resources
  persistence:
    <<: *app_jellyfin_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

embyhobbit:
  <<: *app_emby
  podAnnotations: *hobbit_streamer_podAnnotations
  resources: *hobbit_streamer_resources
  persistence:
    <<: *app_emby_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

plexranger:
  <<: *app_plex
  persistence:
    <<: *app_plex_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g
  podAnnotations: *ranger_streamer_podAnnotations
  resources: *ranger_streamer_resources

plexhalfling:
  <<: *app_plex
  persistence:
    <<: *app_plex_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  
  podAnnotations: *halfling_streamer_podAnnotations
  resources: *halfling_streamer_resources

jellyfinhalfling:
  <<: *app_jellyfin
  podAnnotations: *halfling_streamer_podAnnotations
  resources: *halfling_streamer_resources
  persistence:
    <<: *app_jellyfin_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: jellygfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

embyhalfling:
  <<: *app_emby
  podAnnotations: *halfling_streamer_podAnnotations
  resources: *halfling_streamer_resources
  persistence:
    <<: *app_emby_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

plexnazgul:
  <<: *app_plex
  persistence:
    <<: *app_plex_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  
  podAnnotations: *nazgul_streamer_podAnnotations
  resources: *nazgul_streamer_resources

jellyfinnazgul:
  <<: *app_jellyfin
  podAnnotations: *nazgul_streamer_podAnnotations
  resources: *nazgul_streamer_resources
  persistence:
    <<: *app_jellyfin_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

embynazgul:
  <<: *app_emby
  podAnnotations: *nazgul_streamer_podAnnotations
  resources: *nazgul_streamer_resources
  persistence:
    <<: *app_emby_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

# We use these to set aside resources for dedicated bundles
starter: &app_resource_reserver
  enabled: false
  image:
    repository: ghcr.io/elfhosted/tooling
    tag: focal-20240530@sha256:458d1f3b54e9455b5cdad3c341d6853a6fdd75ac3f1120931ca3c09ac4b588de
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "tooling-scripts" # Reload the deployment every time the yaml config changes
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  # we mount config to force the resource-reserver to run on the same node as the volumes
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config    
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755          
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  command:
  - /bin/bash
  - -c
  - |
    echo "This pod simple reserves contended resources for a tenant"

    sleep infinity
  affinity: *standard_affinity
  service:
    main:
      enabled: false
  probes:
    liveness:
      enabled: false
    startup:
      enabled: false
    readiness:
      enabled: false      
  resources:
    requests: 
      cpu: 250m
      memory: 1Gi
  initContainers:
    update-dns:  &update_dns_on_init
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - /tooling-scripts/update-dns-on-init.sh
      env:
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: ELF_TENANT_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['app.kubernetes.io/instance']
      envFrom:
      - secretRef:
          name: cloudflare-api-token
      volumeMounts:
      - mountPath: /tooling-scripts
        name: tooling-scripts
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
  additionalContainers:
    clean-up-dns:
      <<: *update_dns_on_init
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - /tooling-scripts/clean-up-dns-on-termination.sh      

streamer:
  enabled: false
  <<: *app_resource_reserver

hobbit:
  enabled: false
  <<: *app_resource_reserver

ranger:
  enabled: false
  <<: *app_resource_reserver

halfling:
  enabled: false
  <<: *app_resource_reserver

nazgul:
  enabled: false
  <<: *app_resource_reserver

# This file must end on a single newline
