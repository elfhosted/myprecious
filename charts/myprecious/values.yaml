# This controls whether our automation will auto-release this to stable during the daily maint window
safeToRelease: true

# false by default means the FSN cluster (so no migrations)
location:
  enabled: false
zurgling:
  enabled: false
debridling:
  enabled: false
sidekick:
  enabled: false

# false by default
volsync:
  enabled: false
  restic_repository:
  restic_password:
  aws_access_key_id:
  aws_secret_access_key:

# Set these to the default if nothing else is set
storageclass:
  rwx:
    name: ceph-filesystem-ssd
    accessMode: ReadWriteMany
    volumeSnapshotClassName: ceph-filesystem
  rwo:
    name: ceph-block-ssd
    accessMode: ReadWriteOnce
    volumeSnapshotClassName: ceph-block

# These control the egress bandwidth of the semi-dedi products
hobbit_streamer_podAnnotations: &hobbit_streamer_podAnnotations
  kubernetes.io/egress-bandwidth: "250M"
ranger_streamer_podAnnotations: &ranger_streamer_podAnnotations
  kubernetes.io/egress-bandwidth: "500M"
halfling_streamer_podAnnotations: &halfling_streamer_podAnnotations
  kubernetes.io/egress-bandwidth: "1000M"
nazgul_streamer_podAnnotations: &nazgul_streamer_podAnnotations
  kubernetes.io/egress-bandwidth: "1000M"

# These control the requests used to "anchor" a stack to a particular dedicated node. The following defaults can be overridden on a per-cluster basis:
hobbit_zurg_resources: &hobbit_zurg_resources
  requests:
    cpu: "1.8"
    memory: 30Mi
  limits:
    cpu: "2"
    memory: 4Gi

ranger_zurg_resources: &ranger_zurg_resources
  requests:
    cpu: "3500m"
    memory: 30Mi
  limits:
    cpu: "4"
    memory: 4Gi

halfling_zurg_resources: &halfling_zurg_resources
  requests:
    cpu: "7"
    memory: 30Mi
  limits:
    cpu: "8"
    memory: 4Gi

nazgul_zurg_resources: &nazgul_zurg_resources
  requests:
    cpu: "7"
    memory: 30Mi
  limits:
    cpu: "16"
    memory: 4Gi


# These allow us to manage RAM usage on streamers
hobbit_streamer_resources: &hobbit_streamer_resources
  requests:
    cpu: "10m"
    memory: 30Mi
  limits:
    cpu: "2"
    memory: 4Gi

ranger_streamer_resources: &ranger_streamer_resources
  requests:
    cpu: 10m
    memory: 30Mi
  limits:
    cpu: 4
    memory: 4Gi

# Giving more than 4 CPU to a streamer is unwise regardless
halfling_streamer_resources: &halfling_streamer_resources
  requests:
    cpu: 10m
    memory: 30Mi
  limits:
    cpu: 4
    memory: 4Gi

nazgul_streamer_resources: &nazgul_streamer_resources
  requests:
    cpu: 10m
    memory: 30Mi
  limits:
    cpu: 4
    memory: 4Gi

# sets the user's base dns domain
dns_domain: elfhosted.com

tooling_image: &tooling_image ghcr.io/elfhosted/tooling:focal-20240530@sha256:458d1f3b54e9455b5cdad3c341d6853a6fdd75ac3f1120931ca3c09ac4b588de

# all RD pods have to exist with zurg - make this soft for now
standard_affinity: &standard_affinity
  podAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app.elfhosted.com/role
          operator: In
          values:
          - nodefinder # use nodefinder in the absence of zurg...
      topologyKey: "kubernetes.io/hostname"

dedicated_affinity: &dedicated_affinity
  podAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app.elfhosted.com/role
          operator: In
          values:
          - nodefinder # use nodefinder in the absence of zurg...
      topologyKey: "kubernetes.io/hostname"

standard_tolerations: &standard_tolerations
# not using tolerations anymore
# - key: node-role.elfhosted.com/dedicated
#   operator: Exists
# - key: node-role.elfhosted.com/hobbit
#   operator: Exists

hobbit_tolerations: &hobbit_tolerations
# not using tolerations anymore
# - key: node-role.elfhosted.com/hobbit
#   operator: Exists

# Set minimal requests so that pods can co-exist with streamers
hobbit_resources: &hobbit_resources
  requests:
    cpu: "1m"
    memory: "16Mi"
  limits:
    cpu: "1"
    memory: 4Gi

ranger_resources: &ranger_resources
  requests:
    cpu: "1m"
    memory: "16Mi"
  limits:
    cpu: "2"
    memory: 8Gi

volumespec_ephemeral_volume_1000g: &volumespec_ephemeral_volume_1000g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 1000Gi

volumespec_ephemeral_volume_100g: &volumespec_ephemeral_volume_100g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 100Gi

volumespec_ephemeral_volume_1g: &volumespec_ephemeral_volume_1g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 1Gi

volumespec_ephemeral_volume_10g: &volumespec_ephemeral_volume_10g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 10Gi

volumespec_ephemeral_volume_50g: &volumespec_ephemeral_volume_50g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 50Gi

volumespec_ephemeral_volume_200g: &volumespec_ephemeral_volume_200g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 200Gi

volumespec_ephemeral_volume_500g: &volumespec_ephemeral_volume_500g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 500Gi

# And this makes the media / rclone mounts tidier.
rclonemountrealdebridzurg: &rclonemountrealdebridzurg
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: realdebrid-zurg
  mountPath: /storage/realdebrid-zurg
rclonemountdebridlink: &rclonemountdebridlink
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: debridlink
  mountPath: /storage/debridlink
rclonemountalldebrid: &rclonemountalldebrid
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: alldebrid
  mountPath: /storage/alldebrid
rclonemountpremiumize: &rclonemountpremiumize
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: premiumize
  mountPath: /storage/premiumize
rclonemounttorbox: &rclonemounttorbox
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: torbox
  mountPath: /storage/torbox
rclone: &rclone
  enabled: true # everyone gets an rclone mount
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: rclone
  mountPath: /storage/rclone  

# This simplfies the process of adding all the optional mounts to every app
storagemounts: &storagemounts
  rclone: *rclone
  rclonemountrealdebridzurg: *rclonemountrealdebridzurg
  rclonemountdebridlink: *rclonemountdebridlink
  rclonemountalldebrid: *rclonemountalldebrid
  rclonemountpremiumize: *rclonemountpremiumize
  rclonemounttorbox: *rclonemounttorbox
  tmp: &tmp
    enabled: true
    type: emptyDir
    mountPath: /tmp
  symlinks: &symlinks
    enabled: true
    type: custom
    volumeSpec:
      persistentVolumeClaim:
        claimName: symlinks
    mountPath: /storage/symlinks
  backup: &backup
    enabled: true
    type: custom
    volumeSpec:
      persistentVolumeClaim:
        claimName: backup

# The entire bootstrap sidecar/additionalcontainer
default_resources: &default_resources
  requests:
    cpu: 1m
    memory: 1Mi
    # ephemeral-storage: 50Mi
  limits:
    cpu: 1
    memory: 4Gi # just a safety net against bugs!
    # ephemeral-storage: 2Gi # a safety net against node ephemeral space exhaustion

default_securitycontext: &default_securitycontext
  seccompProfile:
    type: RuntimeDefault
  readOnlyRootFilesystem: true
  allowPrivilegeEscalation: false
  runAsUser: 568
  runAsGroup: 568
  capabilities:
    drop:
    - ALL

speedtest_securitycontext: &speedtest_securitycontext
  seccompProfile:
    type: RuntimeDefault
  readOnlyRootFilesystem: false
  allowPrivilegeEscalation: false
  runAsUser: 101
  runAsGroup: 101
  capabilities:
    drop:
    - ALL

# We use this to provide env not only to bootstrap, but also to the torrent clients which use elfvpn
# it's necessary since the wireguard configs are in S3
bootstrap_env: &bootstrap_env
- name: AWS_ACCESS_KEY_ID
  valueFrom:
    secretKeyRef:
      key: access-key-id
      name: b2-elfhosted-config-ro
- name: AWS_SECRET_ACCESS_KEY
  valueFrom:
    secretKeyRef:
      key: secret-key
      name: b2-elfhosted-config-ro
- name: S3_ENDPOINT_URL
  value: https://s3.us-west-000.backblazeb2.com
- name: K8S_APP_NAME
  valueFrom:
    fieldRef:
      fieldPath: metadata.labels['app.kubernetes.io/name']
- name: ELF_APP_NAME
  valueFrom:
    fieldRef:
      fieldPath: metadata.labels['app.elfhosted.com/name']

migrate_data: &migrate_data
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /bin/bash
  - -c
  - |

    if [[ ! -f /config/.migrated-20241007 ]]
    then
      if [[ ! -z "$(ls -A /migration)" ]]
      then
        echo "Migrating from /migration/..."
        cp -rfpv /migration/* /config/
        touch /config/.migrated-20241007
      fi
    else
      echo "No migration necessary"
    fi

  volumeMounts:
  - mountPath: /config
    name: config
  - mountPath: /migration
    name: migration

  env: *bootstrap_env
  resources: *default_resources
  securityContext: *default_securitycontext

update_dns_on_init: &update_dns_on_init
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /usr/bin/dumb-init
  - /bin/bash
  - -c
  - /tooling-scripts/update-dns-on-init.sh
  env:
    - name: MY_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: MY_POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: ELF_APP_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.labels['app.elfhosted.com/name']
    - name: ELF_TENANT_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.labels['app.kubernetes.io/instance']
    - name: CF_API_DOMAIN
      value: elfhosted.com
  envFrom:
  - secretRef:
      name: cloudflare-api-token
  volumeMounts:
  - mountPath: /tooling-scripts
    name: tooling-scripts
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true

# This is almost the same as the dns-update script, except we run it as a sidecar, waiting on pod termination
clean_up_dns_on_termination: &clean_up_dns_on_termination
  <<: *update_dns_on_init
  command:
  - /usr/bin/dumb-init
  - /bin/bash
  - -c
  - /tooling-scripts/clean-up-dns-on-termination.sh

bootstrap: &bootstrap
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /bin/bash
  - -c
  - |
    set -e

    # Allows us to use app.elfhosted.com/name, but fall back to app.kubernetes.io/name if the former doesn't exist
    if [[ -z "$ELF_APP_NAME" ]]; then
      ELF_APP_NAME=$K8S_APP_NAME
    fi

    # look for commands - we match specific names in order of least-destructive
    TIMESTAMP_NOW=$(date +%s)
    if [[ -f /etc/elfbot/pause ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/pause)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=pause
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/backup && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/backup)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=backup
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/reset && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/reset)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=reset
      fi
    fi

    case $COMMAND in

      "pause")
        echo "Recent pause command found, sleeping 5m.."
        sleep 300
        ;;

      "reset")
        echo "Recent reset command found, resetting"
        rm -rf /config/*
        ;;

      "backup")
        echo "Recent backup command found, backing up to /storage/backup/${ELF_APP_NAME}-${TIMESTAMP}"
        TIMESTAMP=$(printf '%(%Y-%m-%d--%H-%M)T\n' -1)
        cp -rfp /config /storage/backup/$ELF_APP_NAME-$TIMESTAMP
        ;;

    esac

    if [[ ! -f /config/i-am-bootstrapped ]]
    then
      echo "Bootstrapping from goldilocks config..."
      s5cmd sync s3://elfhosted-config/goldilocks/$ELF_APP_NAME/* /config/
      touch /config/i-am-bootstrapped

    fi

  volumeMounts:
  - mountPath: /etc/elfbot
    name: elfbot
  - mountPath: /config
    name: config
  - mountPath: /storage/backup
    name: backup
  - mountPath: /tmp
    name: tmp
  env: *bootstrap_env
  resources: *default_resources
  securityContext: *default_securitycontext

bootstrap_elfbot: &bootstrap_elfbot
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /bin/bash
  - -c
  - |
    set -e

    # Allows us to use app.elfhosted.com/name, but fall back to app.kubernetes.io/name if the former doesn't exist
    if [[ -z "$ELF_APP_NAME" ]]; then
      ELF_APP_NAME=$K8S_APP_NAME
    fi

    # look for commands - we match specific names in order of least-destructive
    TIMESTAMP_NOW=$(date +%s)
    if [[ -f /etc/elfbot/pause ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/pause)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=pause
      fi
    fi


    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/backup && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/backup)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=backup
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/reset && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/reset)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=reset
      fi
    fi

    case $COMMAND in

      "pause")
        echo "Recent pause command found, sleeping 5m.."
        sleep 300
        ;;

      "reset")
        echo "Recent reset command found, resetting"
        rm -rf /config/*
        ;;

      "backup")
        echo "Recent backup command found, backing up to /storage/elfstorage/backup/${ELF_APP_NAME}-${TIMESTAMP}"
        mkdir -p /storage/elfstorage/backup
        TIMESTAMP=$(printf '%(%Y-%m-%d--%H-%M)T\n' -1)
        cp -rfp /config /storage/elfstorage/backup/$ELF_APP_NAME-$TIMESTAMP
        ;;

    esac
  volumeMounts:
  - mountPath: /etc/elfbot
    name: elfbot
  - mountPath: /config
    name: config
  - mountPath: /tmp
    name: tmp
  env: *bootstrap_env
  resources: *default_resources
  securityContext: *default_securitycontext

storagehub_bootstrap: &storagehub_bootstrap
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /bin/bash
  - -c
  - |
    set -e

    # Allows us to use app.elfhosted.com/name, but fall back to app.kubernetes.io/name if the former doesn't exist
    if [[ -z "$ELF_APP_NAME" ]]; then
      ELF_APP_NAME=$K8S_APP_NAME
    fi

    # look for commands - we match specific names in order of least-destructive
    TIMESTAMP_NOW=$(date +%s)
    if [[ -f /etc/elfbot/pause ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/pause)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=pause
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/backup && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/backup)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=backup
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/reset && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/reset)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=reset
      fi
    fi

    case $COMMAND in

      "pause")
        echo "Recent pause command found, sleeping 5m.."
        sleep 300
        ;;

      "reset")
        echo "Recent reset command found, resetting"
        rm -rf /config/${ELF_APP_NAME}/*
        ;;

      "backup")
        echo "Recent backup command found, backing up to /storage/elfstorage/backup/${ELF_APP_NAME}-${TIMESTAMP}"
        mkdir -p /storage/elfstorage/backup
        TIMESTAMP=$(printf '%(%Y-%m-%d--%H-%M)T\n' -1)
        cp -rfp /config/${ELF_APP_NAME} /storage/elfstorage/backup/$ELF_APP_NAME-$TIMESTAMP
        ;;

    esac

    if [[ ! -f /config/${ELF_APP_NAME}/i-am-bootstrapped ]]
    then
      echo "Bootstrapping from goldilocks config..."
      s5cmd sync s3://elfhosted-config/goldilocks/$ELF_APP_NAME/* /config/${ELF_APP_NAME}/
      touch /config/${ELF_APP_NAME}/i-am-bootstrapped
    fi

  volumeMounts:
  - mountPath: /etc/elfbot
    name: elfbot
  - mountPath: /config
    name: config
  - mountPath: /tmp
    name: tmp
  env: *bootstrap_env
  resources: *default_resources
  securityContext: *default_securitycontext

# Eventually we'll remove the old one, and rename this to bootstrap
bootstrap_migration: &bootstrap_migration
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /bin/bash
  - -c
  - |
    set -e

    # Allows us to use app.elfhosted.com/name, but fall back to app.kubernetes.io/name if the former doesn't exist
    if [[ -z "$ELF_APP_NAME" ]]; then
      ELF_APP_NAME=$K8S_APP_NAME
    fi

    # look for commands - we match specific names in order of least-destructive
    TIMESTAMP_NOW=$(date +%s)
    if [[ -f /etc/elfbot/pause ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/pause)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=pause
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/backup && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/backup)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=backup
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/reset && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/reset)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=reset
      fi
    fi

    case $COMMAND in

      "pause")
        echo "Recent pause command found, sleeping 5m.."
        sleep 300
        ;;

      "reset")
        echo "Recent reset command found, resetting"
        rm -rfv /config/*
        ;;

      "backup")
        echo "Recent backup command found, backing up to /storage/elfstorage/backup/${ELF_APP_NAME}-${TIMESTAMP}"
        mkdir -p /storage/elfstorage/backup
        TIMESTAMP=$(printf '%(%Y-%m-%d--%H-%M)T\n' -1)
        cp -rfp /config /storage/elfstorage/backup/$ELF_APP_NAME-$TIMESTAMP
        ;;

    esac

    if [[ ! -f /config/i-am-migrated ]]
    then
      if [[ ! -z "$(ls -A /config-hdd)" ]]
      then
        echo "Migrating from /config-hdd/..."
        time cp /config-hdd/* /config/ -rfpv
        touch /config/i-am-migrated
      fi
    fi


    if [[ ! -f /config/i-am-bootstrapped ]]
    then
      echo "Bootstrapping from goldilocks config..."
      s5cmd sync s3://elfhosted-config/goldilocks/$ELF_APP_NAME/* /config/
      touch /config/i-am-bootstrapped
    fi
  volumeMounts:
  - mountPath: /etc/elfbot
    name: elfbot
  - mountPath: /config
    name: config
  - mountPath: /migation
    name: confighdd
  env: *bootstrap_env
  resources: *default_resources
  securityContext: *default_securitycontext

# This lets users buy blocks of 1TB storage, and add it to their 100Gi
elfstoragetb:
  quantity: 0
# And this lets a user buy a bundle (different SKU), and then still add more elfstorage later
elfstoragetbbundled:
  quantity: 0

# provide a default
userId: 1

# our VPN loadbalancerIP
torrentLoadBalancerIP: 10.0.42.101

# these are the "exposed" services which allow users to override SSO
# by themselves, they do nothing, but they allow us to selectively disable
# SSO on ingressroutes, or to use non-standard API keys in Homer
radarrexposed:
  enabled: false
  apikey: 041776c8d5f74bf295aa486d9d51c33a
radarr4kexposed:
  enabled: false
  apikey: 7da5d4ba79804527b78a78b68c7a0781
sonarrexposed:
  enabled: false
  apikey: a6f1c7d07fab4be49c5c1cb545f85a76
sonarr4kexposed:
  enabled: false
  apikey: e4f93c115169484bbed19821f7ac8e49
lidarrexposed:
  enabled: false
  apikey: 0e68e28531a249659737513d3102bfe9
readarrexposed:
  enabled: false
  apikey: 74b033ff59964011b8a32c014fdb9b68
readarraudioexposed:
  enabled: false
  apikey: 8496cefe2c6b46ee921e18caddf6a943
prowlarrexposed:
  enabled: false
  apikey: c53bc3bd17c645c3a457e5342a02cd66
bazarrexposed:
  enabled: false
  apikey: 94ab8212a12378fa5333cbf75a3c0390
bazarr4kexposed:
  enabled: false
  apikey: 393bda5f898886a2b87413e6452313af
qbittorrentexposed:
  enabled: false
rdtclientexposed:
  enabled: false
rdtclientalldebridexposed:
  enabled: false
delugeexposed:
  enabled: false
rutorrentexposed:
  enabled: false
sabnzbdexposed:
  enabled: false
  apikey: 8flkbru7ncdps3dzzgk48q2msz41m4on
nzbgetexposed:
  enabled: false
mylarrexposed:
  enabled: false
  apikey: 0f97f6a7f352c63eb43fcb7e53ea9d8f
rivenexposed:
  enabled: false
  apikey: 1nMZNC0Cg6UP7sblvFirUi9Sad4ga84u  
tunarrexposed:
  enabled: false
cometexposed:
  enabled: false  
davioexposed:
  enabled: false  
jackettioexposed:
  enabled: false  
knightcrawlerexposed:
  enabled: false  
mediafusionexposed:
  enabled: false
stremthruexposed:
  enabled: false  
stremiojackettexposed:
  enabled: false  
xtremioexposed:
  enabled: false
ersatztv:
  enabled: false  
threadfinexposed:
  enabled: false
zurgexposed:
  enabled: false
elfassessment:
  enabled: false
uptimekumacustomdomain:
  enabled: false
mattermostcustomdomain:
  enabled: false
vaultwardencustomdomain:
  enabled: false
jellyseerrcustomdomain:
  enabled: false
overseerrcustomdomain:
  enabled: false
plexcustomdomain:
  enabled: false
jellyfincustomdomain:
  enabled: false
embycustomdomain:
  enabled: false
flixiocustomdomain:
  enabled: false
pairdropcustomdomain:
  enabled: false

rutorrentgluetun: &rutorrent
  enabled: false
  sso:
    enabled: true
  automountServiceAccountToken: false
  image:
      repository: ghcr.io/elfhosted/rutorrent
      tag: 4.3.6-60@sha256:d26fa29c30bd2622137426dca451d0d670811096676e7a07b5d4d00bab275fd1
  priorityClassName: tenant-bulk
  podLabels:
    app.elfhosted.com/name: rutorrent
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,rutorrent-config,rutorrent-gluetun-config,elfbot-rutorrent" # Reload the deployment every time the yaml config changes
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    # runAsUser: 568 # enforced in env vars
    # runAsGroup: 568
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  envFrom:
  - configMapRef:
      name: elfbot-rutorrent
      optional: true
  # we need the injected initcontainer to run as root, so we can't change the pod-level uid/gid
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 568 # s6's fault
    # runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  persistence:
    <<: *storagemounts
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_100g
    config:
      enabled: true
      type: custom
      mountPath: /data/rtorrent/
      subPath: rutorrent
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rutorrent
          optional: true
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    port-range: # Used for dynamic port-forwarding
      enabled: true
      type: emptyDir
      mountPath: /port-range
      sizeLimit: 1Gi
    custom-rtlocal:
      enabled: "true"
      mountPath: "/.rtlocal.rc-elfhosted"
      subPath: ".rtlocal.rc-elfhosted"
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
    custom-rtorrentrc:
      enabled: "true"
      mountPath: "/.rtorrent.rc-elfhosted"
      subPath: ".rtorrent.rc-elfhosted"
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
    custom-s6-init-05:
      enabled: "true"
      mountPath: "/etc/cont-init.d/05-apply-elfhosted-config.sh"
      subPath: "05-apply-elfhosted-config.sh"
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
          defaultMode: 0755
    custom-s6-init-06:
      enabled: "true"
      mountPath: "/etc/cont-init.d/02-wait-for-vpn.sh"
      subPath: "02-wait-for-vpn.sh"
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
          defaultMode: 0755
    custom-s6-init-07:
      enabled: "true"
      mountPath: "/etc/cont-init.d/03-set-inbound-port.sh"
      subPath: "03-set-inbound-port.sh "
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
          defaultMode: 0755
    dante-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: dante-config
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: false # necessary for probes, but probes aren't working with vpn addon currently
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1200Mi
  env:
    # -- Set the container timezone
    PUID: 568
    GUID: 568
    RUTORRENT_PORT: 8080 # necessary for health checks
    # S6_READ_ONLY_ROOT: 1 # this seems to break rutorrent :(
    WAIT_FOR_VPN: "true"
    PORT_FILE: /data/rtorrent/forwarded-port
    WAN_IP_CMD: 'curl -s ifconfig.me'
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rutorrent
      - mountPath: /tmp
        name: tmp
    update-dns: *update_dns_on_init
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi
      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext: *default_securitycontext
      resources: *default_resources
      envFrom:
      - configMapRef:
          name: rutorrent-gluetun-config
  addons:
    vpn: &rutorrent_addons_vpn
      enabled: true
      type: gluetun
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:2ce9c1407bad9897f4269b3c7f53862befb52a25c712731c91ae05906d41358f
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      envFrom:
      - configMapRef:
          name: rutorrent-gluetun-config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
        subPath: rutorrent
      config: # We have to set this to null so that we can override with our own config

      # The scripts that get run when the VPN connection opens/closes are defined here.
      # The default scripts will write a string to represent the current connection state to a file.
      # Our qBittorrent image has a feature that can wait for this file to contain the word 'connected' before actually starting the application.
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus
  additionalContainers:
    # Use this to provied proxied access to arrs
    dante:
      image: ghcr.io/elfhosted/dante:v1.4.3
      env: *bootstrap_env
      securityContext: *default_securitycontext
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /etc/sockd.conf
        name: dante-config
        subPath: sockd.conf
    mam-helper:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |
        set -e
        set -x

        echo "Waiting for VPN to be connected..."
        while ! grep -s -q "connected" /shared/vpnstatus; do
            # Also account for gluetun-style http controller
            if (curl -s http://localhost:8042/v1/openvpn/status | grep -q running); then
                break
            fi
            echo "VPN not connected"
            sleep 2
        done
        echo "VPN Connected, processing cookies..."

        # If we have a cookie already, try to use it
        if [[ -f /config/mam/saved.cookies ]]; then
          curl -c /config/mam/saved.cookies -b /config/mam/saved.cookies https://t.myanonamouse.net/json/dynamicSeedbox.php  -o /config/mam/mam_id-curl-output.log
        fi

        # Now whether that worked or not, look for /config/mam/mam_id
        mkdir -p /config/mam
        while [ 1 ]; do
          if [[ -f /config/mam/mam_id ]]; then
            curl -c /config/mam/saved.cookies -b "mam_id=$(cat /config/mam/mam_id)" https://t.myanonamouse.net/json/dynamicSeedbox.php -o /config/mam/mam_id-curl-output.log
            mv /config/mam/mam_id /config/mam/mam_id_processed
          fi
          sleep 1m
        done
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: rutorrent
      - mountPath: /shared
        name: shared
      resources: *default_resources
      securityContext: *default_securitycontext
    clean-up-dns: *clean_up_dns_on_termination

rutorrentpia:
  <<: *rutorrent
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,rutorrent-config,rutorrent-pia-config,elfbot-rutorrent,dante-config" # Reload the deployment every time the yaml config changes
  addons:
    vpn:
      <<: *rutorrent_addons_vpn
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      envFrom:
      - configMapRef:
          name: rutorrent-pia-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rutorrent
      - mountPath: /tmp
        name: tmp

delugegluetun: &deluge
  enabled: false
  podLabels:
    app.elfhosted.com/name: deluge
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M"
  sso:
    enabled: true
  automountServiceAccountToken: false
  image:
    repository: ghcr.io/geek-cookbook/deluge
    tag: 2.1.1@sha256:448324e342c47020e4e9fbc236282ceb80ebebd7934a486a6f1e487a7e4034bf
  priorityClassName: tenant-bulk
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  # we need the injected initcontainer to run as root, so we can't change the pod-level uid/gid
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-deluge,deluge-gluetun-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    <<: *storagemounts
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_100g
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: deluge
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-deluge
          optional: true
    elfscripts:
      enabled: "true"
      mountPath: "/elfscripts/"
      type: "custom"
      volumeSpec:
        configMap:
          name: deluge-elfscripts
          defaultMode: 0755
    dante-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: dante-config
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 8112
  env:
    # -- Set the container timezone
    TZ: UTC
    PUID: 568
    PGID: 568
    DELUGE_LOGLEVEL: "info"
  envFrom:
  - configMapRef:
      name: elfbot-deluge
      optional: true
  extraEnvVars:
  - name: PORT_FILE
    valueFrom:
      configMapKeyRef:
        name: deluge-gluetun-config
        key: PORT_FILE
    optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/deluge/torrent_files

        JQ_FILTER=".listen_random_port=false"
        JQ_FILTER="${JQ_FILTER} | .pre_allocate_storage=false"
        JQ_FILTER="${JQ_FILTER} | .stop_seed_ratio=2"
        JQ_FILTER="${JQ_FILTER} | .cache_size=52428"
        JQ_FILTER="${JQ_FILTER} | .share_ratio_limit=2"
        JQ_FILTER="${JQ_FILTER} | .stop_seed_at_ratio=true"

        jq "${JQ_FILTER}" /config/core.conf > /config/core-new.conf
        cp /config/core-new.conf /config/core.conf

        # # Avoid session timeouts
        # sed -i  "s/session_timeout:\".*/session_timeout\": 99999,/" /config/web.conf

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /shared
        name: shared
      env: *bootstrap_env
      securityContext: *default_securitycontext
      envFrom:
      - configMapRef:
          name: deluge-gluetun-config
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1500Mi
  addons:
    vpn: &deluge_addons_vpn
      enabled: true
      type: gluetun
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:2ce9c1407bad9897f4269b3c7f53862befb52a25c712731c91ae05906d41358f
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      envFrom:
      - configMapRef:
          name: deluge-gluetun-config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
        subPath: deluge
      config: # We have to set this to null so that we can override with our own config

      # The scripts that get run when the VPN connection opens/closes are defined here.
      # The default scripts will write a string to represent the current connection state to a file.
      # Our qBittorrent image has a feature that can wait for this file to contain the word 'connected' before actually starting the application.
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus
  additionalContainers:
    deluge-web:
      image: ghcr.io/geek-cookbook/deluge:2.1.1@sha256:448324e342c47020e4e9fbc236282ceb80ebebd7934a486a6f1e487a7e4034bf
      command:
      - /usr/bin/deluge-web
      - -L
      - info
      - -d
      - -c
      - /config
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /tmp
        name: tmp
      env:
        PYTHON_EGG_CACHE: /tmp/.cache

delugepia:
  <<: *deluge
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,deluge-config,deluge-pia-config,elfbot-deluge" # Reload the deployment every time the yaml config changes
  addons:
    vpn:
      <<: *deluge_addons_vpn
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      envFrom:
      - configMapRef:
          name: deluge-pia-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/deluge/torrent_files

        JQ_FILTER=".listen_random_port=false"
        JQ_FILTER="${JQ_FILTER} | .pre_allocate_storage=false"
        JQ_FILTER="${JQ_FILTER} | .stop_seed_ratio=2"
        JQ_FILTER="${JQ_FILTER} | .cache_size=52428"
        JQ_FILTER="${JQ_FILTER} | .share_ratio_limit=2"
        JQ_FILTER="${JQ_FILTER} | .stop_seed_at_ratio=true"

        jq "${JQ_FILTER}" /config/core.conf > /config/core-new.conf
        cp /config/core-new.conf /config/core.conf

        # # Avoid session timeouts
        # sed -i  "s/session_timeout:\".*/session_timeout\": 99999,/" /config/web.conf

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /shared
        name: shared
      env: *bootstrap_env
      securityContext: *default_securitycontext
  additionalContainers:
    deluge-web:
      image: ghcr.io/geek-cookbook/deluge:2.1.1@sha256:448324e342c47020e4e9fbc236282ceb80ebebd7934a486a6f1e487a7e4034bf
      command:
      - /usr/bin/deluge-web
      - -L
      - info
      - -d
      - -c
      - /config
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /tmp
        name: tmp
      env:
        PYTHON_EGG_CACHE: /tmp/.cache
    # Use this to provied proxied access to arrs
    dante:
      image: ghcr.io/elfhosted/dante:v1.4.3
      env: *bootstrap_env
      securityContext: *default_securitycontext
      volumeMounts:
      - mountPath: /tmp
        name: tmp

qbittorrentgluetun: &qbittorrent
  podLabels:
    app.elfhosted.com/name: qbittorrent
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M"
  enabled: false
  sso:
    enabled: true
  automountServiceAccountToken: false
  image:
    registry: ghcr.io
    repository: elfhosted/qbittorrent
    tag: 5.0.2@sha256:7ff09d6a5ca2267f78161fb46eeafaf5b2af7806288fe7f3d2dfed2521374e3d
  priorityClassName: tenant-bulk
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't seem to work well with entrypoint
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-qbittorrent,qbittorrent-gluetun-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    <<: *storagemounts
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_100g
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: qbittorrent
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-qbittorrent
          optional: true
    elfscripts:
      enabled: "true"
      mountPath: "/elfscripts/"
      type: "custom"
      volumeSpec:
        configMap:
          name: qbittorrent-elfscripts
          defaultMode: 0755
    dante-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: dante-config
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
      nameOverride: spanky
  env:
    # -- Set the container timezone
    TZ: UTC
    HOME: /config
    XDG_CONFIG_HOME: /config
    XDG_DATA_HOME: /config
    WAIT_FOR_VPN: "true"
  envFrom:
  - configMapRef:
      name: elfbot-qbittorrent
      optional: true
  extraEnvVars:
  - name: PORT_FILE
    valueFrom:
      configMapKeyRef:
        name: qbittorrent-gluetun-config
        key: PORT_FILE
    optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Remove the lockfile if it exists
        if [[ -f /config/qBittorrent/lockfile ]]; then
          rm /config/qBittorrent/lockfile
        fi

        mkdir -p /config/qBittorrent/torrent_files/complete
        mkdir -p /config/qBittorrent/torrent_files/incomplete

        # Enforce 1:1 seeding ratio, and then delete
        sed -i  "s/Session\\\GlobalMaxRatio=.*/Session\\\GlobalMaxRatio=1/" /config/qBittorrent/qBittorrent.conf

        # Permit TCP only
        sed -i  "s/Session\\\BTProtocol=.*/Session\\\BTProtocol=TCP/" /config/qBittorrent/qBittorrent.conf

        # Disable CSRF protection so that Homer can show qBit stats
        sed -i  "s/WebUI\\\CSRFProtection=.*/WebUI\\\CSRFProtection=false/" /config/qBittorrent/qBittorrent.conf

        # Insist on tun0
        sed -i  "s/Session\\\Interface=.*/Session\\\Interface=tun0/" /config/qBittorrent/qBittorrent.conf
        sed -i  "s/Session\\\InterfaceName=.*/Session\\\InterfaceName=tun0/" /config/qBittorrent/qBittorrent.conf

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /shared
        name: shared
      securityContext: *default_securitycontext
      resources: *default_resources
      envFrom:
      - configMapRef:
          name: qbittorrent-gluetun-config
  additionalContainers:
    # Use this to provied proxied access to arrs
    dante:
      image: ghcr.io/elfhosted/dante:v1.4.3
      env: *bootstrap_env
      securityContext: *default_securitycontext
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /etc/sockd.conf
        name: dante-config
        subPath: sockd.conf
    mam-helper:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |
        set -e
        set -x

        echo "Waiting for VPN to be connected..."
        while ! grep -s -q "connected" /shared/vpnstatus; do
            # Also account for gluetun-style http controller
            if (curl -s http://localhost:8042/v1/openvpn/status | grep -q running); then
                break
            fi
            echo "VPN not connected"
            sleep 2
        done
        echo "VPN Connected, processing cookies..."

        # If we have a cookie already, try to use it
        if [[ -f /config/mam/saved.cookies ]]; then
          curl -c /config/mam/saved.cookies -b /config/mam/saved.cookies https://t.myanonamouse.net/json/dynamicSeedbox.php  -o /config/mam/mam_id-curl-output.log
        fi

        # Now whether that worked or not, look for /config/mam/mam_id
        mkdir -p /config/mam
        while [ 1 ]; do
          if [[ -f /config/mam/mam_id ]]; then
            curl -c /config/mam/saved.cookies -b "mam_id=$(cat /config/mam/mam_id)" https://t.myanonamouse.net/json/dynamicSeedbox.php -o /config/mam/mam_id-curl-output.log
            mv /config/mam/mam_id /config/mam/mam_id_processed
          fi
          sleep 1m
        done
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /shared
        name: shared
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
      ephemeral-storage: 50Mi
    limits:
      cpu: 500m
      memory: 2Gi # .2 GB for headroom
      ephemeral-storage: 100Mi # a safety net against node ephemeral space exhaustion
  addons:
    vpn: &qbittorrent_addons_vpn
      enabled: true
      type: gluetun
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:2ce9c1407bad9897f4269b3c7f53862befb52a25c712731c91ae05906d41358f
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      envFrom:
      - configMapRef:
          name: qbittorrent-gluetun-config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
        subPath: qbittorrent
      config: # We have to set this to null so that we can override with our own config

      # The scripts that get run when the VPN connection opens/closes are defined here.
      # The default scripts will write a string to represent the current connection state to a file.
      # Our qBittorrent image has a feature that can wait for this file to contain the word 'connected' before actually starting the application.
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus


# Custom service for pia
qbittorrentpia:
  <<: *qbittorrent
  env:
    PORT_FILE: /config/forwarded-port
    WAIT_FOR_VPN: "true"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-qbittorrent,qbittorrent-pia-config"
  addons:
    vpn:
      <<: *qbittorrent_addons_vpn
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      envFrom:
      - configMapRef:
          name: qbittorrent-pia-config
  additionalContainers:
    # Use this to provied proxied access to arrs
    dante:
      image: ghcr.io/elfhosted/dante:v1.4.3
      env: *bootstrap_env
      securityContext: *default_securitycontext
      volumeMounts:
      - mountPath: /tmp
        name: tmp
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Remove the lockfile if it exists
        if [[ -f /config/qBittorrent/lockfile ]]; then
          rm /config/qBittorrent/lockfile
        fi

        mkdir -p /config/qBittorrent/torrent_files/complete
        mkdir -p /config/qBittorrent/torrent_files/incomplete

        # Enforce 1:1 seeding ratio, and then delete
        sed -i  "s/Session\\\GlobalMaxRatio=.*/Session\\\GlobalMaxRatio=1/" /config/qBittorrent/qBittorrent.conf

        # Permit TCP only
        sed -i  "s/Session\\\BTProtocol=.*/Session\\\BTProtocol=TCP/" /config/qBittorrent/qBittorrent.conf

        # Disable CSRF protection so that Homer can show qBit stats
        sed -i  "s/WebUI\\\CSRFProtection=.*/WebUI\\\CSRFProtection=false/" /config/qBittorrent/qBittorrent.conf

        # Insist on tun0
        sed -i  "s/Session\\\Interface=.*/Session\\\Interface=tun0/" /config/qBittorrent/qBittorrent.conf
        sed -i  "s/Session\\\InterfaceName=.*/Session\\\InterfaceName=tun0/" /config/qBittorrent/qBittorrent.conf

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /shared
        name: shared
      securityContext: *default_securitycontext
      resources: *default_resources

nzbget:
  enabled: false
  sso:
    enabled: true
  image:
    repository: ghcr.io/elfhosted/nzbget
    tag: 24.5@sha256:8861eae1bdf7b5be8986dcfb75b793948652d3a63c5b0d355a88a99519027dca
  priorityClassName: tenant-bulk
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-nzbget"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: nzbget
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_500g
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-nzbget
          optional: true

  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6789
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: nzbget
      - mountPath: /tmp
        name: tmp

sabnzbd:
  enabled: false
  hostname: sabnzbd # required to prevent whitelisting requirement per https://sabnzbd.org/wiki/extra/hostname-check.html
  podLabels:
    app.elfhosted.com/class: nzb
  sso:
    enabled: true
  image:
    registry: ghcr.io
    repository: elfhosted/sabnzbd
    tag: 4.3.3@sha256:af2ef54052d0d340064997aeb76bb8e612f3b47a8a0fc5c446e821a8bacd80cc
  priorityClassName: tenant-bulk
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-sabnzbd"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: sabnzbd
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_500g
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-sabnzbd
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: sabnzbd
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # fix host_whitelist
        sed -i  's/goldilocks/{{ .Release.Name }}/g' /config/sabnzbd.ini

        # If we've previously backed up a queue, then restore it to /tmp
        files=$(shopt -s nullglob dotglob; echo /config/queue-backup/*)
        if (( ${#files} ))
        then
          cp /config/queue-backup/* /tmp/ -rfp
          rm -rf /config/queue-backup
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: sabnzbd
      - mountPath: /tmp
        name: tmp
      env: *bootstrap_env
      securityContext: *default_securitycontext
      resources: *default_resources
  resources:
    requests:
      cpu: 1m
      memory: 512Mi
    limits:
      cpu: 1500m # if par threads is 1, this leaves 0.5cpu for downloading
      memory: 1500Mi
  additionalContainers:
    backup-queue:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |
        set -e
        IFS=$'\n' # in case of paths with spaces (looking at you, Plex!)

        function backupqueue_on_shutdown {
            echo "Received SIGTERM, waiting 5s for app to shut down..."
            mkdir -p /config/queue-backup
            sleep 5s

            # sync any files < 1MB
            cd /tmp
            find ./ -type f -size -1024k | rsync -avr --files-from=- /tmp /config/queue-backup
        }

        # When we terminate, perform the backup
        trap backupqueue_on_shutdown SIGTERM

        # Hang around doing nothing until terminated
        while true
        do
            echo "Waiting for SIGTERM to backup queue from /tmp"
            sleep infinity
        done
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: sabnzbd
      - mountPath: /tmp
        name: tmp
      env: *bootstrap_env
      securityContext: *default_securitycontext
      resources: *default_resources

  env:
    HOST_WHITELIST_ENTRIES: "{{ .Release.Name }}.sabnzbd.elfhosted.com"
    SABNZBD_UID: 568
    SABNZBD_GID: 568

tautulli:
  enabled: false
  sso:
    enabled: true
  image:
    registry: ghcr.io
    repository: elfhosted/tautulli
    tag: 2.15.0@sha256:e763974ccd5559086786242b7e02db04930e9b45a945ad31aa6a5b0ec9a246cc
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-tautulli"
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: tautulli
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-tautulli
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8181
  resources:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: tautulli
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/sh
      - -c
      - |
        set -x
        set -e

        # Clear out logs older than 24h
        if [ -d "/config/logs" ]; then
            # Find and delete files older than 7 days
            find "/config/logs" -type f -mtime +1 -exec rm -f {} \;
            echo "Files older than 1 day have been removed from /config/logs."
        fi

        # Clear out backups older than 2d
        if [ -d "/config/backups" ]; then
            # Find and delete files older than 2 days
            find "/config/backups" -type f -mtime +2 -exec rm -f {} \;
            echo "Files older than 1 day have been removed from /config/backups."
        fi
      volumeMounts:
      - mountPath: /config
        name: config
      resources: *default_resources
      securityContext: *default_securitycontext

radarr: &app_radarr
  enabled: false
  podLabels:
    app.elfhosted.com/name: radarr
    app.elfhosted.com/class: debrid
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/radarr
    tag: 5.16.3.9541@sha256:ad9e45ba676e8a1b014368c756fa452224e2611c78330583311ebd7d0d46253e
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-radarr" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: radarr-env
  - secretRef:
      name: radarr-env
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: radarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: radarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: radarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-radarr
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 7878
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: radarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/radarr
        mkdir -p /storage/symlinks/movies

      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 1m
      memory: 500Mi
    limits:
      cpu: 1.5
      memory: 2Gi

radarr4k: &app_radarr4k
  enabled: false
  podLabels:
    app.elfhosted.com/name: radarr4k
    app.elfhosted.com/class: debrid
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/radarr
    tag: 5.16.3.9541@sha256:ad9e45ba676e8a1b014368c756fa452224e2611c78330583311ebd7d0d46253e
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-radarr4k" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: radarr4k-env
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: radarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-radarr4k
          optional: true
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: radarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: radarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 7878
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: radarr4k
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/radarr4k
        mkdir -p /storage/symlinks/movies-4k

      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 1m
      memory: 500Mi
    limits:
      cpu: 1.5
      memory: 2Gi


ombi:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/ombi
    tag: 4.47.1@sha256:7728ad073e38a888335e74b70136202482570b0e16bd398ef20176ea2e2b3c1e
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-ombi"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  sso:
    enabled: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: ombi
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-ombi
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5000
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: ombi
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 3m
      memory: 150Mi
    limits:
      cpu: 2
      memory: 1Gi

scannarr: &app_scannarr
  enabled: false
  sso:
    enabled: true
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/scannarr
    tag: rolling@sha256:c9cbc74b5dff7a25b5c32ecc0e081498d77c85af4725dcd9e5c9a74df35d865d
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-scannarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    sonarr-settings:
      enabled: "true"
      mountPath: "/app/settings_sonarr.json"
      subPath: "settings_sonarr.json"
      type: "custom"
      volumeSpec:
        configMap:
          name: scannarr-config
    radarr-settings:
      enabled: "true"
      mountPath: "/app/settings_radarr.json"
      subPath: "settings_radarr.json"
      type: "custom"
      volumeSpec:
        configMap:
          name: scannarr-config
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true
      ports:
        http:
          port: 9898 # doesn't matter this doesn,t actually use ports
  additionalContainers:
    podinfo:
      image: stefanprodan/podinfo # used to run probes from gatus
  resources: *default_resources

scannarr4k:
  <<: *app_scannarr
  enabled: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-scannarr4k"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
    sonarr-settings:
      enabled: "true"
      mountPath: "/app/settings_sonarr.json"
      subPath: "settings_sonarr.json"
      type: "custom"
      volumeSpec:
        configMap:
          name: scannarr4k-config
    radarr-settings:
      enabled: "true"
      mountPath: "/app/settings_radarr.json"
      subPath: "settings_radarr.json"
      type: "custom"
      volumeSpec:
        configMap:
          name: scannarr4k-config

bazarr:
  enabled: false
  sso:
    enabled: true
  image:
    registry: ghcr.io
    repository: elfhosted/bazarr
    tag: 1.5.1@sha256:1e04bf419e6408b0bc65403e1d43d1ad3283d4c56f489ec00d1750561951e1d7
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-bazarr"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: bazarr-config
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: bazarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-bazarr
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6767
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: bazarr
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

bazarr4k:
  enabled: false
  sso:
    enabled: true
  image:
    registry: ghcr.io
    repository: elfhosted/bazarr
    tag: 1.5.1@sha256:1e04bf419e6408b0bc65403e1d43d1ad3283d4c56f489ec00d1750561951e1d7
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-bazarr4k"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: bazarr4k-config
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: bazarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-bazarr4k
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6767
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: bazarr4k
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

filebrowser:
  hostname: elfhosted
  enabled: true
  podLabels:
    app.elfhosted.com/name: filebrowser
  image:
    repository: ghcr.io/elfhosted/filebrowser
    tag: 2.23.0@sha256:2b236a953c6221d7a29918ec6e58beebdbb362cbfbe2431299df21077f543b43
  podAnnotations:
    kubernetes.io/egress-bandwidth: "5M" # filebrowser is not for streaming
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  envFrom:
  - configMapRef:
      name: filebrowser-env
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  deploymentStrategy:
    type: Recreate
    rollingUpdate: null
  controller:
    replicas: 1 # not sure we need 2 replicas anymore
    strategy: Recreate
    # rollingUpdate:
    #   unavailable: 1
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,filebrowser-elfbot-script,elfbot-filebrowser" # Reload the deployment every time the rclones change
  # We will use this to alter configmaps to trigger pod restarts
  serviceAccount:
    create: true
    name: filebrowser
  automountServiceAccountToken: true
  persistence:
    <<: *storagemounts  
    backup:
      enabled: true
      type: custom
      mountPath: /storage/backup
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    config:
      enabled: true
      type: custom
      mountPath: /storage/config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /storage/logs
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    elfterm-state: # so auto-provisioning doesn't break
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mouthPath: /home/elfie/.local/state
    dummy-storage: # so auto-provisioning doesn't break
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
    elfbot:
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mountPath: /elfbot
    elfbot-script:
      enabled: "true"
      mountPath: "/usr/local/bin/elfbot"
      subPath: "elfbot"
      type: "custom"
      volumeSpec:
        configMap:
          name: filebrowser-elfbot-script
          defaultMode: 0755
    elfbot-script-ucfirst:
      enabled: "true"
      mountPath: "/usr/local/bin/Elfbot" # make it easier for mobile users
      subPath: "elfbot"
      type: "custom"
      volumeSpec:
        configMap:
          name: filebrowser-elfbot-script
          defaultMode: 0755
    recyclarr-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: recyclarr-config
    symlinks: *symlinks
    tmp: *tmp
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080 # this allows us to run as non-root

  ingress:
    main:
      enabled: false
  initContainers:
    setup:
      image: ghcr.io/elfhosted/filebrowser:2.23.0@sha256:2b236a953c6221d7a29918ec6e58beebdbb362cbfbe2431299df21077f543b43
      # 2.23.0@sha256:1db0f0114a169ea2a877d75c47903a6d01534340421948845d5e298c7ac7ceb4 is the last good version for TFA
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Delete tmp db if necessary
        if [ -f /tmp/filebrowser.db ]
        then
          rm /tmp/filebrowser.db
        fi


        /filebrowser config init \
          --disable-preview-resize \
          --disable-thumbnails \
          --disable-type-detection-by-header \
          --branding.name="{{ .Release.Name }}, by ElfHosted 🧝 " \
          --branding.files=/branding \
          --branding.disableExternal \
          --auth.method=noauth \
          --lockPassword \
          --database /tmp/filebrowser.db \
          --root /storage \
          --cache-dir /tmp

        # allow zip, unzip, rar, unrar, ls, pwd, cd, mv
        /filebrowser config set --database /tmp/filebrowser.db --commands zip,unzip,rar,unrar,ls,pwd,cd,mv,cp,ln,find,echo,grep,cat,touch,tar,gzip,rm,tree,du,mlocate,updatedb,locate,elfbot,Elfbot
        # /filebrowser config set --database /tmp/filebrowser.db --shell 'vstat -c'

        # now tell filebrowser about the user (who gets authenticated via the proxy)
        /filebrowser users add 1 bogus --database /tmp/filebrowser.db

      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /storage
        name: dummy-storage
      resources: *default_resources
      securityContext: *default_securitycontext
    copy-recyclarr-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e
        # If we don't already have an example config, create one
        if [ ! -f /config/recyclarr.yaml ];
        then
          cp /bootstrap/recyclarr.yaml /config/
        fi
      volumeMounts:
      - mountPath: /config/
        name: config
        subPath: recyclarr
      - name: recyclarr-config
        mountPath: "/bootstrap/"
      securityContext: *default_securitycontext
    replace-plextraktsync-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e
        # Only do this if a plextraktconfig even exists
        if [ -f /storage/config/elfterm/PlexTraktSync/config.yml ];
        then
          rm /storage/config/elfterm/PlexTraktSync/*
          echo "PlexTraktSync config has moved to config/plextracktsync (where it always should have been!)" > /storage/config/elfterm/PlexTraktSync/where-is-my-config-readme.txt
        fi
      volumeMounts:
      - mountPath: /config/
        name: config
      securityContext: *default_securitycontext      
  additionalContainers:
    # this container exists to watch for restarts requested by elfbot, and to use create configmaps to trigger restarts using reloader
    elfbot:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |
        # respond to creation or modification, but not deletion
        inotifywait -m -e create -e modify --format "%f" /elfbot \
          | while read APP
            do
              # if we are force-killing the pod, then don't bother with the configmap
              if (cat /elfbot/$APP | grep -q forcerestart); then
                echo "forcerestart requested, deleting $APP pod with --force.."
                kubectl delete pod -l app.kubernetes.io/name=$APP --force
                kubectl delete pod -l app.elfhosted.com/name=$APP --force
              else

                # put the contents of the file into the configmap which will trigger the restart
                echo command received for ${APP} : [$(cat /elfbot/$APP)]
                # create the configmap if it doesn't exist, since reloader only looks at _changes_ to configmaps
                if ! $(kubectl get configmap -n {{ .Release.Namespace }} elfbot-${APP} 2>&1 >/dev/null); then
                    kubectl create configmap -n {{ .Release.Namespace }} elfbot-${APP} --from-literal=elfbot_last_action=$(date +%s)
                    sleep 10s
                fi

                # If we were passed a key=value string in /etc/elfbot, then split it
                COMMAND=$(cat /elfbot/$APP)

                # We separate key and value with an '=', but sometimes the value may contain __another__ '=' (like Plex preferences)
                sep='='
                case $COMMAND in
                  # If we are separated by an =
                  (*"$sep"*)
                    KEY=${COMMAND%%"$sep"*}
                    VALUE=${COMMAND#*"$sep"}
                    ;;
                  # if not, we are a simple command like "backup"
                  (*)
                    KEY=$COMMAND
                    VALUE=$(date +%s)
                    ;;
                esac


                # patch the configmap with the latest key/value
                kubectl patch configmap -n {{ .Release.Namespace }} elfbot-${APP} -p "{\"data\":{\"${KEY}\":\"${VALUE}\"}}"
              fi
            done
      volumeMounts:
      - mountPath: /elfbot
        name: elfbot
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 1m
      memory: 6Mi
    limits:
      cpu: 1
      memory: 1Gi


uptimekuma:
  enabled: false
  sso:
    enabled: true
  image:
    repository: ghcr.io/elfhosted/uptime-kuma
    tag: 1.23.16@sha256:546c07fbd8e038fe0b7331c6efbc14724002fcf809396726d8209db688a0d0c9
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-uptimekuma"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/data/
      subPath: uptimekuma
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-uptimekuba
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: uptimekuma
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 1m
      memory: 512Mi
    limits:
      cpu: 100m
      memory: 1Gi

privatebin:
  enabled: false
  sso:
    enabled: false
  image:
    repository: privatebin/fs
    tag: 1.7.5
  priorityClassName:
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-privatebin"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # crashes privatebin, TBD to determine why, and whether an emptydir /tmpfs might help
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /srv/data
      subPath: privatebin
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-privatebin
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  resources:
    requests:
      cpu: 1m
      memory: 64Mi
      ephemeral-storage: 50Mi
    limits:
      cpu: 100m
      memory: 128Mi
      ephemeral-storage: 100Mi # a safety net against node ephemeral space exhaustion
  config:
    main:
      discussion: false
      opendiscussion: false
      password: true
      fileupload: true
      burnafterreadingselected: false
      defaultformatter: "plaintext"
      syntaxhighlightingtheme: "sons-of-obsidian"
      sizelimit: 1048576
      template: "bootstrap-dark"
      info: "Hosted with ❤️ by ElfHosted 🧝"
      languageselection: true
      languagedefault: "en"
      # urlshortener: "https://shortener.example.com/api?link="
      qrcode: false
      icon: "none"
      zerobincompatibility: false
      # httpwarning: true
      compression: "zlib"
    expire:
      default: "1week"
    expire_options:
      5min: 300
      10min: 600
      1hour: 3600
      1day: 86400
      1week: 604800
    formatter_options:
      plaintext: "Plain Text"
      syntaxhighlighting: "Source Code"
      markdown: "Markdown"
    traffic:
      limit: 10
      # exemptedIp: "1.2.3.4,10.10.10/24"

kapowarr:
  enabled: false
  sso:
    enabled: true
  image:
    registry: ghcr.io
    repository: elfhosted/kapowarr
    tag: V1.0.0@sha256:09443693816a5152c0e6beeb0afe810e8bdf048c8a641009b2d2dea50140ce1e
  priorityClassName:
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-kapowarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # breaks kapowarr
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  probes:
    liveness:
      enabled: false
    startup:
      enabled: false
    readiness:
      enabled: false
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: kapowarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    temp-downloads:
      enabled: true
      type: emptyDir
      mountPath: /app/temp_downloads
      sizeLimit: 10Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-kapowarr
          optional: true
    tmp: *tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5656
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: kapowarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/comics
        mkdir -p /storage/symlinks/comics

      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      resources: *default_resources
      securityContext: *default_securitycontext


calibreweb:
  enabled: false
  podLabels:
    app.elfhosted.com/name: calibre-web
  priorityClassName: tenant-normal  
  image:
    repository: ghcr.io/elfhosted/calibre-web-automated
    tag: v2.1.2@sha256:bf554d609e019db823c4d20d81cae5008ef83e2cff20838646999d771bbd4ff6
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-calibre-web"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
    DOCKER_MODS: linuxserver/mods:universal-calibre
  envFrom:
  - configMapRef:
      name: elfbot-calibre-web
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: calibre-web
      volumeSpec:
        persistentVolumeClaim:
          claimName: config     
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-calibre-web
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 8083
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: calibre-web
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # There are symlinks pre-prepared for these
        mkdir -p /config/calibre-library
        mkdir -p /config/cwa-book-ingest

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: calibre-web
      resources: *default_resources
      securityContext: *default_securitycontext        
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

cwabookdownloader:
  enabled: false
  podLabels:
    app.elfhosted.com/name: cwa-book-downloader
  priorityClassName: tenant-normal  
  image:
    repository: ghcr.io/elfhosted/cwa-downloader
    tag: rolling@sha256:621ce41bffeb89132ad7aca1e1786510204ef77e8c6aa0098e9466e5e762b609
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-cwa-book-downloader"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
  envFrom:
  - configMapRef:
      name: elfbot-cwa-book-downloader
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /cwa-book-ingest/
      subPath: calibre-web/cwa-book-ingest
      volumeSpec:
        persistentVolumeClaim:
          claimName: config     
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-cwa-book-downloader
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 8084
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: calibre-web-book-downloader
      - mountPath: /tmp
        name: tmp      
  additionalContainers:
    cloudflarebypassforscraping:
      image: ghcr.io/elfhosted/cloudflarebypassforscraping@sha256:7bdf614b57f57e47a6cecdaf6048869037123e0731456ed186b5430d6bfbefad
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

pyload:
  enabled: false
  sso:
    enabled: true
  priorityClassName:
  image:
    repository: ghcr.io/geek-cookbook/pyload-ng
    tag: 0.5.0b3.dev71@sha256:17b0414059c2aad0ae0318244a4f024f3e54851430ad6d44bedba260466c78d2
  env:
    PUID: 568
    PGID: 568
    # S6_READ_ONLY_ROOT: 1
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-pyload"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # again, s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-pyload
          optional: true
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: pyload
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: pyload
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 1m
      memory: 40Mi
    limits:
      cpu: 1
      memory: 1Gi

lazylibrarian:
  sso:
    enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/lazylibrarian
    tag: rolling@sha256:10bb730c681879d6412eb7ef121218b00a7aa2e41abe4455d5b24c96ba1d9aaf
  enabled: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-lazylibrarian"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    runAsUser: 568
    runAsGroup: 568
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: lazylibrarian
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-lazylibrarian
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 5299
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: lazylibrarian
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 1m
      memory: 96Mi
    limits:
      cpu: 1
      memory: 1Gi

mylar:
  enabled: false
  sso:
    enabled: true
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/mylar3
    tag: 0.8.0@sha256:64406246b893f7b910a4fb8f0b94e7c5ddfe58ee3a81e4982f5f6fd04ead7d6e
  env:
    PUID: 568
    PGID: 568
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mylar" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: mylar
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-mylar
          optional: true

  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8090
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: mylar
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

komga:
  enabled: false
  sso:
    enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/komga
    tag: 1.11.1@sha256:7317be399b637e7c0855a7b9e6fdc5d0ba4060a6d57a502c24528ade8af777a9
  env:
    KOMGA_CONFIGDIR: /config
    KOMGA_REMEMBERME_KEY: yesplease
    JAVA_TOOL_OPTIONS: -Xmx2g
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-komga" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: komga
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-komga
          optional: true

  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 25600
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: komga
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 1m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 2Gi

kavita:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/kavita
    tag: 0.8.4@sha256:5b61be5f506bdba6aaffaa982578b1e794ce7bb9a53bca1bd16369121b9871a4
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-kavita" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /kavita/config
      subPath: kavita
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-kavita
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: kavita
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 5000
  resources:
    requests:
      cpu: 1m
      memory: 256Mi
    limits:
      cpu: 2
      memory: 1Gi

calibre:
  enabled: false
  sso:
    enabled: true
  # runtimeClassName: kata
  image:
    repository: quay.io/linuxserver.io/calibre
    tag: 7.23.0@sha256:6236e60ea9a3df9940803f1e3dcbcddb0dc55f710671f9bc268d6a1c1143f984
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with s6
    allowPrivilegeEscalation: false # do we need this too?
    # runAsUser: 568
    # runAsGroup: 568
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-calibre"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: calibre
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-calibre
          optional: true
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
  env:
    PUID: 568
    PGID: 568
    TITLE: Calibre | ElfHosted
    START_DOCKER: false
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 8080
  resources:
    requests:
      cpu: 1m
      memory: 1Gi
    limits:
      cpu: 1
      memory: 4Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: calibre
      - mountPath: /tmp
        name: tmp
  envFrom:
  - configMapRef:
      name: elfbot-calibre
      optional: true

sonarr: &app_sonarr
  enabled: false
  podLabels:
    app.elfhosted.com/name: sonarr
    app.elfhosted.com/class: debrid
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/sonarr-develop
    tag: 4.0.12.2825@sha256:1a4f8498846c84fbccbe066ed55b792dfe3cd648fe2cffcec1bd49d7fdfb5021
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-sonarr" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: sonarr-env
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: sonarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: sonarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: sonarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-sonarr
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8989
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: sonarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/sonarr
        mkdir -p /storage/symlinks/series

      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 1m
      memory: 600Mi
    limits:
      cpu: 1.5
      memory: 2Gi

sonarr4k: &app_sonarr4k
  enabled: false
  podLabels:
    app.elfhosted.com/name: sonarr4k
    app.elfhosted.com/class: debrid
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/sonarr-develop
    tag: 4.0.12.2825@sha256:1a4f8498846c84fbccbe066ed55b792dfe3cd648fe2cffcec1bd49d7fdfb5021
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-sonarr4k" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: sonarr4k-env
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: sonarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: sonarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: sonarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-sonarr4k
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8989
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: sonarr4k
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/sonarr4k
        mkdir -p /storage/symlinks/series-4k

      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 1m
      memory: 256Mi
    limits:
      cpu: 1.5
      memory: 2Gi

resiliosync:
  service:
    main:
      enabled: false
  command:
  - rslsync
  - --config
  - /sync.conf
  - --nodaemon
  enabled: false
  sso:
    enabled: true
  priorityClassName: tenant-bulk
  image:
    repository: ghcr.io/elfhosted/resilio-sync
    tag: 3.0.0.1409-1@sha256:145e237fab29ba422f6faa081e23e953ab34f5f829b450d0acc613efcf798334
  env:
    PUID: 568
    GUID: 568
    S6_READ_ONLY_ROOT: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # another s6 containeir!
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-resiliosync"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: resiliosync
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    setup-config:
      enabled: "true"
      mountPath: "/sync.conf"
      subPath: "sync.conf"
      type: "custom"
      volumeSpec:
        configMap:
          name: resiliosync-config
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-resiliosync
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: resiliosync
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 1m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 1Gi

prowlarr: &app_prowlarr
  enabled: false
  podLabels:
    app.elfhosted.com/name: prowlarr
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/prowlarr-develop
    tag: 1.29.1.4903@sha256:2fa35c599b2ceb601471c7823ea1ba8df945356cd4543d08c8ccdc48b6c3d6d5
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-prowlarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: prowlarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: prowlarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: prowlarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-prowlarr
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9696
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: prowlarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/sh
      - -c
      - |
        set -x
        set -e
        # Set auth to external
        sed -i  "s|<AuthenticationMethod>None</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml

        # Clear out logs older than 24h
        if [ -d "/config/logs" ]; then
            # Find and delete files older than 7 days
            find "/config/logs" -type f -mtime +1 -exec rm -f {} \;
            echo "Files older than 1 day have been removed from /config/logs."
        fi

        # Get custom torrent.io definition
        mkdir -p /config/Definitions/Custom

        if [ -f /config/Definitions/Custom/elfhosted-torrentio.yml ]; then rm /config/Definitions/Custom/elfhosted-torrentio.yml; fi
        curl https://raw.githubusercontent.com/elfhosted/prowlarr-indexers/main/Custom/torrentio.yml > /config/Definitions/Custom/torrentio.yml
        curl https://raw.githubusercontent.com/elfhosted/prowlarr-indexers/main/Custom/elfcomet.yml > /config/Definitions/Custom/elfcomet.yml
        curl https://raw.githubusercontent.com/elfhosted/prowlarr-indexers/main/Custom/elfzilean.yml > /config/Definitions/Custom/elfzilean.yml
        curl https://raw.githubusercontent.com/elfhosted/prowlarr-indexers/main/Custom/torbox.yml > /config/Definitions/Custom/torbox.yml
        curl https://raw.githubusercontent.com/elfhosted/prowlarr-indexers/main/Custom/elfhosted-mediafusion.yml > /config/Definitions/Custom/elfhosted-mediafusion.yml
        # curl https://raw.githubusercontent.com/elfhosted/prowlarr-indexers/main/Custom/ygg-api.yml > /config/Definitions/Custom/ygg-api.yml
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: prowlarr
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 1Gi
  env:
    S6_READ_ONLY_ROOT: 1

lidarr:
  enabled: false
  sso:
    enabled: true
  image:
    registry: ghcr.io
    repository: elfhosted/lidarr-develop
    tag: 2.9.1.4517@sha256:736706de89074a46124634dd9183811d91d0913106f807c7170c7defe6ae8cb6
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-lidarr" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: lidarr-config
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: lidarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    s6:
      enabled: true
      type: emptyDir
      mountPath: /var/run/s6
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-lidarr
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8686
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: lidarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Set auth to external
        sed -i  "s|<AuthenticationMethod>None</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml
        # Clean up wasteful temporary mediacover storage (Radarr will just re-download these)

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: lidarr
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 1Gi

navidrome:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/navidrome
    tag: 0.54.3@sha256:25873987e248172a5e81914be06181598be2f340c34777bdf16009b9206101e4
  sso:
    enabled: true
  priorityClassName: tenant-streaming
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-navidrome"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  env:
    ND_MUSICFOLDER: /tmp
    ND_DATAFOLDER: /config
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: navidrome
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-navidrome
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: navidrome
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 4533
  resources:
    requests:
      cpu: 1m
      memory: 32Mi
    limits:
      cpu: 2
      memory: 1Gi

readarr:
  enabled: false
  sso:
    enabled: true
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/readarr-develop
    tag: 0.4.6.2711@sha256:a312c8a8b48de7777cb4ff11c65a268f2e1acaac96ccd6319a3356c494c58b59
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-readarr" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: readarr-config
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: readarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: readarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    tmp-readarr-backup:
      enabled: true
      type: emptyDir
      mountPath: /tmp/readarr_backup
      sizeLimit: 32Mi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-readarr
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8787
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: readarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Set auth to external
        sed -i  "s|<AuthenticationMethod>None</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml
        sed -i  "s|<AuthenticationMethod>Basic</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: readarr
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 20m
      memory: 256Mi
    limits:
      cpu: 2
      memory: 1Gi

readarraudio:
  enabled: false
  sso:
    enabled: true
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/readarr-develop
    tag: 0.4.6.2711@sha256:a312c8a8b48de7777cb4ff11c65a268f2e1acaac96ccd6319a3356c494c58b59
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: readarraudio-config
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-readarraudio" # Reload the deployment every time the rclones change
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: readarraudio
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: readarraudio
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    tmp-readarr-backup:
      enabled: true
      type: emptyDir
      mountPath: /tmp/readarr_backup
      sizeLimit: 32Mi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-readarraudio
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8787
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: readarraudio
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Set auth to external
        # Set auth to external
        sed -i  "s|<AuthenticationMethod>None</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml
        sed -i  "s|<AuthenticationMethod>Basic</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: readarraudio
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 20m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 1Gi

plex: &app_plex
  enabled: false
  priorityClassName: tenant-streaming
  podLabels:
    app.elfhosted.com/name: plex
    app.elfhosted.com/class: debrid
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M" # tested with _kilos in Discord on a 97Mbit remux
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    runAsUser: 568
    runAsGroup: 568    
    # fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex,elfbot-imagemaid,plex-config,imagemaid-env" # Reload the deployment every time the rclones change
  image:
    registry: ghcr.io
    repository: elfhosted/plex
    tag: 1.41.3.9314-a0bfb8370 
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    transcode: # in case users use /tmp
      enabled: true
      mountPath: /transcode
      type: emptyDir
      sizeLimit: 50Gi   
    phototranscoder:
      enabled: true
      mountPath: /phototranscoder
      type: emptyDir
      sizeLimit: 50Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex
          optional: true
    render-device: &streamer_render_device
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 32400
  envFrom:
  - configMapRef:
      name: plex-config
  - configMapRef:
      name: elfbot-plex
      optional: true
  resources:
    requests:
      cpu: "100m"
      memory: 1Gi
    limits:
      cpu: "2" # 1.5 works, but results in buffering when playback starts, see https://github.com/elfhosted/charts/issues/501
      memory: 4Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex
      - mountPath: /tmp
        name: tmp
    update-dns: *update_dns_on_init
    # repair-db:
    #   image: ghcr.io/elfhosted/plex:rolling
    #   command:
    #   - /bin/bash
    #   - -c
    #   - |
    #     set -x
    #     set -e

    #     grep -q PlexOnlineToken /config/Library/Application\ Support/Plex\ Media\ Server/Preferences.xml || (
    #       echo "Plex is not claimed yet, no point repairing" && exit 0)

    #     # If the DB directory exists, then repair
    #     if [ -d "/config/Library/Application Support/Plex Media Server/Plug-in Support/Databases" ]; then
    #       /usr/local/bin/DBRepair.sh --sqlite /usr/lib/plexmediaserver --databases '/config/Library/Application Support/Plex Media Server/Plug-in Support/Databases' auto
    #     fi
    #   volumeMounts:
    #   - mountPath: /config
    #     name: config
    #     subPath: plex
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Clean up wasteful temporary media storage (Plex will just re-download these)
        if [ -d "/config/Library/Application Support/Plex Media Server/Cache/PhotoTranscoder" ]; then
          rm -rf "/config/Library/Application Support/Plex Media Server/Cache/PhotoTranscoder"
          ln -s /phototranscoder '/config/Library/Application Support/Plex Media Server/Cache/PhotoTranscoder'
        fi
        
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: plex
      - mountPath: /phototranscoder
        name: phototranscoder
      # can't use default resources because the ephemeral limit kicks out /phototranscoder later
      # resources: *default_resources
      securityContext: *default_securitycontext 
    setup-warp:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false
  additionalContainers:
    clean-up-dns: *clean_up_dns_on_termination
    speedtest:
      image: openspeedtest/latest:latest@sha256:0d2d94087a68cf3f6e2a99b9bcc03c49a8624f144cb670f841bfa3c1570a0eb6
      env:
        CHANGE_CONTAINER_PORTS: "True"
        HTTP_PORT: "3002"
        HTTPS_PORT: "3003"
      securityContext: *speedtest_securitycontext
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:2ce9c1407bad9897f4269b3c7f53862befb52a25c712731c91ae05906d41358f
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "32400,3000,8888,3001,3002"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /web/index.html
          port: 32400
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /web/index.html
          port: 32400
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /web/index.html
          port: 32400
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10

plexranger:
  <<: *app_plex
  podLabels:
    app.elfhosted.com/name: plex
    app.elfhosted.com/class: dedicated
  podAnnotations:
    kubernetes.io/egress-bandwidth: "500M"
  enabled: false
  automountServiceAccountToken: false
  resources: *ranger_streamer_resources
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex,plex-config"

jellyfin: &app_jellyfin
  hostname: elfhosted
  image:
    repository: ghcr.io/elfhosted/jellyfin
    tag: 10.10.3@sha256:e2140c289dc8cb94d41a698abb8fb89306fb0294a6d9fb69dc744f368159facc
  enabled: false
  podLabels:
    app.elfhosted.com/class: debrid
    app.elfhosted.com/name: jellyfin
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M" # tested with _kilos in Discord on a 97Mbit remux
  priorityClassName: tenant-streaming
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    runAsUser: 568
    runAsGroup: 568    
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jellyfin" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: "/config/log/"
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs             
    transcode: # in case users use /tmp
      enabled: true
      type: custom
      mountPath: /transcode
      volumeSpec: *volumespec_ephemeral_volume_50g
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jellyfin
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jellyfin
      - mountPath: /tmp
        name: tmp
    update-dns: *update_dns_on_init
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Remove /config/transcode if it exists (could be old transcode data in there)
        if [[ -f /config/transcode ]]; then
          rm -rf /config/transcode
        fi
        if [[ -f /config/transcoding-temp ]]; then
          rm -rf /config/transcoding-temp
        fi
        if [[ -f /config/transcodes ]]; then
          rm -rf /config/transcodes
        fi

        # Make symlinks for various variations of transcode paths to /transcode
        ln -sf /transcode /config/transcodes
        ln -sf /transcode /config/transcode
        ln -sf /transcode /config/transcoding-temp

        # Also keep cache in /transcode
        mkdir -p /transcode/cache
        rm -rf /config/cache
        ln -sf /transcode/cache /config/

      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /transcode
        name: transcode
      securityContext: *default_securitycontext
      resources: *default_resources
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: false # necessary for probes
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
  resources:
    requests:
      cpu: "50m"
      memory: 1Gi
    limits:
      cpu: 2
      memory: 4Gi
  envFrom:
  - configMapRef:
      name: jellyfin-config
  additionalContainers:
    clean-up-dns: *clean_up_dns_on_termination
    speedtest:
      image: openspeedtest/latest:latest@sha256:0d2d94087a68cf3f6e2a99b9bcc03c49a8624f144cb670f841bfa3c1570a0eb6
      securityContext: *speedtest_securitycontext
    jellyfixer:
      image: quay.io/xsteadfastx/jellyfixer:latest
      env:
        JELLYFIXER_INTERNAL_URL: http://jellyfin:8096
        JELLYFIXER_EXTERNAL_URL: https://{{ .Release.Name }}-jellyfin.elfhosted.com
      

jellyfinranger:
  <<: *app_jellyfin
  podLabels:
    app.elfhosted.com/name: jellyfin
    app.elfhosted.com/class: dedicated
  podAnnotations:
    kubernetes.io/egress-bandwidth: "500M"
  enabled: false
  automountServiceAccountToken: false
  resources: *ranger_streamer_resources
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-jellyfin,elfbot-all"


emby: &app_emby
  hostname: elfhosted
  image:
    registry: ghcr.io
    repository: elfhosted/emby
    tag: 4.9.0.35@sha256:195475a235aa4a10eea1d01f50b25af074e311c93456cf3a8a43ff42e499e3c0
  enabled: false
  priorityClassName: tenant-streaming
  podLabels:
    app.elfhosted.com/class: debrid
    app.elfhosted.com/name: emby
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M" # tested with _kilos in Discord on a 97Mbit remux
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem:
    privileged: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-emby" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: "/config/log/"
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs                
    transcode: # in case users use /tmp
      enabled: true
      type: custom
      mountPath: /transcode
      volumeSpec: *volumespec_ephemeral_volume_50g
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-emby
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: emby
      - mountPath: /tmp
        name: tmp
    update-dns: *update_dns_on_init
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Remove /config/transcode if it exists (could be old transcode data in there)
        if [[ -f /config/transcode ]]; then
          rm -rf /config/transcode
        fi
        if [[ -f /config/transcoding-temp ]]; then
          rm -rf /config/transcoding-temp
        fi
        if [[ -f /config/transcodes ]]; then
          rm -rf /config/transcodes
        fi

        # Make symlinks for various variations of transcode paths to /transcode
        ln -sf /transcode /config/transcodes
        ln -sf /transcode /config/transcode
        ln -sf /transcode /config/transcoding-temp

        # Also keep cache in /transcode
        mkdir -p /transcode/cache
        rm -rf /config/cache
        ln -sf /transcode/cache /config/

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: emby
      - mountPath: /transcode
        name: transcode
      securityContext: *default_securitycontext
      resources: *default_resources
  additionalContainers:
    clean-up-dns: *clean_up_dns_on_termination
    speedtest:
      image: openspeedtest/latest:latest@sha256:0d2d94087a68cf3f6e2a99b9bcc03c49a8624f144cb670f841bfa3c1570a0eb6
      securityContext: *speedtest_securitycontext
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: false # necessary for probes
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
  resources:
    requests:
      cpu: "50m"
      memory: 1Gi
    limits:
      cpu: 2
      memory: 4Gi

embyranger:
  <<: *app_emby
  podLabels:
    app.elfhosted.com/name: emby
    app.elfhosted.com/class: dedicated
  podAnnotations:
    kubernetes.io/egress-bandwidth: "500M"
  enabled: false
  automountServiceAccountToken: false
  resources: *ranger_streamer_resources
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-emby,elfbot-all"

homer:
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    runAsNonRoot: false
    runAsUser: 568
    runAsGroup: 568
  podSecurityContext:
    runAsNonRoot: false
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "Always"
  automountServiceAccountToken: false
  image:
    repository: ghcr.io/elfhosted/tooling
    tag: focal-20230605@sha256:6088a1e9fc0ce83aec9910af0899661c23b5f2025428d7da631b9b9390241b6c
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podLabels:
    app.elfhosted.com/role: nodefinder # let this be an anchor for replicationdestinations
  persistence:
    <<: *storagemounts
    logs:
      enabled: true
      type: custom
      mountPath: /logs
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    config:
      enabled: true
      type: custom
      mountPath: /config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config          
    backup:
      enabled: true
      type: custom
      mountPath: /backup
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    config-yml:
      enabled: "true"
      subPath: "config.yml"
      type: "custom"
      volumeSpec:
        configMap:
          name: homer-config
    custom-css:
      enabled: "true"
      subPath: "custom-css"
      type: "custom"
      volumeSpec:
        configMap:
          name: homer-config
    gatus-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: gatus-config
    disk-usage:
      enabled: "true"
      mountPath: "/usr/local/bin/disk_usage.sh"
      subPath: "disk_usage.sh"
      type: "custom"
      volumeSpec:
        configMap:
          name: homer-config
    message:
      enabled: true
      type: emptyDir
      mountPath: /www/assets/message
  command:
  - /bin/bash
  - /usr/local/bin/disk_usage.sh
  additionalContainers:
    ui:
      image: ghcr.io/elfhosted/homer:v24.12.1@sha256:ce1588afcacf9db7f16c3b241c0f2be7c7dd5eae5a09fb2a140001af26ac292f
      imagePullPolicy: IfNotPresent
      volumeMounts:
      - mountPath: /www/assets/config.yml
        name: config-yml
        subPath: "config.yml"
      - mountPath: /www/assets/custom.css
        name: custom-css
        subPath: "custom.css"
      - mountPath: /www/assets/message
        name: message
      - mountPath: /www/assets/backgrounds
        name: config
        subPath: homer/backgrounds
        readOnly: true
      resources: *default_resources
      securityContext: *default_securitycontext
  configmap:
    config:
      # -- Store homer configuration as a ConfigMap, but don't specify any config, since we'll supply our own
      enabled: false
  controller:
    replicas: 1
    strategy: RollingUpdate
    rollingUpdate:
      unavailable: 1
    annotations:
      configmap.reloader.stakater.com/reload: "homer-config, elfbot-homer" # Reload the deployment every time the yaml config changes
  resources:
    requests:
      cpu: 1m
      memory: 1Mi
    limits:
      cpu: 200m
      memory: 1Gi

traefikforwardauth:
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  whitelist: admin@elfhosted.com
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  controller:
    replicas: 1
    annotations:
      configmap.reloader.stakater.com/reload: traefik-forward-auth-config
    strategy: RollingUpdate
  image:
    repository: ghcr.io/elfhosted/traefik-forward-auth
    pullPolicy: IfNotPresent
    tag: 3.1.0@sha256:19cd990fae90c544100676bc049f944becc8c454639e57d20f6f48e27de90776

  middleware:
    # middleware.enabled -- Enable to deploy a preconfigured middleware
    enabled: false

  envFrom:
  - configMapRef:
      name: traefik-forward-auth-config

  ingress:
    main:
      enabled: false

  service:
    main:
      enabled: true # necessary for probes

  resources:
    requests:
      cpu: 1m
      memory: 6Mi
    limits:
      cpu: 1
      memory: 32Mi

gatus:
  image:
    repository: ghcr.io/elfhosted/gatus
    tag: 5.15.0@sha256:934e43f97e3504a74c5e5e605d29e538e30061f38f9896cac137ddc4023c2386
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 20Mi
    limits:
      cpu: 1
      memory: 128Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  controller:
    # strategy: RollingUpdate
    annotations:
      configmap.reloader.stakater.com/reload: "gatus-config"
  env:
    GATUS_CONFIG_PATH: /config/config.yaml
    SMTP_FROM: 'health@elfhosted.com'
    SMTP_PORT: 587
  persistence:
    gatus-config:
      enabled: "true"
      mountPath: /config
      type: "custom"
      volumeSpec:
        configMap:
          name: gatus-config
    config:
      enabled: true
      type: custom
      mountPath: /data/
      subPath: gatus
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  envFrom:
  - secretRef:
      name: gatus-smtp-config
  configmap:
    config:
      # -- Store homer configuration as a ConfigMap, but don't specify any config, since we'll supply our own
      enabled: false

gotify:
  sso:
    enabled: true
  enabled: false
  image:
    repository: ghcr.io/elfhosted/gotify
    tag: 2.5.0@sha256:f5c89bb3ccbf857bca816e4550b46c442cfb6c0eae0f081975ba5c5099779c3f
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-gotify"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  env:
    GOTIFY_SERVER_PORT: 8080
  resources:
    requests:
      cpu: 1m
      memory: 32Mi
    limits:
      cpu: 1
      memory: 64Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/data/
      subPath: gotify
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-gotify
          optional: true
    tmp: *tmp # Avoids issues with readOnlyRootFilesystem
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: gotify
      - mountPath: /tmp
        name: tmp
    bootstrap: *bootstrap

flaresolverr: &app_flaresolverr
  enabled: false
  podLabels:
    app.elfhosted.com/name: flaresolverr
  image:
    registry: ghcr.io
    repository: elfhosted/flaresolverr
    tag: 3.3.21@sha256:2c3c7087eaf809f2b032fb9df28cf8884546463a1c532c9dd3b244f424bfd6ad
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # makes node unhappy
  affinity:
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.elfhosted.com/name
              operator: In
              values:
              - zurg
          topologyKey: "kubernetes.io/hostname"
      - weight: 50
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - kubernetesdashboard
          topologyKey: "kubernetes.io/hostname"
  tolerations:
  - key: node-role.elfhosted.com/dedicated
    operator: Exists
  - key: node-role.elfhosted.com/hobbit
    operator: Exists
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-flaresolverr"
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-flaresolverr
          optional: true
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 1000
    runAsGroup: 1000
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 600m
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8191
  # env:
  #   WAIT_FOR_VPN: "true"
  #   LOG_LEVEL: debug
    # DRIVER: nodriver
  initContainers:
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false
  addons:
    vpn:
      enabled: false # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:2ce9c1407bad9897f4269b3c7f53862befb52a25c712731c91ae05906d41358f
      env:
        IPTABLES_BACKEND: nft
        KILLSWITCH: "true"
        LOCAL_NETWORK: 10.0.0.0/8
        NFTABLES: "1"
        VPNDNS: "0"
        # HTTP_CONTROL_SERVER_PORT: "8000"
        # HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "8191"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared

seafile:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/seafile
    tag: 10.0.1@sha256:9513eb378e72adc5d91b30ec7a0c45860246ed2d49d0f531dbfc585b822bb4cd
  priorityClassName: tenant-bulk
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't seem to work with seafile, no output from container either
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 568 # has to run as root, see https://github.com/haiwen/seafile-docker/issues/86
    # runAsGroup: 568
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-seafile"
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 1024m
      memory: 512Mi
  env:
    # -- Set the container timezone
    TIME_ZONE: Etc/UTC
    # -- The hostname of your database
    DB_HOST: "{{ .Release.Name }}-seafile-mysql"
    # -- The root password for mysql (used for initial setup)
    DB_ROOT_PASSWD: wLu5UUuT@3Zu33eT
    # -- The initial admin user's password
    SEAFILE_ADMIN_PASSWORD: changeme
    # -- The hostname for the server (set to your ingress hostname)
    SEAFILE_SERVER_HOSTNAME: "{{ .Release.Name }}-seafile.elfhosted.com"
    SEAFILE_SERVER_LETSENCRYPT: false
    FORCE_HTTPS_IN_CONF: true
    NON_ROOT: true # yes, and with our custom image, this runs the seafile/seahub components as user 568
  envFrom:
  - configMapRef:
      name: seafile-config
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # debug whether this gets us probes
  memcached:
    nameOverride: seafile-memcached
    enabled: true
  mysql:
    nameOverride: seafile-mysql
    enabled: true
    architecture: standalone
    commonAnnotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-seafile"

    auth:
      rootPassword: "wLu5UUuT@3Zu33eT"
      database: "seafile"
      username: "seafile"
      password: "nXCXSmqU4TMk3okD"

    primary:
      readinessProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      livenessProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      startupProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      persistence:
        enabled: true
        existingClaim: config
        subPath: seafile/database
      resources:
        requests:
          cpu: 5m
          memory: 1Gi
        limits:
          cpu: 2
          memory: 1024Mi
      containerSecurityContext:
        enabled: true
        seccompProfile:
          type: RuntimeDefault
        runAsUser: 568
        runAsGroup: 568
      podSecurityContext:
        enabled: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
      extraVolumeMounts:
      - mountPath: /opt/bitnami/mysql/tmp/
        name: tmp
      extraVolumes:
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi
      - name: backup-database-script
        configMap:
          name: seafile-backup

  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /shared/seafile
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
      subPath: seafile/data
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-seafile
          optional: true

tunarr:
  enabled: false
  image:
    registry: ghcr.io
    repository: chrisbenincasa/tunarr
    tag: 0.16.13-vaapi
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    privileged: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-tunarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podAnnotations:
    kubernetes.io/egress-bandwidth: "128M"  
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 256Mi
    limits:
      cpu: 1
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /root/.local
      subPath: tunarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-tunarr
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    media: # in case users use /tmp
      enabled: true
      mountPath: /streams
      type: emptyDir
      sizeLimit: 50Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: tunarr
      - mountPath: /tmp
        name: tmp

ersatztv:
  enabled: false
  image:
    registry: docker.io
    repository: jasongdove/ersatztv
    tag: develop-vaapi@sha256:a5c14d00550725335d578193b52b9a184369fbc9452ff1e50bef438f8c9ccb68
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    privileged: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-erzatztv"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 256Mi
    limits:
      cpu: 1
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8409
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /root/.local/share/ersatztv
      subPath: ersatztv
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    transcode:
      enabled: true
      type: custom
      mountPath: /root/.local/share/etv-transcode
      volumeSpec: *volumespec_ephemeral_volume_50g          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-tunarr
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: ersatztv
      - mountPath: /tmp
        name: tmp

threadfin:
  enabled: false
  sso:
    enabled: true
  image:
    registry: ghcr.io
    repository: elfhosted/threadfin
    tag: 1.2.20@sha256:e734c699830d2f44ec4b9726db9bb0adc83e97233ff8fb6257736767867376f1
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-threadfin"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 256Mi
    limits:
      cpu: 1
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 34400
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /home/threadfin/conf/
      subPath: threadfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-tunarr
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: threadfin
      - mountPath: /tmp
        name: tmp

thelounge:
  enabled: false
  sso:
    enabled: true
  image:
    repository: ghcr.io/elfhosted/thelounge
    tag: "4.4.3@sha256:74ae8d9fc36d5a8396bb70cfaa58d222730fa69d2f71c3e2ec3ae010f2a0b264"
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-thelounge"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because the node modules in /app try to create files
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  env:
    TZ: UTC
    THELOUNGE_HOME: /config/thelounge # avoids attempts to chown /config
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 40Mi
    limits:
      cpu: 100m
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9000
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: thelounge
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-thelounge
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: thelounge
      - mountPath: /tmp
        name: tmp
    create-user:
      image: ghcr.io/elfhosted/thelounge:4.4.3@sha256:74ae8d9fc36d5a8396bb70cfaa58d222730fa69d2f71c3e2ec3ae010f2a0b264
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e


        # If we don't already have a config, create one
        if [ ! -f /config/thelounge/config.json ];
        then
          mkdir -p /config/thelounge
          cp /config-bootstrap/* /config/thelounge/ -R
        fi

        # If we don't already have a user, create one
        if [ ! -f /config/thelounge/users/${USERNAME}.json ];
        then
          thelounge add ${USERNAME} --password ${PASSWORD}
        fi
      volumeMounts:
      - mountPath: /config
        subPath: thelounge
        name: config
      env:
      - name: THELOUNGE_HOME
        value: /config/thelounge # avoids attempts to chown /config
      - name: USERNAME
        valueFrom:
          configMapKeyRef:
            name: elfhosted-user-config
            key: USERNAME
      - name: PASSWORD
        value: ireadthedocs
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true

overseerr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/overseerr
    tag: 1.33.2@sha256:619c38f2d53750433fcac10c794ca2a937e08be50d06bc50dc5fba5cc472f0cc
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: overseerr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-overseerr,overseerr-config"
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  sso:
    enabled: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: overseerr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-overseerr
          optional: true
    tmp: *tmp
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
    overseerr-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: overseerr-config
          optional: true          
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: overseerr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        # run the setup script from the configmap, so that we can make templated changes
        bash /bootstrap/setup.sh
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: overseerr
      - name: overseerr-config
        mountPath: "/bootstrap/"
    update-dns: *update_dns_on_init
    # We do this so that we can override the /app/jellyseer/public path with our own, allowing the user to customize the branding
    copy-branding:
      image: ghcr.io/elfhosted/overseerr:1.33.2@sha256:619c38f2d53750433fcac10c794ca2a937e08be50d06bc50dc5fba5cc472f0cc
      command:
        - /bin/bash
        - -c
        - |
          mkdir -p /config/branding
          cp --no-clobber -rf /app/overseerr/public/logo_* /config/branding
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: overseerr
      resources: *default_resources
      securityContext: *default_securitycontext
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5055
  resources:
    requests:
      cpu: 1m
      memory: 175Mi
    limits:
      cpu: 2
      memory: 1Gi
  additionalContainers:
    branding:
      image: nginxinc/nginx-unprivileged
      volumeMounts:
      - mountPath: /usr/share/nginx/html
        name: config
        subPath: overseerr/branding
        readOnly: true
      - mountPath: /tmp
        name: tmp
      resources: *default_resources
      securityContext: *default_securitycontext
    clean-up-dns: *clean_up_dns_on_termination

jellyseerr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/jellyseerr
    tag: 2.2.3@sha256:934db49b03db58e7ad5edf7f9f998b6e12fcd2d1b540199983f7f9830d659f4b
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: jellyseerr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jellyseerr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  sso:
    enabled: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: jellyseerr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jellyseerr
          optional: true
    tmp: *tmp
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  envFrom:
  - configMapRef:
      name: jellyseerr-env
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jellyseerr
      - mountPath: /tmp
        name: tmp
    update-dns: *update_dns_on_init
    # We do this so that we can override the /app/jellyseer/public path with our own, allowing the user to customize the branding
    # copy-branding:
    #   image: ghcr.io/elfhosted/jellyseerr:2.0.1@sha256:ee475414b42e17152f67f75169137ece981b6912466182132dc3d207eebee27c
    #   command:
    #     - /bin/bash
    #     - -c
    #     - |
    #       mkdir -p /config/branding
    #       cp --no-clobber -rf /app/overseerr/public/logo_* /config/branding/
    #   volumeMounts:
    #   - mountPath: /config
    #     name: config
    #     subPath: jellyseerr
    #   resources: *default_resources
    #   securityContext: *default_securitycontext
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5055
  resources:
    requests:
      cpu: 1m
      memory: 160Mi
    limits:
      cpu: 2
      memory: 1Gi
  additionalContainers:
    branding:
      image: nginxinc/nginx-unprivileged
      volumeMounts:
      - mountPath: /usr/share/nginx/html
        name: config
        subPath: jellyseerr/branding
        readOnly: true
      - mountPath: /tmp
        name: tmp
      resources: *default_resources
      securityContext: *default_securitycontext
    clean-up-dns: *clean_up_dns_on_termination

jellystat:
  enabled: false
  podLabels:
    app.elfhosted.com/name: jellystat
  image:
    repository: ghcr.io/elfhosted/jellystat
    tag: 1.1.2@sha256:47d823878d750003a2dacd48a6820d3200bcf110815f4cbf7a6daee499eb9eae
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jellystat,jellystat-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with ilikedanger currently
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: jellystat-env
  - configMapRef:
      name: elfbot-jellystat
      optional: true
  resources:
    requests:
      cpu: 1m
      memory: 20Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: jellystat/config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jellystat
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jellystat
      - mountPath: /tmp
        name: tmp
    setup-postgres:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/database
        chown elfie:elfie /config/database -R

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: jellystat
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault    
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      env:
        - name: POSTGRES_PASSWORD
          value: jellystat
        - name: POSTGRES_DB
          value: jellystat
        - name: POSTGRES_USER
          value: jellystat
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: jellystat/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 1m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 1Gi

audiobookshelf:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/audiobookshelf
    tag: 2.12.3@sha256:b8356000ad913a01d672fd8882474a92908c668f272749f7349cf7333148cc9c
  priorityClassName: tenant-streaming
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: audiobookshelf
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-audiobookshelf,elfbot-all"
  sso:
    enabled: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: audiobookshelf
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    # never used, just satisfies startup scripts
    metadata:
      enabled: true
      type: emptyDir
      mountPath: /metadata
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-audiobookshelf
          optional: true
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  env:
    METADATA_PATH: /config/metadata
    SOURCE: ElfHosted
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: audiobookshelf
      - mountPath: /tmp
        name: tmp
    update-dns: *update_dns_on_init
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 2
      memory: 1Gi
  additionalContainers:
    clean-up-dns: *clean_up_dns_on_termination

openbooks:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/openbooks
    tag: 4.5.0@sha256:ddcee6e8be21a03d87208c113a43922df251bdf7ce9c43665f045145c0e5fa15
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  persistence:
    <<: *storagemounts
  command:
  - /bin/bash
  - -c
  - |
    set -x
    set -e
    sleep 5s
    USER=$(tr -dc A-Za-z0-9 </dev/urandom | head -c 13 ; echo '')
    ./openbooks server \
      --dir ${DATA_DIR-/tmp} \
      --port 8000 \
      --name $USER \
      --tls=false \
      --persist \
      --server irc.irchighway.net:6661 \
      --no-browser-downloads \
      --debug
  envFrom:
  - configMapRef:
      name: elfbot-openbooks
      optional: true
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-openbooks"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 2
      memory: 1Gi

vaultwarden:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/vaultwarden
    tag: 1.32.7@sha256:36b2bf5ee51b71ea625b9fc817409cd633f8d1db1f6173399bcab13fb9d860aa
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-vaultwarden"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: elfbot-vaultwarden
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: vaultwarden
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-vaultwarden
          optional: true
    tmp: *tmp
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: vaultwarden
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 1m
      memory: 16Mi
    limits:
      cpu: 1
      memory: 1Gi


notifiarr:
  enabled: false
  sso:
    enabled: true
  hostname: elfhosted
  image:
    repository: ghcr.io/elfhosted/notifiarr
    tag: 0.8.3@sha256:6995e6db3e4f91c12d5a6c94fc03dd4154fb0bb5c2c17729b10dad3badb3150a
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because the node modules in /app try to create files
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-notifiarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 16Mi
    limits:
      cpu: 2
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5454
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: notifiarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: notifiarr-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-notifiarr
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: notifiarr
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [ ! -f /config/notifiarr.conf ];
        then
          cp /bootstrap/notifiarr.conf /config/
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: notifiarr
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true

shoko:
  enabled: false
  sso:
    enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/shokoserver
    tag: v5.0.0@sha256:85ad89084fd24a03586d1f79b3e5ea5da0f2554778bdf04e3bec8a18504ace6d
  env:
    PUID: 568
    PGID: 568
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-shoko"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # again, s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /home/shoko/.shoko/
      subPath: shoko
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-shoko
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: shoko
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8111
  resources:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi

filebot:
  enabled: false
  sso:
    enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/filebot-node
    tag: 0.4.7@sha256:439e3604543fd4ce970dacc8ced8c15a3bba8902886b54eeffdd82721ecbf06d
  env:
    PUID: 568
    PGID: 568
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-filebot"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # again, s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: filebot
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-filebot
          optional: true

    tmp: # to avoid errors about storing java prefs
      enabled: true
      type: emptyDir
      mountPath: /home/seedy
      sizeLimit: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: filebot
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5452
  resources:
    requests:
      cpu: 1m
      memory: 16Mi
    limits:
      cpu: 2
      memory: 1Gi

rpdb:
  enabled: false
  sso:
    enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rpdb
    tag: 0.2.7@sha256:9a4378c84aea8fa77c97c10f88484ff8fb30cbaf4fe7589e30e58ca1a1a31bcc
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,storage-changed,elfbot-rpdb"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /.config
      subPath: rpdb
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rpdb
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rpdb
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8750
  resources:
    requests:
      cpu: 1m
      memory: 40Mi
    limits:
      cpu: 1
      memory: 1Gi

kometa: &app_kometa
  enabled: false
  sso:
    enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/kometa
    tag: v2.1.0@sha256:598cea4b0d31c383e1bfee9f473991cbcf5f03ef175e13a6472bfb12f842802b
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: kometa
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-kometa"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: elfbot-kometa
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: kometa
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs/
      subPath: kometa
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-kometa
          optional: true
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: kometa-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: kometa
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [ ! -f /config/config.yml ];
        then
          cp /bootstrap/config.yml /config/
        fi

        # Create directories we need by default
        mkdir -p /config/kometa/assets
        mkdir -p /config/kometa/logs
        mkdir -p /config/kometa/metadata
        mkdir -p /config/kometa/missing
        mkdir -p /config/kometa/overlays

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: kometa
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 4Gi

imagemaid:
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/imagemaid
    tag: v1.1.1@sha256:1ac456479c1dc40f4da0e604a619f84f847442bab5adb2720b057fbbdaa041dc
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: imagemaid
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-imagemaid"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: imagemaid-env
  - configMapRef:
      name: elfbot-imagemaid
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-imagemaid
          optional: true
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: kometa-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: imagemaid
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 4Gi  

cinesync:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/cinesync
    tag: v2.1@sha256:91afa6a1bff0face609a7b924e8a024775b2ed23aade67dadc4619b3d95263b7
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: cinesync
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-cinesync,cinesync-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: cinesync-env
  - configMapRef:
      name: elfbot-cinesync
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/db
      subPath: cinesync
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-cinesync
          optional: true  
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: cinesync
      - mountPath: /tmp
        name: tmp     
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 2Gi 

seerrbridge:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/seerrbridge-dev
    tag: rolling@sha256:0a4ddd8fc765700ab707963ec523bb52318d82879ea540c0ee90cd2c83ca261b
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # needs to write to /app/seerrbridge.log, apparently
  podLabels:
    app.elfhosted.com/name: cinesync
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-seerrbridge,seerrbridge-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory  
    tmp:
      enabled: true
      type: emptyDir
      mountPath: /tmp
      sizeLimit: 1Gi  
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: seerrbridge-env
  - configMapRef:
      name: elfbot-seerrbridge
      optional: true
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 2Gi  
  initContainers:      
    setup-warp:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false    
  addons:
    vpn:
      enabled: false # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.39.1@sha256:47688e70bd1519bcedaf48270328d85a5405496330787e53371d23fa590af4d3
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "3001,8777"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared

plextraktsync:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/plextraktsync
    tag: 0.32.7@sha256:8009c1d6cf0bb1463a8aaccad1f83203149f4b8f9a099534fdb6141ef9c4360d
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: plextraktsync
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plextraktsync"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: elfbot-plextraktsync
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /home/elfie/.config/PlexTraktSync
      subPath: plextraktsync
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-kometa
          optional: true
    state: # plextraktsync needs this
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mountPath: /home/elfie/.local/state
    cache: # plextraktsync needs this
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mountPath: /home/elfie/.cache      
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plextraktsync
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 100m # no way this should be using so much resources
      memory: 1Gi

decluttarr: &app_decluttarr
  enabled: false
  sso:
    enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/decluttarr
    tag: rolling@sha256:e7e5ecdc7fdb0f174a257217a108c2535f6f684e5e84d996335bf0fb8b9f6945
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: decluttarr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-decluttarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: elfbot-decluttarr
      optional: true
  - configMapRef:
      name: zenv-decluttarr
      optional: true      
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi

rdebridui:
  enabled: false
  sso:
    enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rdebrid-ui
    tag: rolling@sha256:2c9885f1918097d8b762e0e60fcee5e12f147c10d79f13decbe4f3b73d24e4ad
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: rdebrid-ui
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rdebrid-ui"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  envFrom:
  - configMapRef:
      name: elfbot-rdebrid-ui
      optional: true  
  - configMapRef:
      name: zenv-rdebrid-ui
  - secretRef:
      name: zenv-rdebrid-ui      
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi

suggestarr:
  enabled: false
  sso:
    enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/suggestarr
    tag: v1.0.15@sha256:d765222e0c6060dc7616f3c6a3b67cc922d900706ecb967edaf297fd12b5a2a4
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with app :(
  podLabels:
    app.elfhosted.com/name: suggestarr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-suggestarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /app/config/
      subPath: suggestarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /var/log
      subPath: suggestarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-suggestarr
          optional: true          
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5000
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: suggestarr
      - mountPath: /tmp
        name: tmp      

decluttarr4k: 
  <<: *app_decluttarr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-decluttarr4k"  
  envFrom:
  - configMapRef:
      name: elfbot-decluttarr4k
      optional: true
  - configMapRef:
      name: zenv-decluttarr4k
      optional: true      


rcloneui:
  enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.68.2@sha256:7f0485bd06cb3153825f85cfd0ff9f747b205ebca05d3e6767c4f44fc4d0d626
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rcloneui,elfhosted-user-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    privileged: true
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    backup: *backup
    cache: 
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      # volumeSpec: *volumespec_ephemeral_volume_10g    
    mount:
      enabled: true
      type: emptyDir
      mountPath: /mount
      sizeLimit: 1Gi 
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: rclone
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    rclone-remote-storage:
      enabled: "true"
      subPath: "rclone-remote-storage"
      type: "custom"
      volumeSpec:
        configMap:
          name: rclonefm-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rclonebrowser
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rclone
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # add local remote if it doesn't exist
        grep -q '/storage' /config/rclone.conf || cat /rclone-remote-storage >> /config/rclone.conf

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: rclone
      - mountPath: /tmp # need this for cating into a file
        name: tmp
      - mountPath: /rclone-remote-storage
        subPath: rclone-remote-storage
        name: rclone-remote-storage
      resources: *default_resources
      securityContext: *default_securitycontext
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5572
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 1m
      memory: 60Mi
    limits:
      cpu: 1
      memory: 512Mi

rclonefm:
  enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.68.2@sha256:7f0485bd06cb3153825f85cfd0ff9f747b205ebca05d3e6767c4f44fc4d0d626
  command:
  - /rclonefm.sh
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rclonefm,rclonefm-config"
  podAnnotations:
    kubernetes.io/egress-bandwidth: "40M"
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    cache:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      sizeLimit: 1Gi
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: rclone
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    rclonefm-config:
      enabled: "true"
      mountPath: /var/lib/rclonefm/js/settings.js
      subPath: "settings.js"
      type: "custom"
      volumeSpec:
        configMap:
          name: rclonefm-config
    rclone-remote-storage:
      enabled: "true"
      subPath: "rclone-remote-storage"
      type: "custom"
      volumeSpec:
        configMap:
          name: rclonefm-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rclonefm
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rclone
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # add local remote if it doesn't exist
        grep -q '/storage' /config/rclone.conf || cat /rclone-remote-storage >> /config/rclone.conf

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: rclone
      - mountPath: /tmp # need this for cating into a file
        name: tmp
      - mountPath: /rclone-remote-storage
        subPath: rclone-remote-storage
        name: rclone-remote-storage
      resources: *default_resources
      securityContext: *default_securitycontext
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5573
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 1m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi

webdav: &webdav
  enabled:
    false
  podLabels:
    app.elfhosted.com/name: webdav
  podAnnotations:
    kubernetes.io/egress-bandwidth: "40M"
  priorityClassName: tenant-normal
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.68.2@sha256:7f0485bd06cb3153825f85cfd0ff9f747b205ebca05d3e6767c4f44fc4d0d626
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-webdav-plus,elfbot-webdav"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  command:
  - /webdav.sh
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: webdav-config
  - configMapRef:
      name: elfbot-webdav
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /storage/config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-webdav-plus
          optional: true
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  initContainers:
    update-dns: *update_dns_on_init
  additionalContainers:
    clean-up-dns: *clean_up_dns_on_termination
  service:
    main:
      enabled: false # necessary for probes
      ports:
        http:
          port: 5574
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 1m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi

storagehub:
  enabled: false # down for now
  podLabels:
    app.elfhosted.com/name: storagehub
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "storagehub-scripts,storagehub-env"
      secret.reloader.stakater.com/reload: ",storagehub-config,storagehub-env"
  # affinity:
  #   podAffinity:
  #     # prefer to be located with zurg, if tolerations permit
  #     preferredDuringSchedulingIgnoredDuringExecution:
  #     - weight: 100
  #       podAffinityTerm:
  #         labelSelector:
  #           matchExpressions:
  #           - key: app.elfhosted.com/name
  #             operator: In
  #             values:
  #             - zurg
  #         topologyKey: "kubernetes.io/hostname"
  # tolerations:
  # - key: node-role.elfhosted.com/download-only
  #   operator: Exists
  # - key: node-role.elfhosted.com/dedicated
  #   operator: Exists
  priorityClassName: tenant-critical
  image:
    repository: itsthenetwork/nfs-server-alpine
    tag: latest
  env:
    SHARED_DIRECTORY: /export
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    privileged: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    # # this is an ephemeral volume
    # storage:
    #   enabled: true
    #   type: custom
    #   mountPath: /storage
    #   volumeSpec: *volumespec_ephemeral_volume_50g
    # these are the persistent volumes we support currently
    # needed for the migration

    rclonemountrealdebridzurg: *rclonemountrealdebridzurg
    # this is the old symlinks on HDD
    # symlinks: *symlinks # these only get mounted on storagehub. Everything else accesses symlinks **through** storagehub
    # samba config
    # config:
    #   enabled: "true"
    #   subPath: "container.json"
    #   mountPath: /etc/samba/container.json
    #   type: "custom"
    #   volumeSpec:
    #     secret:
    #       secretName: storagehub-config
    # storagehub-scripts:
    #   enabled: "true"
    #   type: "custom"
    #   volumeSpec:
    #     configMap:
    #       name: storagehub-scripts
    #       defaultMode: 0755
    tmp:
      enabled: true
      type: emptyDir
      mountPath: /tmp
      sizeLimit: 1Gi
  service:
    main:
      enabled: false # necessary for probes
      ports:
        http:
          port: 2049
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 1m
      memory: 10Mi
    limits:
      cpu: 1
      memory: 512Mi
  # envFrom:
  # - configMapRef:
  #     name: storagehub-env
  # - secretRef:
  #     name: storagehub-env
  # initContainers:
    # setup:
    #   image: *tooling_image
    #   imagePullPolicy: IfNotPresent
    #   command:
    #   - /bin/bash
    #   - -c
    #   - |
    #     # don't error on failure
    #     # not doing this yet
    #     # /storagehub-scripts/restore.sh

    #     echo nothing
    #     # if [[ ! -f /storage/symlinks/i-was-migrated-to-storagehub ]]
    #     # then
    #     #   if [[ ! -z "$(ls -A /migration)" ]]
    #     #   then
    #     #     echo "Tar-migrating from /migration/..."
    #     #     tar -cf - -C /migration/ . | tar xvmf - -C /storage/symlinks/
    #     #     touch /storage/symlinks/i-was-migrated-to-storagehub
    #     #   fi
    #     # fi

    #   envFrom:
    #   - configMapRef:
    #       name: storagehub-env
    #   - configMapRef:
    #       name: elfhosted-user-config
    #   - secretRef:
    #       name: storagehub-env
    #   volumeMounts:
    #   - mountPath: /storagehub-scripts
    #     name: storagehub-scripts
    #   - mountPath: /home/elfie
    #     name: tmp
    #   - mountPath: /migration
    #     name: migration
    #   - mountPath: /storage/symlinks
    #     name: symlinks
    #   resources: *default_resources
    #   securityContext: *default_securitycontext
  # additionalContainers:
  #   backup-on-termination:
  #     image: *tooling_image
  #     command:
  #     - /usr/bin/dumb-init
  #     - /bin/bash
  #     - -c
  #     - /storagehub-scripts/backup.sh
  #     envFrom:
  #     - configMapRef:
  #         name: storagehub-env
  #     - configMapRef:
  #         name: elfhosted-user-config
  #     - secretRef:
  #         name: storagehub-env
  #     volumeMounts:
  #     - mountPath: /storagehub-scripts
  #       name: storagehub-scripts
  #     - mountPath: /home/elfie
  #       name: tmp
  #     - mountPath: /ephemeral
  #       name: ephemeral
  #     # need a folder here for each app. what a pita
  #     - mountPath: /persistent/plex
  #       name: persistent-plex
  #     - mountPath: /symlinks
  #       name: symlinks
  # terminationGracePeriodSeconds: "3600" # take up to an hour to backup

webdavplus:
  enabled: false
  <<: *webdav
  podLabels:
    app.elfhosted.com/name: webdav-plus
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M"
  envFrom:
  - configMapRef:
      name: webdav-plus-config

jfa:
  enabled: false
  sso:
    enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/jfa-go
    tag: v0.5.1@sha256:7f1313de026821bffdffe9bc8bf522b392052c421321105868b0eab7f58bb883
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jfa"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: jfa
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jfa
          optional: true
    tmp:
      enabled: true
      type: emptyDir
      mountPath: /tmp
      sizeLimit: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jfa
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8056
  resources:
    requests:
      cpu: 1m
      memory: 150Mi
    limits:
      cpu: 2
      memory: 1Gi

mattermost:
  enabled: false
  # Default values for mattermost-team-edition.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.
  image:
    repository: mattermost/mattermost-team-edition
    tag: 10.3.1@sha256:5892ec2bdb79d74adafd2ef689ac4ea585370646a8028cc6da249ed2715ae6fa
    imagePullPolicy: IfNotPresent
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mattermost"

  initContainerImage:
    repository: appropriate/curl
    tag: latest
    imagePullPolicy: IfNotPresent

  extraInitContainers: []

  ## Deployment Strategy
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  deploymentStrategy:
    type: Recreate
    rollingUpdate: null

  ## How many old ReplicaSets for Mattermost Deployment you want to retain
  revisionHistoryLimit: 1

  ## Enable persistence using Persistent Volume Claims
  ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
  ## ref: https://docs.gitlab.com/ee/install/requirements.html#storage
  ##
  persistence:
    ## This volume persists generated data from users, like images, attachments...
    ##
    data:
      enabled: true
      size: 10Gi
      ## If defined, volume.beta.kubernetes.io/storage-class: <storageClass>
      ## Default: volume.alpha.kubernetes.io/storage-class: default
      ##
      # storageClass:
      accessMode: ReadWriteOnce
      existingClaim: "config"
      subPath: mattermost/data
    plugins:
      enabled: false # these just end up under data anyway

  service:
    type: ClusterIP
    externalPort: 8065
    internalPort: 8065
    annotations: {}
    # loadBalancerIP:
    loadBalancerSourceRanges: []

  ingress:
    enabled: false
    path: /
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # certmanager.k8s.io/issuer:  your-issuer
      # nginx.ingress.kubernetes.io/proxy-body-size: 50m
      # nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
      # nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
      # nginx.ingress.kubernetes.io/proxy-buffering: "on"
      # nginx.ingress.kubernetes.io/configuration-snippet: |
      #   proxy_cache mattermost_cache;
      #   proxy_cache_revalidate on;
      #   proxy_cache_min_uses 2;
      #   proxy_cache_use_stale timeout;
      #   proxy_cache_lock on;
      #### To use the nginx cache you will need to set an http-snippet in the ingress-nginx configmap
      #### http-snippet: |
      ####     proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=mattermost_cache:10m max_size=3g inactive=120m use_temp_path=off;
    hosts:
      - mattermost.example.com
    tls:
      # - secretName: mattermost.example.com-tls
      #   hosts:
      #     - mattermost.example.com

  route:
    enabled: false

  ## If use this please disable the mysql chart by setting mysql.enable to false
  externalDB:
    enabled: true

    ## postgres or mysql
    externalDriverType: "mysql"

    ## postgres:  "<USERNAME>:<PASSWORD>@<HOST>:5432/<DATABASE_NAME>?sslmode=disable&connect_timeout=10"
    ## mysql:     "<USERNAME>:<PASSWORD>@tcp(<HOST>:3306)/<DATABASE_NAME>?charset=utf8mb4,utf8&readTimeout=30s&writeTimeout=30s"
    externalConnectionString: "mattermost:IUzI1NiJ9.eyJhdWQiOiIwMDk1MTkyYjhjZWIyYjVhNDQwMT@tcp(mattermost-mysql:3306)/mattermost?charset=utf8mb4,utf8&readTimeout=30s&writeTimeout=30s"

  mysql:
    nameOverride: mattermost-mysql
    enabled: true
    architecture: standalone
    # nameOverride: mattermost-mariadb

    commonAnnotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mattermost"

    auth:
      rootPassword: "3uAaJYGJLR3d2qbBM2FsSThJ"
      database: "mattermost"
      username: "mattermost"
      password: "IUzI1NiJ9.eyJhdWQiOiIwMDk1MTkyYjhjZWIyYjVhNDQwMT"

    primary:
      readinessProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      livenessProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      startupProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      persistence:
        enabled: true
        existingClaim: config
        subPath: mattermost/database
      resources:
        requests:
          cpu: 5m
          memory: 512Mi
        limits:
          cpu: 2
          memory: 1024Mi
      containerSecurityContext:
        enabled: true
        seccompProfile:
          type: RuntimeDefault
        runAsUser: 568
        runAsGroup: 568
      podSecurityContext:
        enabled: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
      extraVolumeMounts:
      - mountPath: /opt/bitnami/mysql/tmp/
        name: tmp
      extraVolumes:
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi
      - name: backup-database-script
        configMap:
          name: mattermost-backup
      - name: confighdd
        persistentVolumeClaim:
          claimName: config
          subPath: mattermost/database
      sidecars:
        - name: backup-database
          image: *tooling_image
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: 3uAaJYGJLR3d2qbBM2FsSThJ
            - name: MYSQL_DATABASE
              value: mattermost
          command:
          - /usr/bin/dumb-init
          - /bin/bash
          - -c
          - |

            sleep 2m # give mysql time to start up
            while true
            do
              now=$(date +"%s_%Y-%m-%d")
              /usr/bin/mysqldump --opt -h mattermost-mysql -u root -p${MYSQL_ROOT_PASSWORD} ${MYSQL_DATABASE} > "/backup/${now}_${MYSQL_DATABASE}.sql"
              sleep 1d
            done

  ## Additional pod annotations
  extraPodAnnotations: {}

  ## Additional env vars
  extraEnvVars: []
    # This is an example of extra env vars when using with the deployment with GitLab Helm Charts
    # - name: POSTGRES_PASSWORD_GITLAB
    #   valueFrom:
    #     secretKeyRef:
    #       # NOTE: Needs to be manually created
    #       # kubectl create secret generic gitlab-postgresql-password --namespace <NAMESPACE> --from-literal postgres-password=<PASSWORD>
    #       name: gitlab-postgresql-password
    #       key: postgres-password
    # - name: POSTGRES_USER_GITLAB
    #   value: gitlab
    # - name: POSTGRES_HOST_GITLAB
    #   value: gitlab-postgresql
    # - name: POSTGRES_PORT_GITLAB
    #   value: "5432"
    # - name: POSTGRES_DB_NAME_MATTERMOST
    #   value: mm5
    # - name: MM_SQLSETTINGS_DRIVERNAME
    #   value: "postgres"
    # - name: MM_SQLSETTINGS_DATASOURCE
    #   value: postgres://$(POSTGRES_USER_GITLAB):$(POSTGRES_PASSWORD_GITLAB)@$(POSTGRES_HOST_GITLAB):$(POSTGRES_PORT_GITLAB)/$(POSTGRES_DB_NAME_MATTERMOST)?sslmode=disable&connect_timeout=10

  ## Additional init containers
  # extraInitContainers: []
    # This is an example of extra Init Container when using with the deployment with GitLab Helm Charts
    # - name: bootstrap-database
    #   image: "postgres:9.6-alpine"
    #   imagePullPolicy: IfNotPresent
    #   env:
    #     - name: POSTGRES_PASSWORD_GITLAB
    #       valueFrom:
    #         secretKeyRef:
    #           name: gitlab-postgresql-password
    #           key: postgres-password
    #     - name: POSTGRES_USER_GITLAB
    #       value: gitlab
    #     - name: POSTGRES_HOST_GITLAB
    #       value: gitlab-postgresql
    #     - name: POSTGRES_PORT_GITLAB
    #       value: "5432"
    #     - name: POSTGRES_DB_NAME_MATTERMOST
    #       value: mm5
    #   command:
    #     - sh
    #     - "-c"
    #     - |
    #       if PGPASSWORD=$POSTGRES_PASSWORD_GITLAB psql -h $POSTGRES_HOST_GITLAB -p $POSTGRES_PORT_GITLAB -U $POSTGRES_USER_GITLAB -lqt | cut -d \| -f 1 | grep -qw $POSTGRES_DB_NAME_MATTERMOST; then
    #       echo "database already exist, exiting initContainer"
    #       exit 0
    #       else
    #       echo "Database does not exist. creating...."
    #       PGPASSWORD=$POSTGRES_PASSWORD_GITLAB createdb -h $POSTGRES_HOST_GITLAB -p $POSTGRES_PORT_GITLAB -U $POSTGRES_USER_GITLAB $POSTGRES_DB_NAME_MATTERMOST
    #       echo "Done"
    #       fi

  # Add additional volumes and mounts, for example to add SAML keys in the app or other files the app server may need to access
  extraVolumes: []
    # - hostPath:
    #     path: /var/log
    #   name: varlog
  extraVolumeMounts:
    - name: mattermost-data
      mountPath: mattermost/mattermost/logs
      subPath: mattermost/logs
      readOnly: true

  ## Node selector
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
  nodeSelector: {}

  ## Affinity
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}

  ## Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  ## Pod Security Context
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  securityContext:
    fsGroup: 568
    runAsGroup: 568
    runAsUser: 568
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL

  serviceAccount:
    create: false
    name:
    annotations: {}

  ## Configuration
  ## The config here will be injected as environment variables in the deployment
  ## Please refer to https://docs.mattermost.com/administration/config-settings.html#configuration-in-database for more information
  ## You can add any config here, but need to respect the format: MM_<GROUPSECTION>_<SETTING>. ie: MM_SERVICESETTINGS_ENABLECOMMANDS: false
  config:
    MM_PLUGINSETTINGS_CLIENTDIRECTORY: "./client/plugins"

syncthing:
  enabled: false
  hostname: elfhosted
  sso:
    enabled: true
  priorityClassName: tenant-bulk
  image:
    repository: ghcr.io/elfhosted/syncthing
    tag: 1.28.0@sha256:ab5fb7a23c0605d87a11d34cf8792d53523fab3c68c926a0bb5f3f4856838c54
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-syncthing"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: syncthing
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-syncthing
          optional: true

  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8384
  resources:
    requests:
      cpu: 1m
      memory: 70Mi
    limits:
      cpu: 1
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: syncthing
      - mountPath: /tmp
        name: tmp
    setup:
      image: ghcr.io/elfhosted/syncthing:1.28.0@sha256:ab5fb7a23c0605d87a11d34cf8792d53523fab3c68c926a0bb5f3f4856838c54
      imagePullPolicy: IfNotPresent
      envFrom:
      - configMapRef:
          name: elfhosted-user-config
      command:
      - /bin/ash
      - -c
      - |
        set -x
        set -e

        # Generate a new config if necessary
        if [ ! -f /config/config.xml ]
        then
          # We are generating a new config
          syncthing generate --config=/config
        fi

        # Apply the port every time (incase the user changes it and reboots)
        # sed -i  "s/<listenAddress>tcp.*/<listenAddress>tcp:\/\/0.0.0.0:${PORT_SYNCTHING}<\/listenAddress>/" /config/config.xml
        # sed -i  "s/<listenAddress>quic.*/<listenAddress>quic:\/\/0.0.0.0:${PORT_SYNCTHING}<\/listenAddress>/" /config/config.xml

        # # And if it's defaulted...
        # sed -i  "s/<<listenAddress>default<\/listenAddress>/<listenAddress>tcp:\/\/0.0.0.0:${PORT_SYNCTHING}<\/listenAddress>\n\t<listenAddress>quic:\/\/0.0.0.0:${PORT_SYNCTHING}<\/listenAddress>/" /config/config.xml

        # Ignore the fact that we have no password set
        # grep '<insecureAdminAccess>true</insecureAdminAccess>' /config/config.xml || sed -i  "s/<\/gui>/<insecureAdminAccess>true<\/insecureAdminAccess>\n\t<\/gui>/" /config/config.xml

        # Avoid foolish use of capital letters in default sync folder
        # sed -i  "s/\/storage\/elfstorage\/Sync/\/storage\/elfstorage\/syncthing/" /config/config.xml

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: syncthing
      resources: *default_resources
      securityContext: *default_securitycontext

rdtclient: &app_rdtclient
  enabled: false
  hostname: elfhosted
  sso:
    enabled: true
  priorityClassName: tenant-bulk
  podLabels:
    app.elfhosted.com/class: debrid
  image:
    repository: ghcr.io/elfhosted/rdtclient
    tag: v2.0.94@sha256:f202c19afee891829c697fea8cc59824974dada53f7fa496f089543c982a1d75
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rdtclient"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data/db
      subPath: rdtclient
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rdtclient
          optional: true
    download: # in case users use /tmp
      enabled: true
      type: custom
      mountPath: /data/downloads
      volumeSpec: *volumespec_ephemeral_volume_1g
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6500
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
      ephemeral-storage: 50Mi
    limits:
      cpu: 100m
      memory: 2Gi
      ephemeral-storage: 100Mi # a safety net against node ephemeral space exhaustion
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rdtclient
      - mountPath: /tmp
        name: tmp

rdtclientpremiumize:
  enabled: false
  <<: *app_rdtclient
  podLabels:
    app.elfhosted.com/name: rdtclient-premiumize
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rdtclient-premiumize"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data/db
      subPath: rdtclient-premiumize
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rdtclient-premiumize
          optional: true
    download: # in case users use /tmp
      enabled: true
      type: custom
      mountPath: /data/downloads
      volumeSpec: *volumespec_ephemeral_volume_1g  
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rdtclient-premiumize
      - mountPath: /tmp
        name: tmp

rdtclienttorbox:
  enabled: false
  <<: *app_rdtclient
  podLabels:
    app.elfhosted.com/name: rdtclient-torbox
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rdtclient-torbox"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data/db
      subPath: rdtclient-torbox
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rdtclient-torbox
          optional: true
    download: # in case users use /tmp
      enabled: true
      type: custom
      mountPath: /data/downloads
      volumeSpec: *volumespec_ephemeral_volume_1g  
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rdtclient-torbox
      - mountPath: /tmp
        name: tmp

# RDTClient for AllDebrid
rdtclientalldebrid:
  enabled: false
  <<: *app_rdtclient
  podLabels:
    app.elfhosted.com/name: rdtclient-alldebrid
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rdtclient-alldebrid"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data/db
      subPath: rdtclient-alldebrid
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rdtclient-alldebrid
          optional: true
    download: # in case users use /tmp
      enabled: true
      type: custom
      mountPath: /data/downloads
      volumeSpec: *volumespec_ephemeral_volume_1g  
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rdtclient-alldebrid
      - mountPath: /tmp
        name: tmp      
  addons:
    vpn:
      enabled: true
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:2ce9c1407bad9897f4269b3c7f53862befb52a25c712731c91ae05906d41358f
      envFrom:
      - configMapRef:
          name: gluetun-config
      env:
        DOT: "off"
        FIREWALL_INPUT_PORTS: "6500"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

jdownloader:
  enabled: false
  hostname: elfhosted
  # runtimeClassName: kata
  priorityClassName: tenant-bulk
  image:
    repository: jlesage/jdownloader-2
    tag: v24.12.1
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jdownloader"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 568
    # runAsGroup: 568
    fsGroup: 568 # need this so that the bootstrap can run
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    JDOWNLOADER_HEADLESS: 1
    APP_NICENESS: 19
  envFrom:
  - configMapRef:
      name: jdownloader-config
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
      mountPath: /output
      subPath: jdownloader/downloads/completed/jdownloader
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jdownloader
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9898
  resources:
    requests:
      cpu: 1m
      memory: 10Mi
    limits:
      cpu: 0.5
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jdownloader
      - mountPath: /tmp
        name: tmp
  additionalContainers:
    podinfo:
      image: stefanprodan/podinfo # used to run probes from gatus

miniflux:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/miniflux
    tag: 2.2.4
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-miniflux,miniflux-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 512Mi
    limits:
      cpu: 1500m # if par threads is 1, this leaves 0.5cpu for downloading
      memory: 1Gi
  envFrom:
  - configMapRef:
      name: miniflux-config
  postgresql:
    enabled: true
    nameOverride: miniflux-postgresql
    commonAnnotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-miniflux"
    auth:
      username: miniflux
      password: miniflux
      database: miniflux
      postgresPassword: miniflux
    primary:
      affinity: *standard_affinity
      tolerations: *standard_tolerations
      persistence:
        enabled: true
        existingClaim: config
        subPath: miniflux/database
      resources:
        requests:
          cpu: 5m
          memory: 128Mi
        limits:
          cpu: 2
          memory: 1024Mi
      containerSecurityContext:
        enabled: true
        seccompProfile:
          type: RuntimeDefault
        runAsUser: 568
        runAsGroup: 568
      podSecurityContext:
        enabled: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
      extraVolumeMounts:
      - mountPath: /opt/bitnami/postgresql/conf/
        name: conf
      - mountPath: /opt/bitnami/postgresql/tmp/
        name: tmp
      extraVolumes:
      - name: conf
        emptyDir:
          sizeLimit: 1Gi
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi
      sidecars:
        - name: backup-database
          image: *tooling_image
          env:
            - name: POSTGRES_PASSWORD
              value: miniflux
            - name: POSTGRES_DATABASE
              value: miniflux
            - name: POSTGRES_USER
              value: miniflux
          command:
          - /usr/bin/dumb-init
          - /bin/bash
          - -c
          - |

            set +e # for debug
            sleep 2m # give postgres time to start up
            while true
            do
              now=$(date +"%s_%Y-%m-%d")
              PGPASSWORD=$POSTGRES_PASSWORD pg_dump -U $POSTGRES_USER -h localhost -d $POSTGRES_DATABASE -F c -f /backup/${now}_${POSTGRES_DATABASE}.psql
              sleep 1d
            done

joplinserver:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/geek-cookbook/joplin-server
    tag: v2.14.2@sha256:b4f52bffce08541dd54e823b78bbfa18e091d53064ed507f69fe9e1ca92b719b
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-joplinserver,joplinserver-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # breaks migrations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 512Mi
    limits:
      cpu: 1500m # if par threads is 1, this leaves 0.5cpu for downloading
      memory: 1Gi
  envFrom:
  - configMapRef:
      name: joplinserver-config
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: joplinserver/data
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  postgresql:
    enabled: true
    nameOverride: joplinserver-postgresql
    commonAnnotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-joplinserver"
    auth:
      username: joplinserver
      password: joplinserver
      database: joplinserver
      postgresPassword: joplinserver
    primary:
      affinity: *standard_affinity
      tolerations: *standard_tolerations
      persistence:
        enabled: true
        existingClaim: config
        subPath: joplinserver/database
      resources:
        requests:
          cpu: 5m
          memory: 128Mi
        limits:
          cpu: 2
          memory: 1024Mi
      containerSecurityContext:
        enabled: true
        seccompProfile:
          type: RuntimeDefault
        runAsUser: 568
        runAsGroup: 568
      podSecurityContext:
        enabled: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
      extraVolumeMounts:
      - mountPath: /opt/bitnami/postgresql/conf/
        name: conf
      - mountPath: /opt/bitnami/postgresql/tmp/
        name: tmp
      extraVolumes:
      - name: conf
        emptyDir:
          sizeLimit: 1Gi
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi
      sidecars:
        - name: backup-database
          image: *tooling_image
          env:
            - name: POSTGRES_PASSWORD
              value: joplinserver
            - name: POSTGRES_DATABASE
              value: joplinserver
            - name: POSTGRES_USER
              value: joplin
          command:
          - /usr/bin/dumb-init
          - /bin/bash
          - -c
          - |

            set +e # for debug
            sleep 2m # give postgres time to start up
            while true
            do
              now=$(date +"%s_%Y-%m-%d")
              PGPASSWORD=$POSTGRES_PASSWORD pg_dump -U $POSTGRES_USER -h localhost -d $POSTGRES_DATABASE -F c -f /backup/${now}_${POSTGRES_DATABASE}.psql
              sleep 1d
            done

homepage:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/gethomepage/homepage
    tag: v0.10.9
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-homepage,homepage-config,homepage-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
    privileged: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 568
    # runAsGroup: 568
    fsGroup: 568 # need this so that the bootstrap can run
    fsGroupChangePolicy: "OnRootMismatch"
  serviceAccount:
    create: true
    name: homepage
  automountServiceAccountToken: true
  env:
    PUID: 568
    PGID: 568
  envFrom:
  - configMapRef:
      name: elfbot-homepage
      optional: true
  - configMapRef:
      name: homepage-env
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/config
      subPath: homepage
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    config-default:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: homepage-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-homepage
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 250m # deliberately hobble the CPU in favor of GPU transcoding
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: homepage
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /app/config/user-change-these/
        touch /app/config/user-change-these/JELLYFIN_KEY
        touch /app/config/user-change-these/PLEX_KEY
        touch /app/config/user-change-these/EMBY_KEY
        touch /app/config/user-change-these/NAVIDROME_USER
        touch /app/config/user-change-these/NAVIDROME_TOKEN
        touch /app/config/user-change-these/NAVIDROME_SALT
        touch /app/config/user-change-these/CALIBREWEB_USERNAME
        touch /app/config/user-change-these/CALIBREWEB_PASSWORD
        touch /app/config/user-change-these/KOMGA_USERNAME
        touch /app/config/user-change-these/KOMGA_PASSWORD
        touch /app/config/user-change-these/KAVITA_USERNAME
        touch /app/config/user-change-these/KAVITA_PASSWORD
        touch /app/config/user-change-these/AUDIOBOOKSHELF_KEY
        touch /app/config/user-change-these/OMBI_KEY
        touch /app/config/user-change-these/OVERSEERR_KEY
        touch /app/config/user-change-these/JELLYSEERR_KEY
        touch /app/config/user-change-these/TAUTULLI_KEY
        touch /app/config/user-change-these/tunarr_USERNAME
        touch /app/config/user-change-these/tunarr_PASSWORD
        touch /app/config/user-change-these/MINIFLUX_KEY
        touch /app/config/user-change-these/UPTIMEKUMA_SLUG
        touch /app/config/user-change-these/GOTIFY_KEY

        # If we don't already have an example config, create one
        if [ ! -f /app/config/dont-overwrite-me ];
        then
          cp /bootstrap/* /app/config/
        fi
      volumeMounts:
      - mountPath: /app/config
        name: config
        subPath: homepage
      - name: config-default
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true


wallabag:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: stefanprodan/podinfo
    tag: latest
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-wallabag,wallabag-config"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
    privileged: false
  # runtimeClassName: kata
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568 # for the mounted volumes
  persistence:
    config:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 1m
      memory: 32Mi
    limits:
      cpu: 100m
      memory: 1Gi
  additionalContainers:
    ui:
      image: ghcr.io/elfhosted/wallabag:2.6.10@sha256:baec0c2d03568cd21c34682a6bc5c53a9961783717f0999d8b1b2d80fc492b8c
      volumeMounts:
      - mountPath: /var/www/wallabag/data
        name: config
        subPath: wallabag/data
      - mountPath: /var/www/wallabag/images
        name: config
        subPath: wallabag/images
      envFrom:
      - configMapRef:
          name: elfbot-wallbag
          optional: true
      - configMapRef:
          name: wallabag-config
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        allowPrivilegeEscalation: false
      resources:
        requests:
          cpu: 1m
          memory: 100Mi
        limits:
          cpu: 500m
          memory: 200Mi

autoscan:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/autoscan
    tag: 1.4.0@sha256:36db88c1cc082a5ae483955a20cb3fb8b78299d2b1e972ed44bc9313cc7ed786
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-autoscan"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because the node modules in /app try to create files
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
     AUTOSCAN_VERBOSITY: 1
  resources:
    requests:
      cpu: 1m
      memory: 16Mi
    limits:
      cpu: 100m
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3030
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: autoscan
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: autoscan-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-autoscan
          optional: true

  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: autoscan
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [[ ! -f /config/config.yml ]];
        then
          cp /bootstrap/config.yml /config/
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: autoscan
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true


riven: &app_riven
  enabled: false
  podLabels:
    app.elfhosted.com/name: riven
  image:
    repository: ghcr.io/elfhosted/riven
    tag: v0.20.1@sha256:229f1da453b23e9ef560ec2071e95d9d376eba17c5e76b80175ba46fe8d40096
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-riven,riven-env,riven-setup"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with ilikedanger currently
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: riven-env
  - configMapRef:
      name: elfbot-riven
      optional: true
  resources:
    requests:
      cpu: 1m
      memory: 20Mi
    limits:
      cpu: 2
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /riven/data
      subPath: riven
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /riven/data/logs
      subPath: riven
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-riven
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory
    setup:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: riven-setup
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: riven
      - mountPath: /tmp
        name: tmp
    setup-postgres:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # make symlink directories if they don't exist, to make onboarding simpler
        # wipe DB entirely (a requirement of 0.17)
        if grep -q 16 "/config/postgresql/database/PG_VERSION" 2>/dev/null; then
          rm -rf "/config/postgresql"
        fi
        mkdir -p /config/postgresql/database
        mkdir -p /config/postgresql/backups
        chown elfie:elfie /config/postgresql -R

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: riven
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        # run the setup script from the configmap, so that we can make templated changes
        bash /setup/setup.sh
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      - mountPath: /config
        name: config
        subPath: riven
      - name: setup
        mountPath: "/setup/"        
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      env:
        - name: POSTGRES_PASSWORD
          value: postgres
        - name: POSTGRES_DB
          value: riven
        - name: POSTGRES_USER
          value: postgres
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: riven/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 1m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi

rivenvpn: 
  enabled: false
  <<: *app_riven
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-riven,riven-env,riven-setup,gluetun-config"  
  addons:
    vpn:
      enabled: true
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:2ce9c1407bad9897f4269b3c7f53862befb52a25c712731c91ae05906d41358f
      envFrom:
      - configMapRef:
          name: gluetun-config
      env:
        DOT: "off"
        FIREWALL_INPUT_PORTS: "3001,8080" # 3001 is ttyd, 8080 is the backend
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"        
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

rivenfrontend: &app_rivenfrontend
  podLabels:
    app.elfhosted.com/name: riven-frontend
  image:
    repository: ghcr.io/elfhosted/riven-frontend
    tag: v0.17.0@sha256:f8543deadc70e6386662a7feeb833398269eb513bf91c229e0a47be7394f7bea
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rivenfrontend,riven-frontend-env,riven-frontend-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with ilikedanger currently
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    config:
      enabled: "true"
      type: "custom"
      mountPath: /riven/config
      volumeSpec:
        configMap:
          name: riven-frontend-config          
  envFrom:
  - configMapRef:
      name: riven-frontend-env
  - configMapRef:
      name: elfbot-rivenfrontend
      optional: true
  resources:
    requests:
      cpu: 1m
      memory: 20Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000

# So that we can toggle it on with rivenvpn
rivenfrontendvpn:
  enabled: false
  <<: *app_rivenfrontend

airdcpp: &app_airdcpp
  enabled: false
  image:
    repository: ghcr.io/geek-cookbook/airdcpp
    tag: 2.9.0@sha256:d9f6e597bcfc38946d0c4cafce775a559e7b8cf7c66397c9c506cb695ea01205
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: airdcpp
  podAnnotations:
    kubernetes.io/egress-bandwidth: "100M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-airdcpp"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 16Mi
    limits:
      cpu: 2
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5600
  env:
    WAIT_FOR_VPN: "true"
    PORT_FILE: /.airdcpp/forwarded-port
  probes:
    liveness:
      enabled: false
    startup:
      enabled: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /.airdcpp/
      subPath: airdcpp
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-airdcpp
          optional: true
  initContainers:
    bootstrap: *bootstrap
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      envFrom:
      - configMapRef:
          name: airdcpp-pia-config
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus

airdcpppia:
  enabled: false
  <<: *app_airdcpp

airdcppgluetun:
  enabled: false
  <<: *app_airdcpp
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:2ce9c1407bad9897f4269b3c7f53862befb52a25c712731c91ae05906d41358f
      envFrom:
      - configMapRef:
          name: airdcpp-gluetun-config
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus

jackett:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/jackett
    tag: 0.22.1211@sha256:2d4ccedbcedbf7034ae8ca04ec85313f9587b6576fc3e71e3f3bbe4e2ac7a67d
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jackett"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9117
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: jackett
      volumeSpec:
        persistentVolumeClaim:
          claimName: config        
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jackett
          optional: true
  initContainers:
    bootstrap: *bootstrap

stremioserver: &app_stremioserver
  enabled: false
  image:
    repository: ghcr.io/elfhosted/stremio-server
    tag: v4.20.8@sha256:53d851cdd0ce35ecd1bfe0bff9ad123b806255e03ff3ba133bde12ed75c57188
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: stremio-server
  podAnnotations:
    kubernetes.io/egress-bandwidth: "100M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-stremio-server,stremio-server-env,stremio-server-config"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # review
    privileged: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    runAsUser: 568 # review
    runAsGroup: 568
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 250m
      memory: 1Gi
    limits:
      cpu: 500m
      memory: 6Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 11470
  envFrom:
  - configMapRef:
      name: stremio-server-env
  persistence:
    tmp: *tmp
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    config:
      enabled: "true"
      subPath: "server-settings.json"
      mountPath: /config/server-settings.json
      type: "custom"
      volumeSpec:
        configMap:
          name: stremio-server-config
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
    transcode: # in case users use /tmp
      enabled: true
      type: custom
      mountPath: /transcode
      volumeSpec: *volumespec_ephemeral_volume_10g
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
  initContainers:
    update-dns: *update_dns_on_init
  additionalContainers:
    clean-up-dns: *clean_up_dns_on_termination
    # Avoid 404s on the /casting endpoint
    casting:
      image: nginxinc/nginx-unprivileged
      volumeMounts:
      - mountPath: /usr/share/nginx/html/casting.json
        name: config
        subPath: casting.json
        readOnly: true
      - mountPath: /tmp
        name: tmp
      resources: *default_resources
      securityContext: *default_securitycontext

# Stremioserver with PIA
stremioserverpia:
  <<: *app_stremioserver
  enabled: false
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      envFrom:
      - configMapRef:
          name: stremioserver-pia-config
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus

# Stremioserver with Gluetun
stremioservergluetun:
  <<: *app_stremioserver
  enabled: false
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:2ce9c1407bad9897f4269b3c7f53862befb52a25c712731c91ae05906d41358f
      envFrom:
      - configMapRef:
          name: stremioserver-gluetun-config
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus

flixio:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/stremio-web
    tag: rolling@sha256:1e5ce4931af36e61071e27fabae0bfeffd18da1f7a6c936714ad50c05a0a5da4
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: flixio
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-flixio"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with iprom's patching trick
    privileged: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: flixio-env
  - configMapRef:
      name: elfbot-flixio
      optional: true  
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080

flixioapi:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/flixio-api
    tag: rolling@sha256:54c2e3adac9580e0549203348fb0886d9f031952439ca6ddba0ebfeb27153e0a
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: flixio-api
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-flixio-api"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    runAsUser: 568
    runAsGroup: 568
  automountServiceAccountToken: false
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-flixio-api
          optional: true
    config:
      enabled: true
      type: custom
      mountPath: /app/data
      subPath: flixio-api
      volumeSpec:
        persistentVolumeClaim:
          claimName: config   
  envFrom:
  - configMapRef:
      name: flixio-api-env
  - configMapRef:
      name: elfbot-flixio-api
      optional: true           
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080

stremiojackett:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/stremio-jackett
    tag: v4.2.2@sha256:a496777c70fbb1aa36da9b4f9a8b16059607007e6bee7d3cc5f7d98fb74fb38c
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-stremio-jackett"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: stremio-jackett-env
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  persistence:
    pm2:
      enabled: true
      type: emptyDir
      mountPath: /.pm2
      sizeLimit: 1Gi
    npm:
      enabled: true
      type: emptyDir
      mountPath: /.npm
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-stremio-jackett
          optional: true

pairdrop:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/pairdrop
    tag: v1.10.10@sha256:c0188414adebb07c69a170c81256825f7ea83784f2f7ff491679fa0165b4425f
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: pairdrop
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-pairdrop"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  # envFrom:
  # - configMapRef:
  #     name: pairdrop-env
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-pairdrop
          optional: true

actual:
  enabled: false
  image:
    repository: ghcr.io/actualbudget/actual-server
    tag: 24.12.0-alpine
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: actual
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-actual"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5006
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-actual
          optional: true
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: actual
      volumeSpec:
        persistentVolumeClaim:
          claimName: config          

petio:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/petio
    tag: v0.5.5@sha256:a28b7ffb5b1b04a8ad798112604c410a9a09f8882e9bf98b9f24e8f41571f505
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-petio"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 7777
  persistence:
    backup: *backup
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-petio
          optional: true
    config:
      enabled: true
      type: custom
      mountPath: /app/api/config/
      subPath: petio/config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    tmp: *tmp
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: petio
      - mountPath: /tmp
        name: tmp
  additionalContainers:
    mongodb:
      image: mongodb/mongodb-community-server:8.0.4-ubi8
      volumeMounts:
        - name: config
          subPath: petio/mongodb
          mountPath: /data/db/
        - name: tmp
          mountPath: /tmp
      securityContext: *default_securitycontext

pgadmin:
  enabled: false
  image:
    repository: dpage/pgadmin4
    tag: "8.14"
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-pgadmin"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    capabilities:
      add:
      - NET_BIND_SERVICE
      drop:
      - ALL
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: pgadmin-env
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 80
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: pgadmin
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    tmp: *tmp

redisinsight:
  enabled: false
  image:
    repository: redislabs/redisinsight
    tag: v2@sha256:7fef8b7ecf2e8597037f906fc69863345dd846d36577210569396f7917333355
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-redisinsight"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    capabilities:
      add:
      - IPC_LOCK
      drop:
      - ALL
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5540
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: redisinsight
      volumeSpec:
        persistentVolumeClaim:
          claimName: config

mongoexpress:
  enabled: false
  image:
    repository: mongo-express
    tag: 1.0.2-18
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mongoexpress"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8081
  envFrom:
  - configMapRef:
      name: elfbot-mongoexpress
      optional: true

comet:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/comet
    tag: v1.51.0@sha256:40842893e73c000a10eb1fb304b05c07d5ae11b5ee26432e0c8155785c934a07
  priorityClassName: tenant-normal
  podAnnotations:
    kubernetes.io/egress-bandwidth: "64M"
  podLabels:
      app.elfhosted.com/name: comet
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-comet,comet-env"
      secret.reloader.stakater.com/reload: "comet-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568    
  initContainers:
    update-dns: *update_dns_on_init
    setup-warp:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false    
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
    config:
      enabled: true
      type: custom
      mountPath: /app/data
      subPath: comet
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    cache:
      enabled: true
      type: emptyDir
      mountPath: /.cache
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-comet
          optional: true
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory          
  resources:
    requests:
      cpu: 1m
      memory: 512Mi
    limits:
      cpu: 2
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  envFrom:
  - configMapRef:
      name: comet-env
  - secretRef:
      name: comet-env
  - configMapRef:
      name: elfbot-comet
      optional: true
  additionalContainers:
    clean-up-dns: *clean_up_dns_on_termination
    # don't need this at present
    # warp:
    #   image: ghcr.io/elfhosted/warp:rolling@sha256:4e89fc07ff24c30ffc7f3804d7f3738d634c7b9aa4fecb4941618a366e43d8c9
    #   securityContext:
    #     runAsUser: 1000
    #     runAsGroup: 1000
    #     privileged: false
    #     capabilities:
    #       add:
    #       - NET_ADMIN
    #   envFrom:
    #   - secretRef:
    #       name: comet-env
    #   volumeMounts:
    #   - mountPath: /data
    #     subPath: comet/warp
    #     name: config
    speedtest:
      image: openspeedtest/latest:latest@sha256:0d2d94087a68cf3f6e2a99b9bcc03c49a8624f144cb670f841bfa3c1570a0eb6
      env:
        CHANGE_CONTAINER_PORTS: "True"
        HTTP_PORT: "3002"
        HTTPS_PORT: "3003"        
      securityContext: *speedtest_securitycontext
  addons:
    vpn:
      enabled: false # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:2ce9c1407bad9897f4269b3c7f53862befb52a25c712731c91ae05906d41358f
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8001"
        HTTP_CONTROL_SERVER_ADDRESS: ":8001"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "8000"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared      

jackettio:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/jackettio
    tag: v1.7.0@sha256:cd575cf575be731701d2d8b0c2b1a85acf4d9678fbd8e045ea6c50fba0a02d49
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jackettio,jackettio-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: jackettio
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory              
  resources:
    requests:
      cpu: 1m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 4000
  envFrom:
  - configMapRef:
      name: jackettio-env
  - configMapRef:
      name: elfbot-jackettio
      optional: true
  initContainers:      
    setup-warp:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false    
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.39.1@sha256:47688e70bd1519bcedaf48270328d85a5405496330787e53371d23fa590af4d3
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "4000"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared

stremthru:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/stremthru
    tag: 0.28.1@sha256:dc922f619cd1946a410add3be80df481c034278317977c4e84e22ec37484dbd4
  priorityClassName: tenant-normal
  podAnnotations:
    kubernetes.io/egress-bandwidth: "64M"  
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-stremthru,stremthru-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: stremthru
      volumeSpec:
        persistentVolumeClaim:
          claimName: config      
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755           
    # shared:
    #   enabled: true
    #   mountPath: /shared
    #   type: emptyDir
    #   volumeSpec:
    #     medium: Memory                    
  resources:
    requests:
      cpu: 1m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  envFrom:
  - configMapRef:
      name: stremthru-env
  - configMapRef:
      name: elfbot-stremthru
      optional: true
  - secretRef:
      name: stremthru-env      
  # initContainers:      
  #   setup-warp:
  #     image: *tooling_image
  #     imagePullPolicy: IfNotPresent
  #     command:
  #     - /bin/bash
  #     - -c
  #     - |
  #       set -x

  #       cd /shared

  #       # Create cloudflare account
  #       wgcf register --accept-tos

  #       # Create gluetun config
  #       wgcf generate -p /shared/wg0.conf

  #       # grab the values from the profile and put them into env vars for gluetun to consume
  #       echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
  #       echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
  #       echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

  #       echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

  #     volumeMounts:
  #     - mountPath: /shared
  #       name: shared
  #     securityContext:
  #       seccompProfile:
  #         type: RuntimeDefault
  #       readOnlyRootFilesystem: false    
  # addons:
  #   vpn:
  #     enabled: false
  #     gluetun:
  #       image:
  #         repository: ghcr.io/elfhosted/gluetun
  #         tag: 3.40.0@sha256:2ce9c1407bad9897f4269b3c7f53862befb52a25c712731c91ae05906d41358f
  #     env:
  #       FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
  #       DNS_KEEP_NAMESERVER: "on"
  #       HTTP_CONTROL_SERVER_PORT: "8000"
  #       HTTP_CONTROL_SERVER_ADDRESS: ":8000"
  #       VPN_TYPE: wireguard
  #       VPN_SERVICE_PROVIDER: custom
  #       FIREWALL_INPUT_PORTS: "8080"
  #       WIREGUARD_MTU: "1280"
  #       VPN_ENDPOINT_PORT: "2408"
  #       DOT: "off"
  #     securityContext:
  #       runAsUser: 0
  #       capabilities:
  #         add:
  #           - NET_ADMIN
  #           - SYS_MODULE
  #     config: # We have to set this to null so that we can override with our own config
  #     volumeMounts:
  #     - mountPath: /shared
  #       name: shared
  initContainers:
    update-dns: *update_dns_on_init
  additionalContainers:
    clean-up-dns: *clean_up_dns_on_termination
    speedtest:
      image: openspeedtest/latest:latest@sha256:0d2d94087a68cf3f6e2a99b9bcc03c49a8624f144cb670f841bfa3c1570a0eb6
      env:
        CHANGE_CONTAINER_PORTS: "True"
        HTTP_PORT: "3002"
        HTTPS_PORT: "3003"    
      securityContext: *speedtest_securitycontext  

davio:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/davio
    tag: v1.0.4@sha256:2cb9c20abe0e29a956f4a6ec4cdb214bce31d744777c8795148d610f6b4a7008
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-davio,davio-env"
      secret.reloader.stakater.com/reload: "davio-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config
      volumeSpec: *volumespec_ephemeral_volume_1g
  resources:
    requests:
      cpu: 1m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 4000
  envFrom:
  - configMapRef:
      name: davio-env
  - secretRef:
      name: davio-env
  - configMapRef:
      name: elfbot-davio
      optional: true

mediafusion:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/mediafusion
    tag: 4.3.5@sha256:bd5ca87407f1fa677f811ccb1f32b6434919ec1793a1797fb0639be658cd26ec
  podLabels:
      app.elfhosted.com/name: mediafusion
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mediafusion,mediafusion-env"
      secret.reloader.stakater.com/reload: "mediafusion-env,mediafusion-vpn"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    tinyproxy-conf:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tinyproxy-conf 
  resources:
    requests:
      cpu: 1m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8000
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10           
  envFrom:
  - configMapRef:
      name: mediafusion-env
  - configMapRef:
      name: elfbot-mediafusion
      optional: true
  - secretRef:
      name: mediafusion-env

tinyproxy:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/tinyproxy
    tag: v1.4.3@sha256:afc49b44bcab30c261e4e0232e23126da9b63220d2f9a8201ce3e3a90132a035
  podLabels:
      app.elfhosted.com/name: tinyproxy
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "tinyproxy-conf"
      secret.reloader.stakater.com/reload: "tinyproxy-vpn"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    tinyproxy-conf:
      enabled: "true"
      type: "custom"
      mountPath: /etc/tinyproxy/tinyproxy.conf
      subPath: tinyproxy.conf  
      volumeSpec:
        configMap:
          name: tinyproxy-conf 
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory          
  resources:
    requests:
      cpu: 1m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8888
  env:
    WAIT_FOR_VPN: "true"    
  initContainers:      
    setup-warp:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false    
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:2ce9c1407bad9897f4269b3c7f53862befb52a25c712731c91ae05906d41358f
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "8888"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared

mediaflowproxy:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/mediaflow-proxy
    tag: 1.9.7@sha256:98e3643a3e2007e5424500c41c1198bcc01eb46c3584971de61200ec50c0c6da
  podLabels:
      app.elfhosted.com/name: mediaflow-proxy
  priorityClassName: tenant-normal
  podAnnotations:
    kubernetes.io/egress-bandwidth: "64M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mediaflow-proxy,mediaflow-proxy-env"
      secret.reloader.stakater.com/reload: "mediaflowproxy-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  initContainers:
    update-dns: *update_dns_on_init
  additionalContainers:
    clean-up-dns: *clean_up_dns_on_termination
    speedtest:
      image: openspeedtest/latest:latest@sha256:0d2d94087a68cf3f6e2a99b9bcc03c49a8624f144cb670f841bfa3c1570a0eb6
      env:
        CHANGE_CONTAINER_PORTS: "True"
        HTTP_PORT: "3002"
        HTTPS_PORT: "3003"    
      securityContext: *speedtest_securitycontext
  resources:
    requests:
      cpu: 1m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8888
  envFrom:
  - configMapRef:
      name: mediaflow-proxy-env
  - configMapRef:
      name: elfbot-mediaflow-proxy
      optional: true   

xtremio:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/xtremio
    tag: rolling@sha256:e29cd242282bf6fbd95253520a9646f42e06cf559edd199f4f83aba7a9eefc6b
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-xtremio"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PORT: 3649
  persistence:
    tmp: *tmp
  resources:
    requests:
      cpu: 1m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3649
  envFrom:
  - configMapRef:
      name: elfbot-xtremio
      optional: true

stremify:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/stremify
    tag: rolling@sha256:dccb712ee19e8e16512175a4ca87471e24cd50a05c5c1aed18f4a6f8b25b208c
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-stremify"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp:
      enabled: true
      type: emptyDir
      mountPath: /tmp
    nuxt-node-modules:
      enabled: true
      type: emptyDir
      mountPath: /nuxt/node_modules
    nuxt:
      enabled: true
      type: emptyDir
      mountPath: /nuxt/.nuxt
    nitro:
      enabled: true
      type: emptyDir
      mountPath: /home/node/app/.nitro
  resources:
    requests:
      cpu: 1m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  envFrom:
  - configMapRef:
      name: stremify-env
  - configMapRef:
      name: elfbot-stremify
      optional: true
  - secretRef:
      name: stremify-env
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      env:
        IPTABLES_BACKEND: nft
        KILLSWITCH: "true"
        LOCAL_NETWORK: 10.0.0.0/8
        LOC: de-frankfurt
        PORT_FORWARDING: "0"
        PORT_PERSIST: "1"
        NFTABLES: "1"
        VPNDNS: "0"
      envFrom:
      - secretRef:
          name: stremify-vpn
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

recyclarr:
  enabled: false
  image:
    repository: ghcr.io/recyclarr/recyclarr
    tag: latest@sha256:619c3b8920a179f2c578acd0f54e9a068f57c049aff840469eed66e93a4be2cf
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-recyclarr"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9898
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: recyclarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: recyclarr-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-recyclarr
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: recyclarr
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [ ! -f /config/recyclarr.yaml ];
        then
          cp /bootstrap/recyclarr.yaml /config/
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: recyclarr
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
  additionalContainers:
    podinfo:
      image: stefanprodan/podinfo # used to run probes from gatus
    sync:
      image: ghcr.io/recyclarr/recyclarr:latest@sha256:619c3b8920a179f2c578acd0f54e9a068f57c049aff840469eed66e93a4be2cf
      command:
      - /bin/bash
      - -c
      - |
        recyclarr sync
        sleep infinity
      volumeMounts:
      - mountPath: /config
        name: config
      envFrom:
      - configMapRef:
          name: recyclarr-env

knightcrawler: &app_knightcrawler
  enabled: false
  image:
    repository: ghcr.io/elfhosted/knightcrawler-addon
    tag: v2.0.28@sha256:7c183f40192d5455359a6e039ec61e0e42a725a313fe97c432efef07f6d3a269
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: knightcrawler
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-knightcrawler,elfbot-torrentio,knightcrawler-env"
      secret.reloader.stakater.com/reload: "knightcrawler-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 128Mi
    limits:
      cpu: 100m
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 7000
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-knightcrawler
          optional: true
    npm:
      enabled: true
      type: emptyDir
      mountPath: /.npm
    pm2:
      enabled: true
      mountPath: /.pm2
      type: emptyDir
  envFrom:
  - configMapRef:
      name: knightcrawler-env
  - secretRef:
      name: knightcrawler-env

zurg: &app_zurg
  enabled: false
  podLabels:
    app.elfhosted.com/class: debrid
    app.elfhosted.com/name: zurg
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M" # tested with _kilos in Discord on a 97Mbit remux
  image:
    repository: ghcr.io/elfhosted/zurg-rc
    tag: 2024.12.26.0027-nightly@sha256:7bb20a54d64c401deebf05b1dcb72451590af9c86b050a878de3c15a365b1c1b
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-zurg,zurg-env,gluetun-config"
    strategy: Recreate
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: node-role.elfhosted.com/contended
            operator: In
            values:
            - "true"         
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.elfhosted.com/role
              operator: In
              values:
              - nodefinder # use nodefinder in the absense of zurg...
          topologyKey: "kubernetes.io/hostname"
      - weight: 2
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.elfhosted.com/name
              operator: In
              values:
              - zurg # .. but prefer zurg
          topologyKey: "kubernetes.io/hostname"
          namespaceSelector: {}  # i.e., in the absense of any better signal, pick a node which already has zurg on it
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 200m
      memory: 32Mi
    limits:
      cpu: 1
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9999
  persistence:
    tmp: *tmp
    backup: *backup # to pin zurg to the node with the backup PVC
    rclonemountrealdebridzurg: *rclonemountrealdebridzurg
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: zurg
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: zurg
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs          
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: zurg-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-zurg
          optional: true
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  envFrom:
  - configMapRef:
      name: zurg-env # this is here so we can use env vars to detect whether to enable warp
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: zurg
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # We need a /config/logs folder
        mkdir -p /config/logs

        # If we don't already have an example config, create one
        if [[ ! -f /config/config.yml ]];
        then
          cp /bootstrap/config.yml /config/
        fi

        # If we don't already have an example plex_update, create one
        if [[ ! -f /config/plex_update.sh ]];
        then
          cp /bootstrap/plex_update.sh /config/
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: zurg
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        # run the setup script from the configmap, so that we can make templated changes
        bash /bootstrap/setup.sh
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: zurg
      - name: example-config
        mountPath: "/bootstrap/"
    update-dns: *update_dns_on_init
  additionalContainers:
    clean-up-dns: *clean_up_dns_on_termination
    # warp:
    #   image: ghcr.io/elfhosted/warp:rolling@sha256:9b7e2e5da5b1caab6bb1a4e450fc1a2cc7c99f162325901656d5bc9104439eb6
    #   securityContext:
    #     runAsUser: 1000
    #     runAsGroup: 1000
    #     privileged: false
    #     capabilities:
    #       add:
    #       - NET_ADMIN
    #     sysctls:
    #     - name: net.ipv6.conf.all.disable_ipv6
    #       value: "0"
    #     - name: net.ipv4.conf.all.src_valid_mark
    #       value: "1"
    #   volumeMounts:
    #   - mountPath: /data
    #     subPath: warp
    #     name: config
    #   envFrom:
    #   - configMapRef:
    #       name: zurg-env # this is here so we can use env vars to detect whether to enable warp
  addons:
    vpn: &zurg_addons_vpn
      enabled: false # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:2ce9c1407bad9897f4269b3c7f53862befb52a25c712731c91ae05906d41358f
      envFrom:
      - configMapRef:
          name: gluetun-config
          optional: true
      - configMapRef:
          name: zurg-env # this is here so we can use env vars to detect whether to enable warp
      env:
        DOT: "off"
        FIREWALL_INPUT_PORTS: "9999" # 9999 is for zurg
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus
  probes:
    startup:
      spec:
        initialDelaySeconds: 0
        timeoutSeconds: 1
        ## This means it has a maximum of 5*120=720 seconds to start up before it fails
        periodSeconds: 5
        failureThreshold: 120

zurggluetun:
  <<: *app_zurg
  enabled: false
  podLabels:
    app.elfhosted.com/name: zurg
    app.elfhosted.com/class: debrid
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-zurg,zurg-gluetun-config,zurg-env"
  service:
    main:
      nameOverride: zurg
      enabled: true # necessary for probes, but probes aren't working with vpn addon currently
  env:
    WAIT_FOR_VPN: "true"
  addons:
    vpn:
      enabled: true
      <<: *zurg_addons_vpn
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:2ce9c1407bad9897f4269b3c7f53862befb52a25c712731c91ae05906d41358f
      envFrom:
      - configMapRef:
          name: gluetun-config

zurgranger:
  <<: *app_zurg
  podLabels:
    app.elfhosted.com/name: zurg
    app.elfhosted.com/class: dedicated
  podAnnotations:
    kubernetes.io/egress-bandwidth: "500M"
  enabled: false
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-zurg"
  affinity: *dedicated_affinity # force zurg to go onto the dedicated nodes
  resources: *ranger_zurg_resources

plexdebrid: &app_plexdebrid
  enabled: false
  # podLabels:
  #   app.elfhosted.com/name: plexdebrid
  image:
    repository: ghcr.io/elfhosted/plex-debrid
    tag: rolling@sha256:951da714fb083fa5981680faef63b31b050d09b10c5bebbfec93b9a8aa37f093
  podLabels:
    app.elfhosted.com/name: plex-debrid    
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex-debrid"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because of s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
  resources:
    requests:
      cpu: 2m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 3Gi
  ingress:
    main:
      enabled: false
  envFrom:
  - secretRef:
      name: plex-debrid-env
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plex-debrid
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex-debrid
      - mountPath: /tmp
        name: tmp


plexdebriddebridlink: 
  <<: *app_plexdebrid  
  enabled: false
  podLabels:
    app.elfhosted.com/name: plex-debrid-debridlink  
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex-debrid-debridlink"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plex-debrid-debridlink
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid-debridlink
          optional: true          
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex-debrid-debridlink
      - mountPath: /tmp
        name: tmp

plexdebridtorbox: 
  <<: *app_plexdebrid  
  enabled: false  
  podLabels:
    app.elfhosted.com/name: plex-debrid-torbox  
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex-debrid-torbox"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plex-debrid-torbox
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid-torbox
          optional: true     
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex-debrid-torbox
      - mountPath: /tmp
        name: tmp               

plexdebridpremiumize: 
  <<: *app_plexdebrid  
  enabled: false  
  podLabels:
    app.elfhosted.com/name: plex-debrid-premiumize    
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex-debrid-premiumize"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plex-debrid-premiumize
      volumeSpec:
        persistentVolumeClaim:
          claimName: config    
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid-premiumize
          optional: true                     
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex-debrid-premiumize
      - mountPath: /tmp
        name: tmp

# This is a copy of plexdebrid plumbed into the user's VPN
plexdebridalldebrid: 
  <<: *app_plexdebrid
  podLabels:
    app.elfhosted.com/name: plex-debrid-alldebrid    
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex-debrid-alldebrid"  
  enabled: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plex-debrid-alldebrid
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid-alldebrid
          optional: true   
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex-debrid-alldebrid
      - mountPath: /tmp
        name: tmp                 
  addons:
    vpn:
      enabled: true
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:2ce9c1407bad9897f4269b3c7f53862befb52a25c712731c91ae05906d41358f
      envFrom:
      - configMapRef:
          name: gluetun-config
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DOT: "off"
        FIREWALL_INPUT_PORTS: "3001" # 9999 is for rclone
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

codeserver:
  enabled: false
  # runtimeClassName: kata
  image:
    repository: ghcr.io/elfhosted/codeserver
    tag: 4.96.2@sha256:636c4ae77cc2cd2e2f872b1a9be0d3751c71f35cce4eadc0baeccdb98c1d8e6d
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-codeserver"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because of s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    # runAsUser: 568
    # runAsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 15m
      memory: 200Mi
    limits:
      cpu: 2
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config # no subpath, codeserver wants to see all
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid
          optional: true
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: codeserver-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: codeserver
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        mkdir -p /config/.config/code-server/
        if [ ! -f /config/.config/code-server/config.yaml ];
        then
          cp /bootstrap/config.yaml /config/.config/code-server/
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: codeserver
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true

doplarr: &app_doplarr
  enabled: false
  image:
    repository: ghcr.io/elfhosted/doplarr
    tag: v3.6.3@sha256:7cee3c57d37100c4c4b7e94d4ab910f3d5d44183b4d25669b7f6dcd01260eea6
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-doplarr,doplarr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 1m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: elfbot-doplarr
      optional: true

requestrr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/requestrr
    tag: v2.1.6@sha256:d9daf341af2608f8351ae4cfdb6f685c1ea675e18b88860d0bfbc6343202402b
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-requestrr,requestrr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /app/config
      subPath: requestrr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-requestrr
          optional: true
  resources:
    requests:
      cpu: 1m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 4545
  envFrom:
  - configMapRef:
      name: elfbot-requestrr
      optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: requestrr
      - mountPath: /tmp
        name: tmp

rclonedebridlink:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/davdebrid
    tag: v1.2.2@sha256:6571776a022a36889d1cc8392440ad58fa2654535e82b9741f1c287e045c7fd0
  priorityClassName: tenant-normal
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podLabels:
    app.elfhosted.com/name: debridlink
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-debridlink,debridlink-env"
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
    readOnlyRootFilesystem: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: debridlink
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  envFrom:
  - configMapRef:
      name: debridlink-env
  - configMapRef:
      name: elfbot-debridlink
      optional: true
  resources:
    requests:
      cpu: 1m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 100Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080


rclonealldebrid:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.68.2@sha256:7f0485bd06cb3153825f85cfd0ff9f747b205ebca05d3e6767c4f44fc4d0d626
  command:
  - /debrid-provider.sh
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-alldebrid,alldebrid-config,gluetun-config"
  podAnnotations:
    kubernetes.io/egress-bandwidth: "128M"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    cache:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      sizeLimit: 1Gi
    config: 
      enabled: "true"
      type: emptyDir
      mountPath: /config
    bootstrap:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: alldebrid-config
    tinyproxy-conf:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: alldebrid-tinyproxy-conf 
  additionalContainers:
    # Use this to provied proxied access to mediaflowproxy
    tinyproxy:
      image: docker.io/kalaksi/tinyproxy
      volumeMounts:
      - name: tinyproxy-conf
        mountPath: /etc/tinyproxy/tinyproxy.conf
        subPath: tinyproxy.conf                      
  initContainers:
    setup:
      image: ghcr.io/elfhosted/rclone:1.68.2@sha256:7f0485bd06cb3153825f85cfd0ff9f747b205ebca05d3e6767c4f44fc4d0d626
      command:
      - /bin/ash
      - -c
      - |
        set -x

        # Create directory structure

        OBSCURED_PASS=$(rclone obscure doesntmatter)
        cp /bootstrap/rclone-debrid-provider.conf /config/
        sed -i "s/REPLACEUSER/$USER/" /config/rclone-debrid-provider.conf
        sed -i "s/REPLACEPASS/$OBSCURED_PASS/" /config/rclone-debrid-provider.conf
        
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /bootstrap
        name: bootstrap
      resources: *default_resources
      securityContext: *default_securitycontext
      envFrom:
      - configMapRef:
          name: elfbot-alldebrid
          optional: true
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9999      
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 1m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi
  addons:
    vpn:
      enabled: true
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:2ce9c1407bad9897f4269b3c7f53862befb52a25c712731c91ae05906d41358f
      envFrom:
      - configMapRef:
          name: gluetun-config
      env:
        DOT: "off"
        FIREWALL_INPUT_PORTS: "9999,8888" # 9999 is for rclone, 8888 is tinyproxy
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8,192.168.0.0/16,172.16.0.0/20
        DNS_KEEP_NAMESERVER: "on"        
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

rclonepremiumize:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.68.2@sha256:7f0485bd06cb3153825f85cfd0ff9f747b205ebca05d3e6767c4f44fc4d0d626
  command:
  - /debrid-provider.sh
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-premiumize,premiumize-config"
  podAnnotations:
    kubernetes.io/egress-bandwidth: "128M"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    cache:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      sizeLimit: 1Gi
    config: 
      enabled: "true"
      type: emptyDir
      mountPath: /config
    bootstrap:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: premiumize-config
  initContainers:
    setup:
      image: ghcr.io/elfhosted/rclone:1.68.2@sha256:7f0485bd06cb3153825f85cfd0ff9f747b205ebca05d3e6767c4f44fc4d0d626
      command:
      - /bin/ash
      - -c
      - |
        set -x

        # Create directory structure

        OBSCURED_PASS=$(rclone obscure "$PASS")
        cp /bootstrap/rclone-debrid-provider.conf /config/
        sed -i "s/REPLACEUSER/$USER/" /config/rclone-debrid-provider.conf
        sed -i "s/REPLACEPASS/$OBSCURED_PASS/" /config/rclone-debrid-provider.conf
        
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /bootstrap
        name: bootstrap
      resources: *default_resources
      securityContext: *default_securitycontext
      envFrom:
      - configMapRef:
          name: elfbot-premiumize
          optional: true
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9999      
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 1m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi

rclonetorbox:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.68.2@sha256:7f0485bd06cb3153825f85cfd0ff9f747b205ebca05d3e6767c4f44fc4d0d626
  command:
  - /debrid-provider.sh
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-torbox,torbox-config"
  podAnnotations:
    kubernetes.io/egress-bandwidth: "128M"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    cache:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      sizeLimit: 1Gi
    # we'll run the obscure command and copy the config into here
    config: 
      enabled: "true"
      type: emptyDir
      mountPath: /config
    bootstrap:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: torbox-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-torbox
          optional: true          
  initContainers:
    setup:
      image: ghcr.io/elfhosted/rclone:1.68.2@sha256:7f0485bd06cb3153825f85cfd0ff9f747b205ebca05d3e6767c4f44fc4d0d626
      command:
      - /bin/ash
      - -c
      - |
        set -x

        # Create directory structure

        OBSCURED_PASS=$(rclone obscure "$PASS")
        cp /bootstrap/rclone-debrid-provider.conf /config/
        sed -i "s/REPLACEUSER/$USER/" /config/rclone-debrid-provider.conf
        sed -i "s/REPLACEPASS/$OBSCURED_PASS/" /config/rclone-debrid-provider.conf
        
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /bootstrap
        name: bootstrap
      - mountPath: /elfbot
        name: elfbot
      resources: *default_resources
      securityContext: *default_securitycontext
      envFrom:
      - configMapRef:
          name: elfbot-torbox
          optional: true

  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9999
  envFrom:
  - configMapRef:
      name: elfbot-torbox
      optional: true              
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 1m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi

blackhole: &app_blackhole
  enabled: false
  image:
    repository: ghcr.io/elfhosted/wests-blackhole-script
    tag: v1.5.0@sha256:0c1cf1a7166030555e945c134c208e79152549bb5269fc25da848f6ded4dedd7
  priorityClassName: tenant-normal
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-blackhole,blackhole-env"
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
    readOnlyRootFilesystem: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  env:
    BLACKHOLE_RADARR_PATH: "radarr"
    BLACKHOLE_SONARR_PATH: "sonarr"
  envFrom:
  - configMapRef:
      name: blackhole-env
  - configMapRef:
      name: elfbot-blackhole
      optional: true
  resources:
    requests:
      cpu: 1m
      memory: 1Mi
    limits:
      cpu: 1
      memory: 500Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: blackhole
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs

blackhole4k:
  <<: *app_blackhole
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: blackhole4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
  env:
    BLACKHOLE_RADARR_PATH: "radarr4k"
    BLACKHOLE_SONARR_PATH: "sonarr4k"
  envFrom:
  - configMapRef:
      name: blackhole-env
  - configMapRef:
      name: elfbot-blackhole
      optional: true   
  - configMapRef:
      name: elfbot-blackhole4k
      optional: true    

blackholetorbox:
  <<: *app_blackhole
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-blackholetorbox,blackholetorbox-env"
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: blackholetorbox
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
  env:
    BLACKHOLE_RADARR_PATH: "radarr"
    BLACKHOLE_SONARR_PATH: "sonarr"
  envFrom:
  - configMapRef:
      name: blackholetorbox-env
  - configMapRef:
      name: elfbot-blackholetorbox
      optional: true
  initContainers:
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x

        # Create directory structure
        mkdir -p /storage/symlinks/blackholetorbox/radarr/completed
        mkdir -p /storage/symlinks/blackholetorbox/radarr/processing
        mkdir -p /storage/symlinks/blackholetorbox/sonarr/completed
        mkdir -p /storage/symlinks/blackholetorbox/sonarr/processing
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      resources: *default_resources
      securityContext: *default_securitycontext

blackholetorbox4k:
  <<: *app_blackhole
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-blackholetorbox,blackholetorbox-env"
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: blackholetorbox4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
  env:
    BLACKHOLE_RADARR_PATH: "radarr4k"
    BLACKHOLE_SONARR_PATH: "sonarr4k"
  envFrom:
  - configMapRef:
      name: blackholetorbox-env
  - configMapRef:
      name: elfbot-blackholetorbox
      optional: true
  - configMapRef:
      name: elfbot-blackholetorbox4k
      optional: true      

channelsdvr:
  enabled: false
  image:
    repository: fancybits/channels-dvr
    tag: latest@sha256:a1ab946c0c7ec73d36a232d0dc22c4166aa6f83b404c7ba4d9fa39d31bffc294
  priorityClassName: tenant-normal
  podAnnotations:
    kubernetes.io/egress-bandwidth: "125M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-channelsdvr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 15m
      memory: 200Mi
    limits:
      cpu: 1
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8089
  persistence:
    <<: *storagemounts
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /channels-dvr
      subPath: channelsdvr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-channelsdvr
          optional: true
  initContainers:
    bootstrap: *bootstrap


immich:
  enabled: false
  image:
    repository: ghcr.io/immich-app/immich-server
    tag: v1.123.0@sha256:666ce77995230ff7327da5d285c861895576977237de08564e3c3ddf842877eb
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-immich,immich-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 15m
      memory: 200Mi
    limits:
      cpu: 500m
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 2283
  persistence:
    <<: *storagemounts
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    tmp: *tmp
    config:
      enabled: true
      type: custom
      subPath: immich
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    upload:
      enabled: true
      type: emptyDir
      mountPath: /usr/src/app/upload
      sizeLimit: 1Gi
    upload-encoded:
      enabled: true
      type: emptyDir
      mountPath: /usr/src/app/upload/encoded-video
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-immich
          optional: true
  envFrom:
  - configMapRef:
      name: immich-env
  - configMapRef:
      name: elfbot-immich
      optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: immich
      - mountPath: /tmp
        name: tmp
  additionalContainers:
    ml:
      image: ghcr.io/immich-app/immich-machine-learning:v1.123.0
      envFrom:
      - configMapRef:
          name: immich-env
      resources:
        requests:
          cpu: 15m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 4Gi
    database:
      image: docker.io/tensorchord/pgvecto-rs:pg14-v0.2.0@sha256:90724186f0a3517cf6914295b5ab410db9ce23190a2d9d0b9dd6463e3fa298f0
      env:
        POSTGRES_INITDB_ARGS: '--data-checksums'
        POSTGRES_PASSWORD: immich
        POSTGRES_USER: immich
        POSTGRES_DB: immich
      volumeMounts:
        - name: config
          subPath: immich/database
          mountPath: /var/lib/postgresql/data
      resources:
        requests:
          cpu: 15m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 4Gi
    redis:
      image: docker.io/redis:7.4-alpine@sha256:8860d052306f47904110630a97b1edd8439e24ef7b7ed4bb315ac12f1c3a58c3
      envFrom:
      - configMapRef:
          name: immich-env
      resources:
        requests:
          cpu: 15m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 4Gi

kubernetesdashboard:

  ## Name of Priority Class of pods
  priorityClassName: "tenant-normal"

  ## Pod resource requests & limits
  resources:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 256Mi

  extraArgs:
    - --enable-skip-login
    - --enable-insecure-login
    - --system-banner=Built</A> with ❤️ by <A HREF="https://funkypenguin.co.nz">@funkypenguin</A> and friends (<I><A HREF="https://chat.funkypenguin.co.nz">join us!</A></I>)

  ## Serve application over HTTP without TLS
  ##
  ## Note: If set to true, you may want to add --enable-insecure-login to extraArgs
  protocolHttp: true

  affinity: *standard_affinity
  tolerations: *standard_tolerations

  # Global dashboard settings
  settings:
    ## Cluster name that appears in the browser window title if it is set
    clusterName: "ElfHosted"
    # defaultNamespace: "{{ .Release.Namespace }}"
    # namespaceFallbackList: [ "{{ .Release.Namespace }}" ]

    ## Max number of items that can be displayed on each list page
    itemsPerPage: 10
    ## Number of seconds between every auto-refresh of logs
    logsAutoRefreshTimeInterval: 5
    ## Number of seconds between every auto-refresh of every resource. Set 0 to disable
    resourceAutoRefreshTimeInterval: 5
    ## Hide all access denied warnings in the notification panel
    disableAccessDeniedNotifications: true

  ## Metrics Scraper
  ## Container to scrape, store, and retrieve a window of time from the Metrics Server.
  ## refs: https://github.com/kubernetes-sigs/dashboard-metrics-scraper
  metricsScraper:
    ## Wether to enable dashboard-metrics-scraper
    enabled: true
    image:
      repository: kubernetesui/metrics-scraper
      tag: v1.0.9
    resources: {}
    ## SecurityContext especially for the kubernetes dashboard metrics scraper container
    ## If not set, the global containterSecurityContext values will define these values
    # containerSecurityContext:
    #   allowPrivilegeEscalation: false
    #   readOnlyRootFilesystem: true
    #   runAsUser: 1001
    #   runAsGroup: 2001
  #  args:
  #    - --log-level=info
  #    - --logtostderr=true

  # Don't auto-create RBAC for us, we'll do it manually
  rbac:
    create: false

  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: kubernetes-dashboard

# optional but disabled by default to prevent errors


gluetun:
  enabled: false # just to avoid errors
cometproxystreaming:
  enabled: false
mediafusionproxystreaming:
  enabled: false
elfassesment:
  enabled: false

# The hobbit apps
zurghobbit:
  <<: *app_zurg
  podAnnotations: *hobbit_streamer_podAnnotations
  # zurg is the "anchor" which keeps all the other apps on the same node
  affinity: *dedicated_affinity # force zurg to go onto the dedicated nodes
  resources: *hobbit_zurg_resources

zurghalfling:
  <<: *app_zurg
  podAnnotations: *halfling_streamer_podAnnotations
  # zurg is the "anchor" which keeps all the other apps on the same node
  affinity: *dedicated_affinity # force zurg to go onto the dedicated nodes
  resources: *halfling_zurg_resources

zurgnazgul:
  <<: *app_zurg
  podAnnotations: *nazgul_streamer_podAnnotations
  # zurg is the "anchor" which keeps all the other apps on the same node
  affinity: *dedicated_affinity # force zurg to go onto the dedicated nodes
  resources: *nazgul_zurg_resources

plexhobbit:
  <<: *app_plex
  podAnnotations: *hobbit_streamer_podAnnotations
  resources: *hobbit_streamer_resources

jellyfinhobbit:
  <<: *app_jellyfin
  podAnnotations: *hobbit_streamer_podAnnotations
  resources: *hobbit_streamer_resources

embyhobbit:
  <<: *app_emby
  podAnnotations: *hobbit_streamer_podAnnotations
  resources: *hobbit_streamer_resources

plexhalfling:
  <<: *app_plex
  podAnnotations: *halfling_streamer_podAnnotations
  resources: *halfling_streamer_resources

jellyfinhalfling:
  <<: *app_jellyfin
  podAnnotations: *halfling_streamer_podAnnotations
  resources: *halfling_streamer_resources

embyhalfling:
  <<: *app_emby
  podAnnotations: *halfling_streamer_podAnnotations
  resources: *halfling_streamer_resources

plexnazgul:
  <<: *app_plex
  podAnnotations: *nazgul_streamer_podAnnotations
  resources: *nazgul_streamer_resources

jellyfinnazgul:
  <<: *app_jellyfin
  podAnnotations: *nazgul_streamer_podAnnotations
  resources: *nazgul_streamer_resources

embynazgul:
  <<: *app_emby
  podAnnotations: *nazgul_streamer_podAnnotations
  resources: *nazgul_streamer_resources

# This file must end on a single newline
