# This controls whether our automation will auto-release this to stable during the daily maint window
safeToRelease: true

# false by default means the FSN cluster (so no migrations)
location:
  enabled: false
zurgling:
  enabled: false
debridling:
  enabled: false
sidekick:
  enabled: false
plexremotestreamworkaround:
  enabled: false
decypharrreplacezurg:
  enabled: false

# false by default
volsync:
  enabled: false
  restic_repository:
  restic_password:
  aws_access_key_id:
  aws_secret_access_key:

# Set these to the default if nothing else is set
storageclass:
  rwx:
    name: ceph-filesystem-ssd
    accessMode: ReadWriteMany
    volumeSnapshotClassName: ceph-filesystem
  rwo:
    name: ceph-block-ssd
    accessMode: ReadWriteOnce
    volumeSnapshotClassName: ceph-block

# These control the egress bandwidth of the semi-dedi products
hobbit_streamer_podAnnotations: &hobbit_streamer_podAnnotations
  kubernetes.io/egress-bandwidth: "250M"
ranger_streamer_podAnnotations: &ranger_streamer_podAnnotations
  kubernetes.io/egress-bandwidth: "500M"
halfling_streamer_podAnnotations: &halfling_streamer_podAnnotations
  kubernetes.io/egress-bandwidth: "1000M"
nazgul_streamer_podAnnotations: &nazgul_streamer_podAnnotations
  kubernetes.io/egress-bandwidth: "1000M"

# These control the requests used to "anchor" a stack to a particular dedicated node. The following defaults can be overridden on a per-cluster basis:
hobbit_zurg_resources: &hobbit_zurg_resources
  requests:
    cpu: "1.8"
    memory: 30Mi
  limits:
    cpu: "2"
    memory: 4Gi

ranger_zurg_resources: &ranger_zurg_resources
  requests:
    cpu: "3500m"
    memory: 30Mi
  limits:
    cpu: "4"
    memory: 4Gi

halfling_zurg_resources: &halfling_zurg_resources
  requests:
    cpu: "7"
    memory: 30Mi
  limits:
    cpu: "8"
    memory: 4Gi

nazgul_zurg_resources: &nazgul_zurg_resources
  requests:
    cpu: "7"
    memory: 30Mi
  limits:
    cpu: "16"
    memory: 4Gi


# These allow us to manage RAM usage on streamers
hobbit_streamer_resources: &hobbit_streamer_resources
  requests:
    cpu: "10m"
    memory: 30Mi
  limits:
    cpu: "2"
    memory: 4Gi

ranger_streamer_resources: &ranger_streamer_resources
  requests:
    cpu: 10m
    memory: 30Mi
  limits:
    cpu: 4
    memory: 4Gi

# Giving more than 4 CPU to a streamer is unwise regardless
halfling_streamer_resources: &halfling_streamer_resources
  requests:
    cpu: 10m
    memory: 30Mi
  limits:
    cpu: 4
    memory: 4Gi

nazgul_streamer_resources: &nazgul_streamer_resources
  requests:
    cpu: 10m
    memory: 30Mi
  limits:
    cpu: 4
    memory: 4Gi

# sets the user's base dns domain
dns_domain: elfhosted.com

tooling_image: &tooling_image ghcr.io/elfhosted/tooling:focal-20240530@sha256:458d1f3b54e9455b5cdad3c341d6853a6fdd75ac3f1120931ca3c09ac4b588de

# all RD pods have to exist with zurg - make this soft for now
standard_affinity: &standard_affinity
  podAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app.elfhosted.com/role
          operator: In
          values:
          - nodefinder # use nodefinder in the absence of zurg...
      topologyKey: "kubernetes.io/hostname"

dedicated_affinity: &dedicated_affinity
  podAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app.elfhosted.com/role
          operator: In
          values:
          - nodefinder # use nodefinder in the absence of zurg...
      topologyKey: "kubernetes.io/hostname"

standard_tolerations: &standard_tolerations
# not using tolerations anymore
# - key: node-role.elfhosted.com/dedicated
#   operator: Exists
# - key: node-role.elfhosted.com/hobbit
#   operator: Exists

hobbit_tolerations: &hobbit_tolerations
# not using tolerations anymore
# - key: node-role.elfhosted.com/hobbit
#   operator: Exists

# Set minimal requests so that pods can co-exist with streamers
hobbit_resources: &hobbit_resources
  requests:
    cpu: "1m"
    memory: "16Mi"
  limits:
    cpu: "1"
    memory: 4Gi

ranger_resources: &ranger_resources
  requests:
    cpu: "1m"
    memory: "16Mi"
  limits:
    cpu: "2"
    memory: 8Gi

volumespec_ephemeral_volume_1000g: &volumespec_ephemeral_volume_1000g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 1000Gi

volumespec_ephemeral_volume_100g: &volumespec_ephemeral_volume_100g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 100Gi

volumespec_ephemeral_volume_1g: &volumespec_ephemeral_volume_1g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 1Gi

volumespec_ephemeral_volume_10g: &volumespec_ephemeral_volume_10g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 10Gi

volumespec_ephemeral_volume_50g: &volumespec_ephemeral_volume_50g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 50Gi

volumespec_ephemeral_volume_200g: &volumespec_ephemeral_volume_200g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 200Gi

volumespec_ephemeral_volume_500g: &volumespec_ephemeral_volume_500g
  ephemeral:
    volumeClaimTemplate:
      metadata:
        labels:
          velero.io/exclude-from-backup: "true"
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "topolvm-provisioner-thin"
        resources:
          requests:
            storage: 500Gi

# And this makes the media / rclone mounts tidier.
rclonemountrealdebridzurg: &rclonemountrealdebridzurg
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: realdebrid-zurg
  mountPath: /storage/realdebrid-zurg
rclonemountdebridlink: &rclonemountdebridlink
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: debridlink
  mountPath: /storage/debridlink
rclonemountalldebrid: &rclonemountalldebrid
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: alldebrid
  mountPath: /storage/alldebrid
rclonemountpremiumize: &rclonemountpremiumize
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: premiumize
  mountPath: /storage/premiumize
rclonemounttorbox: &rclonemounttorbox
  enabled: false
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: torbox
  mountPath: /storage/torbox
rclonemountdebridav: &rclonemountdebridav
  enabled: false 
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: debridav
  mountPath: /storage/debridav    
rclone: &rclone
  enabled: true # everyone gets an rclone mount
  type: custom
  volumeSpec:
    persistentVolumeClaim:
      claimName: rclone
  mountPath: /storage/rclone  

# This simplfies the process of adding all the optional mounts to every app
storagemounts: &storagemounts
  rclone: *rclone
  rclonemountrealdebridzurg: *rclonemountrealdebridzurg
  rclonemountdebridlink: *rclonemountdebridlink
  rclonemountalldebrid: *rclonemountalldebrid
  rclonemountpremiumize: *rclonemountpremiumize
  rclonemounttorbox: *rclonemounttorbox
  rclonemountdebridav: *rclonemountdebridav
  tmp: &tmp
    enabled: true
    type: emptyDir
    mountPath: /tmp
  symlinks: &symlinks
    enabled: true
    type: custom
    volumeSpec:
      persistentVolumeClaim:
        claimName: symlinks
    mountPath: /storage/symlinks
  backup: &backup
    enabled: true
    type: custom
    volumeSpec:
      persistentVolumeClaim:
        claimName: backup

# The entire bootstrap sidecar/additionalcontainer
default_resources: &default_resources
  requests:
    cpu: 0m
    memory: 1Mi
    # ephemeral-storage: 50Mi
  limits:
    cpu: 1
    memory: 4Gi # just a safety net against bugs!
    # ephemeral-storage: 2Gi # a safety net against node ephemeral space exhaustion

default_securitycontext: &default_securitycontext
  seccompProfile:
    type: RuntimeDefault
  readOnlyRootFilesystem: true
  allowPrivilegeEscalation: false
  runAsUser: 568
  runAsGroup: 568
  capabilities:
    drop:
    - ALL

speedtest_securitycontext: &speedtest_securitycontext
  seccompProfile:
    type: RuntimeDefault
  readOnlyRootFilesystem: false
  allowPrivilegeEscalation: false
  runAsUser: 101
  runAsGroup: 101
  capabilities:
    drop:
    - ALL

# We use this to provide env not only to bootstrap, but also to the torrent clients which use elfvpn
# it's necessary since the wireguard configs are in S3
bootstrap_env: &bootstrap_env
- name: AWS_ACCESS_KEY_ID
  valueFrom:
    secretKeyRef:
      key: access-key-id
      name: b2-elfhosted-config-ro
- name: AWS_SECRET_ACCESS_KEY
  valueFrom:
    secretKeyRef:
      key: secret-key
      name: b2-elfhosted-config-ro
- name: S3_ENDPOINT_URL
  value: https://s3.us-west-000.backblazeb2.com
- name: K8S_APP_NAME
  valueFrom:
    fieldRef:
      fieldPath: metadata.labels['app.kubernetes.io/name']
- name: ELF_APP_NAME
  valueFrom:
    fieldRef:
      fieldPath: metadata.labels['app.elfhosted.com/name']

migrate_data: &migrate_data
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /bin/bash
  - -c
  - |

    if [[ ! -f /config/.migrated-20241007 ]]
    then
      if [[ ! -z "$(ls -A /migration)" ]]
      then
        echo "Migrating from /migration/..."
        cp -rfpv /migration/* /config/
        touch /config/.migrated-20241007
      fi
    else
      echo "No migration necessary"
    fi

  volumeMounts:
  - mountPath: /config
    name: config
  - mountPath: /migration
    name: migration

  env: *bootstrap_env
  resources: *default_resources
  securityContext: *default_securitycontext

bootstrap: &bootstrap
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /bin/bash
  - -c
  - |
    set -e

    # Allows us to use app.elfhosted.com/name, but fall back to app.kubernetes.io/name if the former doesn't exist
    if [[ -z "$ELF_APP_NAME" ]]; then
      ELF_APP_NAME=$K8S_APP_NAME
    fi

    # look for commands - we match specific names in order of least-destructive
    TIMESTAMP_NOW=$(date +%s)
    if [[ -f /etc/elfbot/pause ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/pause)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=pause
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/backup && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/backup)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=backup
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/reset && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/reset)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=reset
      fi
    fi

    case $COMMAND in

      "pause")
        echo "Recent pause command found, sleeping 5m.."
        sleep 300
        ;;

      "reset")

        # Safety check - if /config/homer exists, we've not mounted /config properly and are about to wipe out EVERYTHING
        if [ -e "/config/homer" ]; then
            echo "The path /config/homer exists. Exiting."
            exit 1
        fi      
        echo "Recent reset command found, resetting"
        rm -rf /config/*
        ;;

      "backup")
        echo "Recent backup command found, backing up to /storage/backup/${ELF_APP_NAME}-${TIMESTAMP}"
        TIMESTAMP=$(printf '%(%Y-%m-%d--%H-%M)T\n' -1)
        cp -rfp /config /storage/backup/$ELF_APP_NAME-$TIMESTAMP
        ;;

    esac

    if [[ ! -f /config/i-am-bootstrapped ]]
    then
      echo "Bootstrapping from goldilocks config..."
      s5cmd sync s3://elfhosted-config/goldilocks/$ELF_APP_NAME/* /config/
      touch /config/i-am-bootstrapped
    fi

  volumeMounts:
  - mountPath: /etc/elfbot
    name: elfbot
  - mountPath: /config
    name: config
  - mountPath: /storage/backup
    name: backup
  - mountPath: /tmp
    name: tmp
  env: *bootstrap_env
  resources: *default_resources
  securityContext: *default_securitycontext

bootstrap_elfbot: &bootstrap_elfbot
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /bin/bash
  - -c
  - |
    set -e

    # Allows us to use app.elfhosted.com/name, but fall back to app.kubernetes.io/name if the former doesn't exist
    if [[ -z "$ELF_APP_NAME" ]]; then
      ELF_APP_NAME=$K8S_APP_NAME
    fi

    # look for commands - we match specific names in order of least-destructive
    TIMESTAMP_NOW=$(date +%s)
    if [[ -f /etc/elfbot/pause ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/pause)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=pause
      fi
    fi


    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/backup && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/backup)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=backup
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/reset && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/reset)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=reset
      fi
    fi

    case $COMMAND in

      "pause")
        echo "Recent pause command found, sleeping 5m.."
        sleep 300
        ;;

      "reset")
        echo "Recent reset command found, resetting"
        rm -rf /config/*
        ;;

      "backup")
        echo "Recent backup command found, backing up to /storage/elfstorage/backup/${ELF_APP_NAME}-${TIMESTAMP}"
        mkdir -p /storage/elfstorage/backup
        TIMESTAMP=$(printf '%(%Y-%m-%d--%H-%M)T\n' -1)
        cp -rfp /config /storage/elfstorage/backup/$ELF_APP_NAME-$TIMESTAMP
        ;;

    esac
  volumeMounts:
  - mountPath: /etc/elfbot
    name: elfbot
  - mountPath: /config
    name: config
  - mountPath: /tmp
    name: tmp
  env: *bootstrap_env
  resources: *default_resources
  securityContext: *default_securitycontext

storagehub_bootstrap: &storagehub_bootstrap
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /bin/bash
  - -c
  - |
    set -e

    # Allows us to use app.elfhosted.com/name, but fall back to app.kubernetes.io/name if the former doesn't exist
    if [[ -z "$ELF_APP_NAME" ]]; then
      ELF_APP_NAME=$K8S_APP_NAME
    fi

    # look for commands - we match specific names in order of least-destructive
    TIMESTAMP_NOW=$(date +%s)
    if [[ -f /etc/elfbot/pause ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/pause)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=pause
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/backup && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/backup)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=backup
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/reset && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/reset)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=reset
      fi
    fi

    case $COMMAND in

      "pause")
        echo "Recent pause command found, sleeping 5m.."
        sleep 300
        ;;

      "reset")
        echo "Recent reset command found, resetting"
        rm -rf /config/${ELF_APP_NAME}/*
        ;;

      "backup")
        echo "Recent backup command found, backing up to /storage/elfstorage/backup/${ELF_APP_NAME}-${TIMESTAMP}"
        mkdir -p /storage/elfstorage/backup
        TIMESTAMP=$(printf '%(%Y-%m-%d--%H-%M)T\n' -1)
        cp -rfp /config/${ELF_APP_NAME} /storage/elfstorage/backup/$ELF_APP_NAME-$TIMESTAMP
        ;;

    esac

    if [[ ! -f /config/${ELF_APP_NAME}/i-am-bootstrapped ]]
    then
      echo "Bootstrapping from goldilocks config..."
      s5cmd sync s3://elfhosted-config/goldilocks/$ELF_APP_NAME/* /config/${ELF_APP_NAME}/
      touch /config/${ELF_APP_NAME}/i-am-bootstrapped
    fi

  volumeMounts:
  - mountPath: /etc/elfbot
    name: elfbot
  - mountPath: /config
    name: config
  - mountPath: /tmp
    name: tmp
  env: *bootstrap_env
  resources: *default_resources
  securityContext: *default_securitycontext

# Eventually we'll remove the old one, and rename this to bootstrap
bootstrap_migration: &bootstrap_migration
  image: *tooling_image
  imagePullPolicy: IfNotPresent
  command:
  - /bin/bash
  - -c
  - |
    set -e

    # Allows us to use app.elfhosted.com/name, but fall back to app.kubernetes.io/name if the former doesn't exist
    if [[ -z "$ELF_APP_NAME" ]]; then
      ELF_APP_NAME=$K8S_APP_NAME
    fi

    # look for commands - we match specific names in order of least-destructive
    TIMESTAMP_NOW=$(date +%s)
    if [[ -f /etc/elfbot/pause ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/pause)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=pause
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/backup && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/backup)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=backup
      fi
    fi

    # If no command is set, then move onto the next possibility
    if [[ -f /etc/elfbot/reset && -z "$COMMAND" ]]; then
      TIMESTAMP_COMMAND=$(cat /etc/elfbot/reset)
      TIMESTAMP_DIFF=$((TIMESTAMP_NOW-TIMESTAMP_COMMAND))
      if [[ $TIMESTAMP_DIFF -lt 600 ]]; then
        COMMAND=reset
      fi
    fi

    case $COMMAND in

      "pause")
        echo "Recent pause command found, sleeping 5m.."
        sleep 300
        ;;

      "reset")
        echo "Recent reset command found, resetting"
        rm -rfv /config/*
        ;;

      "backup")
        echo "Recent backup command found, backing up to /storage/elfstorage/backup/${ELF_APP_NAME}-${TIMESTAMP}"
        mkdir -p /storage/elfstorage/backup
        TIMESTAMP=$(printf '%(%Y-%m-%d--%H-%M)T\n' -1)
        cp -rfp /config /storage/elfstorage/backup/$ELF_APP_NAME-$TIMESTAMP
        ;;

    esac

    if [[ ! -f /config/i-am-migrated ]]
    then
      if [[ ! -z "$(ls -A /config-hdd)" ]]
      then
        echo "Migrating from /config-hdd/..."
        time cp /config-hdd/* /config/ -rfpv
        touch /config/i-am-migrated
      fi
    fi


    if [[ ! -f /config/i-am-bootstrapped ]]
    then
      echo "Bootstrapping from goldilocks config..."
      s5cmd sync s3://elfhosted-config/goldilocks/$ELF_APP_NAME/* /config/
      touch /config/i-am-bootstrapped
    fi
  volumeMounts:
  - mountPath: /etc/elfbot
    name: elfbot
  - mountPath: /config
    name: config
  - mountPath: /migation
    name: confighdd
  env: *bootstrap_env
  resources: *default_resources
  securityContext: *default_securitycontext

# This lets users buy blocks of 1TB storage, and add it to their 100Gi
elfstoragetb:
  quantity: 0
# And this lets a user buy a bundle (different SKU), and then still add more elfstorage later
elfstoragetbbundled:
  quantity: 0

# provide a default
userId: 1

# our VPN loadbalancerIP
torrentLoadBalancerIP: 10.0.42.101

# these are the "exposed" services which allow users to override SSO
# by themselves, they do nothing, but they allow us to selectively disable
# SSO on ingressroutes, or to use non-standard API keys in Homer
radarrexposed:
  enabled: false
  apikey: 041776c8d5f74bf295aa486d9d51c33a
radarr4kexposed:
  enabled: false
  apikey: 7da5d4ba79804527b78a78b68c7a0781
sonarrexposed:
  enabled: false
  apikey: a6f1c7d07fab4be49c5c1cb545f85a76
sonarr4kexposed:
  enabled: false
  apikey: e4f93c115169484bbed19821f7ac8e49
lidarrexposed:
  enabled: false
  apikey: 0e68e28531a249659737513d3102bfe9
readarrexposed:
  enabled: false
  apikey: 74b033ff59964011b8a32c014fdb9b68
readarraudioexposed:
  enabled: false
  apikey: 8496cefe2c6b46ee921e18caddf6a943
prowlarrexposed:
  enabled: false
  apikey: c53bc3bd17c645c3a457e5342a02cd66
bazarrexposed:
  enabled: false
  apikey: 94ab8212a12378fa5333cbf75a3c0390
bazarr4kexposed:
  enabled: false
  apikey: 393bda5f898886a2b87413e6452313af
qbittorrentexposed:
  enabled: false
rdtclientexposed:
  enabled: false
rdtclientalldebridexposed:
  enabled: false
delugeexposed:
  enabled: false
rutorrentexposed:
  enabled: false
sabnzbdexposed:
  enabled: false
  apikey: 8flkbru7ncdps3dzzgk48q2msz41m4on
nzbgetexposed:
  enabled: false
mylarrexposed:
  enabled: false
  apikey: 0f97f6a7f352c63eb43fcb7e53ea9d8f
rivenexposed:
  enabled: false
  apikey: 1nMZNC0Cg6UP7sblvFirUi9Sad4ga84u  
tunarrexposed:
  enabled: false
cometexposed:
  enabled: false  
aiostreamsexposed:
  enabled: false    
davioexposed:
  enabled: false  
jackettioexposed:
  enabled: false  
knightcrawlerexposed:
  enabled: false  
mediafusionexposed:
  enabled: false
stremthruexposed:
  enabled: false  
stremifyexposed:
  enabled: false    
stremiojackettexposed:
  enabled: false  
youriptvexposed:
  enabled: false
threadfinexposed:
  enabled: false
dispatcharrexposed:
  enabled: false  
zurgexposed:
  enabled: false
elfassessment:
  enabled: false
uptimekumacustomdomain:
  enabled: false
mattermostcustomdomain:
  enabled: false
vaultwardencustomdomain:
  enabled: false
jellyseerrcustomdomain:
  enabled: false
overseerrcustomdomain:
  enabled: false
plexcustomdomain:
  enabled: false
jellyfincustomdomain:
  enabled: false
embycustomdomain:
  enabled: false
flixiocustomdomain:
  enabled: false
pairdropcustomdomain:
  enabled: false
gotosocialcustomdomain:
  enabled: false  
blueskypdscustomdomain:
  enabled: false    

rutorrentgluetun: &rutorrent
  enabled: false
  automountServiceAccountToken: false
  image:
      repository: ghcr.io/elfhosted/rutorrent
      tag: 5.1.7-7.2@sha256:b5138140095c158f72e386fd7284add7e3b5349a251a5594666bd7a62b9a75a8
  podLabels:
    app.elfhosted.com/name: rutorrent
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,rutorrent-config,rutorrent-gluetun-config,elfbot-rutorrent" # Reload the deployment every time the yaml config changes
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    # runAsUser: 568 # enforced in env vars
    # runAsGroup: 568
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  envFrom:
  - configMapRef:
      name: elfbot-rutorrent
      optional: true
  # we need the injected initcontainer to run as root, so we can't change the pod-level uid/gid
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 568 # s6's fault
    # runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  persistence:
    <<: *storagemounts
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_100g
    config:
      enabled: true
      type: custom
      mountPath: /data/rtorrent/
      subPath: rutorrent
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rutorrent
          optional: true
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    port-range: # Used for dynamic port-forwarding
      enabled: true
      type: emptyDir
      mountPath: /port-range
      sizeLimit: 1Gi
    custom-rtlocal:
      enabled: "true"
      mountPath: "/.rtlocal.rc-elfhosted"
      subPath: ".rtlocal.rc-elfhosted"
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
    custom-rtorrentrc:
      enabled: "true"
      mountPath: "/.rtorrent.rc-elfhosted"
      subPath: ".rtorrent.rc-elfhosted"
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
    custom-s6-init-05:
      enabled: "true"
      mountPath: "/etc/cont-init.d/05-apply-elfhosted-config.sh"
      subPath: "05-apply-elfhosted-config.sh"
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
          defaultMode: 0755
    custom-s6-init-06:
      enabled: "true"
      mountPath: "/etc/cont-init.d/02-wait-for-vpn.sh"
      subPath: "02-wait-for-vpn.sh"
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
          defaultMode: 0755
    custom-s6-init-07:
      enabled: "true"
      mountPath: "/etc/cont-init.d/03-set-inbound-port.sh"
      subPath: "03-set-inbound-port.sh "
      type: "custom"
      volumeSpec:
        configMap:
          name: rutorrent-config
          defaultMode: 0755
    dante-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: dante-config
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: false # necessary for probes, but probes aren't working with vpn addon currently
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1200Mi
  env:
    # -- Set the container timezone
    PUID: 568
    GUID: 568
    RUTORRENT_PORT: 8080 # necessary for health checks
    # S6_READ_ONLY_ROOT: 1 # this seems to break rutorrent :(
    WAIT_FOR_VPN: "true"
    PORT_FILE: /data/rtorrent/forwarded-port
    WAN_IP_CMD: 'curl -s ifconfig.me'
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rutorrent
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi
      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext: *default_securitycontext
      resources: *default_resources
      envFrom:
      - configMapRef:
          name: rutorrent-gluetun-config
  addons:
    vpn: &rutorrent_addons_vpn
      enabled: true
      type: gluetun
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      envFrom:
      - configMapRef:
          name: rutorrent-gluetun-config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
        subPath: rutorrent
      config: # We have to set this to null so that we can override with our own config

      # The scripts that get run when the VPN connection opens/closes are defined here.
      # The default scripts will write a string to represent the current connection state to a file.
      # Our qBittorrent image has a feature that can wait for this file to contain the word 'connected' before actually starting the application.
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus
  additionalContainers:
    # Use this to provied proxied access to arrs
    # dante:
    #   image: ghcr.io/elfhosted/dante:v1.4.3
    #   env: *bootstrap_env
    #   securityContext: *default_securitycontext
    #   volumeMounts:
    #   - mountPath: /tmp
    #     name: tmp
    #   - mountPath: /etc/sockd.conf
    #     name: dante-config
    #     subPath: sockd.conf
    mam-helper:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |
        set -e
        set -x

        echo "Waiting for VPN to be connected..."
        while ! grep -s -q "connected" /shared/vpnstatus; do
            # Also account for gluetun-style http controller
            if (curl -s http://localhost:8042/v1/openvpn/status | grep -q running); then
                break
            fi
            echo "VPN not connected"
            sleep 2
        done
        echo "VPN Connected, processing cookies..."

        # If we have a cookie already, try to use it
        if [[ -f /config/mam/saved.cookies ]]; then
          curl -c /config/mam/saved.cookies -b /config/mam/saved.cookies https://t.myanonamouse.net/json/dynamicSeedbox.php  -o /config/mam/mam_id-curl-output.log
        fi

        # Now whether that worked or not, look for /config/mam/mam_id
        mkdir -p /config/mam
        while [ 1 ]; do
          if [[ -f /config/mam/mam_id ]]; then
            curl -c /config/mam/saved.cookies -b "mam_id=$(cat /config/mam/mam_id)" https://t.myanonamouse.net/json/dynamicSeedbox.php -o /config/mam/mam_id-curl-output.log
            mv /config/mam/mam_id /config/mam/mam_id_processed
          fi
          sleep 1m
        done
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: rutorrent
      - mountPath: /shared
        name: shared
      resources: *default_resources
      securityContext: *default_securitycontext

rutorrentpia:
  <<: *rutorrent
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,rutorrent-config,rutorrent-pia-config,elfbot-rutorrent,dante-config" # Reload the deployment every time the yaml config changes
  addons:
    vpn:
      <<: *rutorrent_addons_vpn
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      envFrom:  
      - configMapRef:
          name: rutorrent-pia-config
      - configMapRef:
          name: elfbot-rutorrent
          optional: true              
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rutorrent
      - mountPath: /tmp
        name: tmp

delugegluetun: &deluge
  enabled: false
  podLabels:
    app.elfhosted.com/name: deluge
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M"
  automountServiceAccountToken: false
  image:
    repository: ghcr.io/geek-cookbook/deluge
    tag: 2.1.1@sha256:448324e342c47020e4e9fbc236282ceb80ebebd7934a486a6f1e487a7e4034bf
  priorityClassName: tenant-bulk
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  # we need the injected initcontainer to run as root, so we can't change the pod-level uid/gid
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-deluge,deluge-gluetun-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    <<: *storagemounts
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_100g
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: deluge
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-deluge
          optional: true
    elfscripts:
      enabled: "true"
      mountPath: "/elfscripts/"
      type: "custom"
      volumeSpec:
        configMap:
          name: deluge-elfscripts
          defaultMode: 0755
    dante-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: dante-config
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 8112
  env:
    # -- Set the container timezone
    TZ: UTC
    PUID: 568
    PGID: 568
    DELUGE_LOGLEVEL: "info"
  envFrom:
  - configMapRef:
      name: elfbot-deluge
      optional: true
  extraEnvVars:
  - name: PORT_FILE
    valueFrom:
      configMapKeyRef:
        name: deluge-gluetun-config
        key: PORT_FILE
    optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/deluge/torrent_files

        JQ_FILTER=".listen_random_port=false"
        JQ_FILTER="${JQ_FILTER} | .pre_allocate_storage=false"
        JQ_FILTER="${JQ_FILTER} | .stop_seed_ratio=2"
        JQ_FILTER="${JQ_FILTER} | .cache_size=52428"
        JQ_FILTER="${JQ_FILTER} | .share_ratio_limit=2"
        JQ_FILTER="${JQ_FILTER} | .stop_seed_at_ratio=true"

        jq "${JQ_FILTER}" /config/core.conf > /config/core-new.conf
        cp /config/core-new.conf /config/core.conf

        # # Avoid session timeouts
        # sed -i  "s/session_timeout:\".*/session_timeout\": 99999,/" /config/web.conf

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /shared
        name: shared
      env: *bootstrap_env
      securityContext: *default_securitycontext
      envFrom:
      - configMapRef:
          name: deluge-gluetun-config
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1500Mi
  addons:
    vpn: &deluge_addons_vpn
      enabled: true
      type: gluetun
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      envFrom:
      - configMapRef:
          name: deluge-gluetun-config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
        subPath: deluge
      config: # We have to set this to null so that we can override with our own config

      # The scripts that get run when the VPN connection opens/closes are defined here.
      # The default scripts will write a string to represent the current connection state to a file.
      # Our qBittorrent image has a feature that can wait for this file to contain the word 'connected' before actually starting the application.
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus
  additionalContainers:
    deluge-web:
      image: ghcr.io/geek-cookbook/deluge:2.1.1@sha256:448324e342c47020e4e9fbc236282ceb80ebebd7934a486a6f1e487a7e4034bf
      command:
      - /usr/bin/deluge-web
      - -L
      - info
      - -d
      - -c
      - /config
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /tmp
        name: tmp
      env:
        PYTHON_EGG_CACHE: /tmp/.cache

delugepia:
  <<: *deluge
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,deluge-config,deluge-pia-config,elfbot-deluge" # Reload the deployment every time the yaml config changes
  addons:
    vpn:
      <<: *deluge_addons_vpn
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      envFrom:
      - configMapRef:
          name: deluge-pia-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/deluge/torrent_files

        JQ_FILTER=".listen_random_port=false"
        JQ_FILTER="${JQ_FILTER} | .pre_allocate_storage=false"
        JQ_FILTER="${JQ_FILTER} | .stop_seed_ratio=2"
        JQ_FILTER="${JQ_FILTER} | .cache_size=52428"
        JQ_FILTER="${JQ_FILTER} | .share_ratio_limit=2"
        JQ_FILTER="${JQ_FILTER} | .stop_seed_at_ratio=true"

        jq "${JQ_FILTER}" /config/core.conf > /config/core-new.conf
        cp /config/core-new.conf /config/core.conf

        # # Avoid session timeouts
        # sed -i  "s/session_timeout:\".*/session_timeout\": 99999,/" /config/web.conf

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /shared
        name: shared
      env: *bootstrap_env
      securityContext: *default_securitycontext
  additionalContainers:
    deluge-web:
      image: ghcr.io/geek-cookbook/deluge:2.1.1@sha256:448324e342c47020e4e9fbc236282ceb80ebebd7934a486a6f1e487a7e4034bf
      command:
      - /usr/bin/deluge-web
      - -L
      - info
      - -d
      - -c
      - /config
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: deluge
      - mountPath: /tmp
        name: tmp
      env:
        PYTHON_EGG_CACHE: /tmp/.cache
    # Use this to provied proxied access to arrs
    dante:
      image: ghcr.io/elfhosted/dante:v1.4.3
      env: *bootstrap_env
      securityContext: *default_securitycontext
      volumeMounts:
      - mountPath: /tmp
        name: tmp

qbittorrentgluetun: &qbittorrent
  podLabels:
    app.elfhosted.com/name: qbittorrent
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M"
  enabled: false
  automountServiceAccountToken: false
  image:
    registry: ghcr.io
    repository: elfhosted/qbittorrent
    tag: 5.0.2@sha256:7ff09d6a5ca2267f78161fb46eeafaf5b2af7806288fe7f3d2dfed2521374e3d
  priorityClassName: tenant-bulk
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't seem to work well with entrypoint
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-qbittorrent,qbittorrent-gluetun-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    <<: *storagemounts
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_100g
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: qbittorrent
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-qbittorrent
          optional: true
    elfscripts:
      enabled: "true"
      mountPath: "/elfscripts/"
      type: "custom"
      volumeSpec:
        configMap:
          name: qbittorrent-elfscripts
          defaultMode: 0755
    dante-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: dante-config
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
      nameOverride: spanky
  env:
    # -- Set the container timezone
    TZ: UTC
    HOME: /config
    XDG_CONFIG_HOME: /config
    XDG_DATA_HOME: /config
    WAIT_FOR_VPN: "true"
  envFrom:
  - configMapRef:
      name: elfbot-qbittorrent
      optional: true
  extraEnvVars:
  - name: PORT_FILE
    valueFrom:
      configMapKeyRef:
        name: qbittorrent-gluetun-config
        key: PORT_FILE
    optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Remove the lockfile if it exists
        if [[ -f /config/qBittorrent/lockfile ]]; then
          rm /config/qBittorrent/lockfile
        fi

        mkdir -p /config/qBittorrent/torrent_files/complete
        mkdir -p /config/qBittorrent/torrent_files/incomplete

        # Enforce 1:1 seeding ratio, and then delete
        sed -i  "s/Session\\\GlobalMaxRatio=.*/Session\\\GlobalMaxRatio=1/" /config/qBittorrent/qBittorrent.conf

        # Permit TCP only
        sed -i  "s/Session\\\BTProtocol=.*/Session\\\BTProtocol=TCP/" /config/qBittorrent/qBittorrent.conf

        # Disable CSRF protection so that Homer can show qBit stats
        sed -i  "s/WebUI\\\CSRFProtection=.*/WebUI\\\CSRFProtection=false/" /config/qBittorrent/qBittorrent.conf

        # Insist on tun0
        sed -i  "s/Session\\\Interface=.*/Session\\\Interface=tun0/" /config/qBittorrent/qBittorrent.conf
        sed -i  "s/Session\\\InterfaceName=.*/Session\\\InterfaceName=tun0/" /config/qBittorrent/qBittorrent.conf

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /shared
        name: shared
      securityContext: *default_securitycontext
      resources: *default_resources
      envFrom:
      - configMapRef:
          name: qbittorrent-gluetun-config
  additionalContainers:
    # Use this to provied proxied access to arrs
    dante:
      image: ghcr.io/elfhosted/dante:v1.4.3
      env: *bootstrap_env
      securityContext: *default_securitycontext
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /etc/sockd.conf
        name: dante-config
        subPath: sockd.conf
    mam-helper:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |
        set -e
        set -x

        echo "Waiting for VPN to be connected..."
        while ! grep -s -q "connected" /shared/vpnstatus; do
            # Also account for gluetun-style http controller
            if (curl -s http://localhost:8042/v1/openvpn/status | grep -q running); then
                break
            fi
            echo "VPN not connected"
            sleep 2
        done
        echo "VPN Connected, processing cookies..."

        # If we have a cookie already, try to use it
        if [[ -f /config/mam/saved.cookies ]]; then
          curl -c /config/mam/saved.cookies -b /config/mam/saved.cookies https://t.myanonamouse.net/json/dynamicSeedbox.php  -o /config/mam/mam_id-curl-output.log
        fi

        # Now whether that worked or not, look for /config/mam/mam_id
        mkdir -p /config/mam
        while [ 1 ]; do
          if [[ -f /config/mam/mam_id ]]; then
            curl -c /config/mam/saved.cookies -b "mam_id=$(cat /config/mam/mam_id)" https://t.myanonamouse.net/json/dynamicSeedbox.php -o /config/mam/mam_id-curl-output.log
            mv /config/mam/mam_id /config/mam/mam_id_processed
          fi
          sleep 1m
        done
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /shared
        name: shared
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
      ephemeral-storage: 50Mi
    limits:
      cpu: 500m
      memory: 2Gi # .2 GB for headroom
      ephemeral-storage: 100Mi # a safety net against node ephemeral space exhaustion
  addons:
    vpn: &qbittorrent_addons_vpn
      enabled: true
      type: gluetun
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      securityContext:
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      envFrom:
      - configMapRef:
          name: qbittorrent-gluetun-config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
        subPath: qbittorrent
      config: # We have to set this to null so that we can override with our own config

      # The scripts that get run when the VPN connection opens/closes are defined here.
      # The default scripts will write a string to represent the current connection state to a file.
      # Our qBittorrent image has a feature that can wait for this file to contain the word 'connected' before actually starting the application.
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus


# Custom service for pia
qbittorrentpia:
  <<: *qbittorrent
  env:
    PORT_FILE: /config/forwarded-port
    WAIT_FOR_VPN: "true"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-qbittorrent,qbittorrent-pia-config"
  addons:
    vpn:
      <<: *qbittorrent_addons_vpn
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      envFrom:
      - configMapRef:
          name: qbittorrent-pia-config
  additionalContainers:
    # Use this to provied proxied access to arrs
    dante:
      image: ghcr.io/elfhosted/dante:v1.4.3
      env: *bootstrap_env
      securityContext: *default_securitycontext
      volumeMounts:
      - mountPath: /tmp
        name: tmp
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Remove the lockfile if it exists
        if [[ -f /config/qBittorrent/lockfile ]]; then
          rm /config/qBittorrent/lockfile
        fi

        mkdir -p /config/qBittorrent/torrent_files/complete
        mkdir -p /config/qBittorrent/torrent_files/incomplete

        # Enforce 1:1 seeding ratio, and then delete
        sed -i  "s/Session\\\GlobalMaxRatio=.*/Session\\\GlobalMaxRatio=1/" /config/qBittorrent/qBittorrent.conf

        # Permit TCP only
        sed -i  "s/Session\\\BTProtocol=.*/Session\\\BTProtocol=TCP/" /config/qBittorrent/qBittorrent.conf

        # Disable CSRF protection so that Homer can show qBit stats
        sed -i  "s/WebUI\\\CSRFProtection=.*/WebUI\\\CSRFProtection=false/" /config/qBittorrent/qBittorrent.conf

        # Insist on tun0
        sed -i  "s/Session\\\Interface=.*/Session\\\Interface=tun0/" /config/qBittorrent/qBittorrent.conf
        sed -i  "s/Session\\\InterfaceName=.*/Session\\\InterfaceName=tun0/" /config/qBittorrent/qBittorrent.conf

        # If the VPN_ENDPOINT_IP is set, but is not an IP address, then convert it to one
        if [[ ! -z "$VPN_ENDPOINT_IP" ]]
          then
          if echo "$VPN_ENDPOINT_IP" | egrep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
          then
              echo "No changes, VPN_ENDPOINT_IP is an IP"
          else
              # only create the file if DNS lookup succeeds
              dig +short $VPN_ENDPOINT_IP && dig +short $VPN_ENDPOINT_IP | tail -n1 > /shared/VPN_ENDPOINT_IP
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: qbittorrent
      - mountPath: /shared
        name: shared
      securityContext: *default_securitycontext
      resources: *default_resources

nzbget:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/nzbget
    tag: 25.0@sha256:fbccdb14e86fdcae05aa0d6f38a3b7e188a1caed40f07f94a141eb936ce149c3
  priorityClassName: tenant-bulk
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-nzbget"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: nzbget
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_500g
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-nzbget
          optional: true

  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6789
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: nzbget
      - mountPath: /tmp
        name: tmp

sabnzbd:
  enabled: false
  hostname: sabnzbd # required to prevent whitelisting requirement per https://sabnzbd.org/wiki/extra/hostname-check.html
  podLabels:
    app.elfhosted.com/class: nzb
  image:
    registry: ghcr.io
    repository: elfhosted/sabnzbd
    tag: 4.3.3@sha256:af2ef54052d0d340064997aeb76bb8e612f3b47a8a0fc5c446e821a8bacd80cc
  priorityClassName: tenant-bulk
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-sabnzbd"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: sabnzbd
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    tmp:
      enabled: true
      type: custom
      mountPath: /tmp
      volumeSpec: *volumespec_ephemeral_volume_500g
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-sabnzbd
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: sabnzbd
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # fix host_whitelist
        sed -i  's/goldilocks/{{ .Release.Name }}/g' /config/sabnzbd.ini

        # If we've previously backed up a queue, then restore it to /tmp
        files=$(shopt -s nullglob dotglob; echo /config/queue-backup/*)
        if (( ${#files} ))
        then
          cp /config/queue-backup/* /tmp/ -rfp
          rm -rf /config/queue-backup
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: sabnzbd
      - mountPath: /tmp
        name: tmp
      env: *bootstrap_env
      securityContext: *default_securitycontext
      resources: *default_resources
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1500m # if par threads is 1, this leaves 0.5cpu for downloading
      memory: 1500Mi
  additionalContainers:
    backup-queue:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |
        set -e
        IFS=$'\n' # in case of paths with spaces (looking at you, Plex!)

        function backupqueue_on_shutdown {
            echo "Received SIGTERM, waiting 5s for app to shut down..."
            mkdir -p /config/queue-backup
            sleep 5s

            # sync any files < 1MB
            cd /tmp
            find ./ -type f -size -1024k | rsync -avr --files-from=- /tmp /config/queue-backup
        }

        # When we terminate, perform the backup
        trap backupqueue_on_shutdown SIGTERM

        # Hang around doing nothing until terminated
        while true
        do
            echo "Waiting for SIGTERM to backup queue from /tmp"
            sleep infinity
        done
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: sabnzbd
      - mountPath: /tmp
        name: tmp
      env: *bootstrap_env
      securityContext: *default_securitycontext
      resources: *default_resources

  env:
    HOST_WHITELIST_ENTRIES: "{{ .Release.Name }}.sabnzbd.elfhosted.com"
    SABNZBD_UID: 568
    SABNZBD_GID: 568

tautulli:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/tautulli
    tag: 2.15.2@sha256:214a0ef60363bf81722612732eb62830366fc2cbdbd75e39c4acca61aaae6f20
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-tautulli"
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: tautulli
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-tautulli
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8181
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: tautulli
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/sh
      - -c
      - |
        set -x
        set -e

        # Clear out logs older than 24h
        if [ -d "/config/logs" ]; then
            # Find and delete files older than 7 days
            find "/config/logs" -type f -mtime +1 -exec rm -f {} \;
            echo "Files older than 1 day have been removed from /config/logs."
        fi

        # Clear out backups older than 2d
        if [ -d "/config/backups" ]; then
            # Find and delete files older than 2 days
            find "/config/backups" -type f -mtime +2 -exec rm -f {} \;
            echo "Files older than 1 day have been removed from /config/backups."
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: tautulli
      resources: *default_resources
      securityContext: *default_securitycontext

radarr: &app_radarr
  enabled: false
  podLabels:
    app.elfhosted.com/name: radarr
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/radarr
    tag: 5.25.0.10024@sha256:1ec946cdd3a748cde3678d3970925a9e1b399ccfdf32278a4eb0b0f8a22686c3
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-radarr,radarr-env" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: radarr-env
  - configMapRef:
      name: elfbot-radarr
      optional: true       
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: radarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    mediacover:
      enabled: true
      type: custom
      mountPath: /config/MediaCover
      subPath: radarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: mediacovers          
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: radarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: radarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-radarr
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory          
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 7878
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: radarr
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault    
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: radarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/radarr
        mkdir -p /storage/symlinks/movies

        # for database to use 
        mkdir -p /config/postgresql/database

        # if /config/MediaCover exists (on the config volume), purge it, since this is now handled on a dedicated volume
        if [ -d /config/MediaCover ]; then
          rm -rf /config/MediaCover
        fi

      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      - mountPath: /config
        name: config
        subPath: radarr        
      resources: *default_resources
      securityContext: *default_securitycontext
        
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: radarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: radarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: radarr/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 2
          memory: 8Gi     
    database-backup:
      image: ghcr.io/elfhosted/radarr:5.25.0.10024@sha256:1ec946cdd3a748cde3678d3970925a9e1b399ccfdf32278a4eb0b0f8a22686c3
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: radarr-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: radarr/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi
  resources:
    requests:
      cpu: 0m
      memory: 500Mi
    limits:
      cpu: 600m
      memory: 6Gi

radarr4k: &app_radarr4k
  enabled: false
  podLabels:
    app.elfhosted.com/name: radarr4k
    app.elfhosted.com/class: debrid
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/radarr
    tag: 5.25.0.10024@sha256:1ec946cdd3a748cde3678d3970925a9e1b399ccfdf32278a4eb0b0f8a22686c3
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-radarr4k,radarr4k-env" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: radarr4k-env
  - configMapRef:
      name: elfbot-radarr4k
      optional: true       
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: radarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    mediacover:
      enabled: true
      type: custom
      mountPath: /config/MediaCover
      subPath: radarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: mediacovers            
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-radarr4k
          optional: true
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: radarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: radarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory          
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 7878
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config/ -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi
        
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: radarr4k
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault      
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: radarr4k
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/radarr4k
        mkdir -p /storage/symlinks/movies-4k

        # for database to use 
        mkdir -p /config/postgresql/database        

        # if /config/MediaCover exists (on the config volume), purge it, since this is now handled on a dedicated volume
        if [ -d /config/MediaCover ]; then
          rm -rf /config/MediaCover
        fi
        
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      - mountPath: /config
        name: config
        subPath: radarr4k        
      resources: *default_resources
      securityContext: *default_securitycontext    
  resources:
    requests:
      cpu: 0m
      memory: 500Mi
    limits:
      cpu: 2
      memory: 6Gi
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: radarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: radarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: radarr4k/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi   
    database-backup:
      image: ghcr.io/elfhosted/radarr:5.25.0.10024@sha256:1ec946cdd3a748cde3678d3970925a9e1b399ccfdf32278a4eb0b0f8a22686c3
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: radarr4k-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: radarr4k/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi            

ombi:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/ombi
    tag: 4.47.1@sha256:7346137897633f5cb3cd7d26922782f68791201e317bb3051d3f79d25242c54e
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-ombi"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: ombi
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-ombi
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5000
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: ombi
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 3m
      memory: 150Mi
    limits:
      cpu: 2
      memory: 1Gi

scannarr: &app_scannarr
  enabled: false
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/scannarr
    tag: rolling@sha256:a0d7e6f755ef8f9f3cc61d2cd7f540acd81acb39c0431e6cae91030a02a3c7ff
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-scannarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    sonarr-settings:
      enabled: "true"
      mountPath: "/app/settings_sonarr.json"
      subPath: "settings_sonarr.json"
      type: "custom"
      volumeSpec:
        configMap:
          name: scannarr-config
    radarr-settings:
      enabled: "true"
      mountPath: "/app/settings_radarr.json"
      subPath: "settings_radarr.json"
      type: "custom"
      volumeSpec:
        configMap:
          name: scannarr-config
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true
      ports:
        http:
          port: 9898 # doesn't matter this doesn,t actually use ports
  additionalContainers:
    podinfo:
      image: stefanprodan/podinfo # used to run probes from gatus
  resources: *default_resources

scannarr4k:
  <<: *app_scannarr
  enabled: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-scannarr4k"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
    sonarr-settings:
      enabled: "true"
      mountPath: "/app/settings_sonarr.json"
      subPath: "settings_sonarr.json"
      type: "custom"
      volumeSpec:
        configMap:
          name: scannarr4k-config
    radarr-settings:
      enabled: "true"
      mountPath: "/app/settings_radarr.json"
      subPath: "settings_radarr.json"
      type: "custom"
      volumeSpec:
        configMap:
          name: scannarr4k-config

huntarr: &app_huntarr
  enabled: false
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/huntarr
    tag: 7.4.3@sha256:dcdcff344d5fc3df3433b51ba2905b8024a0d3c23f3d5fe46da096c2842ecedd
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-huntarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: huntarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config  
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: huntarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs   
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: huntarr-config   
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-huntarr
          optional: true
    app-frontend-static-data:
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mountPath: /app/frontend/static/data      
  initContainers:  
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/settings/
        if ! find /config/settings/ -maxdepth 1 -name '*.json' | grep -q .; then
          cp /bootstrap/*.json /config/settings/
          mkdir -p /config/user
          mv /config/settings/credentials.json /config/user/credentials.json
        fi
      volumeMounts:
      - mountPath: /config/
        name: config
        subPath: huntarr
      - name: example-config
        mountPath: "/bootstrap/"  
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: huntarr
      - mountPath: /tmp
        name: tmp                         
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true
      ports:
        http:
          port: 9705
  resources: *default_resources

bazarr:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/bazarr
    tag: 1.5.2@sha256:c44714dd7504d72e32d4937ebae0d616738574423b3e319a74dddbeb67755c5a
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-bazarr"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: bazarr-config
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: bazarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-bazarr
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6767
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: bazarr
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

bazarr4k:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/bazarr
    tag: 1.5.2@sha256:c44714dd7504d72e32d4937ebae0d616738574423b3e319a74dddbeb67755c5a
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-bazarr4k"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: bazarr4k-config
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: bazarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-bazarr4k
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6767
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: bazarr4k
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

filebrowser:
  hostname: elfhosted
  enabled: true
  podLabels:
    app.elfhosted.com/name: filebrowser
  image:
    repository: ghcr.io/elfhosted/filebrowser
    tag: 2.23.0@sha256:296e3a3d08c5ca07a26350358fa2e58a597a41adb253a65bba27b557f36383e5
  podAnnotations:
    kubernetes.io/egress-bandwidth: "5M" # filebrowser is not for streaming
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  envFrom:
  - configMapRef:
      name: filebrowser-env
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  deploymentStrategy:
    type: Recreate
    rollingUpdate: null
  controller:
    replicas: 1 # not sure we need 2 replicas anymore
    strategy: Recreate
    # rollingUpdate:
    #   unavailable: 1
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,filebrowser-elfbot-script,elfbot-filebrowser" # Reload the deployment every time the rclones change
  # We will use this to alter configmaps to trigger pod restarts
  serviceAccount:
    create: true
    name: filebrowser
  automountServiceAccountToken: true
  persistence:
    <<: *storagemounts  
    backup:
      enabled: true
      type: custom
      mountPath: /storage/backup
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    config:
      enabled: true
      type: custom
      mountPath: /storage/config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /storage/logs
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    elfterm-state: # so auto-provisioning doesn't break
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mountPath: /home/elfie/.local/state
    dummy-storage: # so auto-provisioning doesn't break
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
    elfbot:
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mountPath: /elfbot
    elfbot-script:
      enabled: "true"
      mountPath: "/usr/local/bin/elfbot"
      subPath: "elfbot"
      type: "custom"
      volumeSpec:
        configMap:
          name: filebrowser-elfbot-script
          defaultMode: 0755
    elfbot-script-ucfirst:
      enabled: "true"
      mountPath: "/usr/local/bin/Elfbot" # make it easier for mobile users
      subPath: "elfbot"
      type: "custom"
      volumeSpec:
        configMap:
          name: filebrowser-elfbot-script
          defaultMode: 0755
    recyclarr-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: recyclarr-config
    symlinks: *symlinks
    tmp: *tmp
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080 # this allows us to run as non-root

  ingress:
    main:
      enabled: false
  initContainers:
    setup:
      image: ghcr.io/elfhosted/filebrowser:2.23.0@sha256:296e3a3d08c5ca07a26350358fa2e58a597a41adb253a65bba27b557f36383e5
      # 2.23.0@sha256:1db0f0114a169ea2a877d75c47903a6d01534340421948845d5e298c7ac7ceb4 is the last good version for TFA
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Delete tmp db if necessary
        if [ -f /tmp/filebrowser.db ]
        then
          rm /tmp/filebrowser.db
        fi


        /filebrowser config init \
          --disable-preview-resize \
          --disable-thumbnails \
          --disable-type-detection-by-header \
          --branding.name="{{ .Release.Name }}, by ElfHosted 🧝 " \
          --branding.files=/branding \
          --branding.disableExternal \
          --auth.method=noauth \
          --lockPassword \
          --database /tmp/filebrowser.db \
          --root /storage \
          --cache-dir /tmp

        # allow zip, unzip, rar, unrar, ls, pwd, cd, mv
        /filebrowser config set --database /tmp/filebrowser.db --commands zip,unzip,rar,unrar,ls,pwd,cd,mv,cp,ln,find,echo,grep,cat,touch,tar,gzip,rm,tree,du,mlocate,updatedb,locate,elfbot,Elfbot
        # /filebrowser config set --database /tmp/filebrowser.db --shell 'vstat -c'

        # now tell filebrowser about the user (who gets authenticated via the proxy)
        /filebrowser users add 1 bogus --database /tmp/filebrowser.db

      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /storage
        name: dummy-storage
      resources: *default_resources
      securityContext: *default_securitycontext
    copy-recyclarr-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e
        # If we don't already have an example config, create one
        if [ ! -f /config/recyclarr.yaml ];
        then
          cp /bootstrap/recyclarr.yaml /config/
        fi
      volumeMounts:
      - mountPath: /config/
        name: config
        subPath: recyclarr
      - name: recyclarr-config
        mountPath: "/bootstrap/"
      securityContext: *default_securitycontext     
  additionalContainers:
    # this container exists to watch for restarts requested by elfbot, and to use create configmaps to trigger restarts using reloader
    elfbot:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |
        # respond to creation or modification, but not deletion
        inotifywait -m -e create -e modify --format "%f" /elfbot \
          | while read APP
            do
              # if we are force-killing the pod, then don't bother with the configmap
              if (cat /elfbot/$APP | grep -q forcerestart); then
                echo "forcerestart requested, deleting $APP pod with --force.."
                kubectl delete pod -l app.kubernetes.io/name=$APP --force
                kubectl delete pod -l app.elfhosted.com/name=$APP --force
              else

                # put the contents of the file into the configmap which will trigger the restart
                echo command received for ${APP} : [$(cat /elfbot/$APP)]
                # create the configmap if it doesn't exist, since reloader only looks at _changes_ to configmaps
                if ! $(kubectl get configmap -n {{ .Release.Namespace }} elfbot-${APP} 2>&1 >/dev/null); then
                    kubectl create configmap -n {{ .Release.Namespace }} elfbot-${APP} --from-literal=elfbot_last_action=$(date +%s)
                    sleep 10s
                fi

                # If we were passed a key=value string in /etc/elfbot, then split it
                COMMAND=$(cat /elfbot/$APP)

                # We separate key and value with an '=', but sometimes the value may contain __another__ '=' (like Plex preferences)
                sep='='
                case $COMMAND in
                  # If we are separated by an =
                  (*"$sep"*)
                    KEY=${COMMAND%%"$sep"*}
                    VALUE=${COMMAND#*"$sep"}
                    ;;
                  # if not, we are a simple command like "backup"
                  (*)
                    KEY=$COMMAND
                    VALUE=$(date +%s)
                    ;;
                esac


                # patch the configmap with the latest key/value
                kubectl patch configmap -n {{ .Release.Namespace }} elfbot-${APP} -p "{\"data\":{\"${KEY}\":\"${VALUE}\"}}"
              fi
            done
      volumeMounts:
      - mountPath: /elfbot
        name: elfbot
      resources: *default_resources
      securityContext: *default_securitycontext
  resources:
    requests:
      cpu: 0m
      memory: 6Mi
    limits:
      cpu: 1
      memory: 1Gi


uptimekuma:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/uptime-kuma
    tag: 1.23.16@sha256:a8f477d080a481ddb08f66d3b9021ef2842bda942aedeed59ee1839713b63876
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-uptimekuma"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/data/
      subPath: uptimekuma
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-uptimekuba
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: uptimekuma
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 100m
      memory: 1Gi

privatebin:
  enabled: false
  image:
    repository: privatebin/fs
    tag: 1.7.6
  priorityClassName:
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-privatebin"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # crashes privatebin, TBD to determine why, and whether an emptydir /tmpfs might help
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /srv/data
      subPath: privatebin
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-privatebin
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
      ephemeral-storage: 50Mi
    limits:
      cpu: 100m
      memory: 128Mi
      ephemeral-storage: 100Mi # a safety net against node ephemeral space exhaustion
  config:
    main:
      discussion: false
      opendiscussion: false
      password: true
      fileupload: true
      burnafterreadingselected: false
      defaultformatter: "plaintext"
      syntaxhighlightingtheme: "sons-of-obsidian"
      sizelimit: 1048576
      template: "bootstrap-dark"
      info: "Hosted with ❤️ by ElfHosted 🧝"
      languageselection: true
      languagedefault: "en"
      # urlshortener: "https://shortener.example.com/api?link="
      qrcode: false
      icon: "none"
      zerobincompatibility: false
      # httpwarning: true
      compression: "zlib"
    expire:
      default: "1week"
    expire_options:
      5min: 300
      10min: 600
      1hour: 3600
      1day: 86400
      1week: 604800
    formatter_options:
      plaintext: "Plain Text"
      syntaxhighlighting: "Source Code"
      markdown: "Markdown"
    traffic:
      limit: 10
      # exemptedIp: "1.2.3.4,10.10.10/24"

kapowarr:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/kapowarr
    tag: 1.2.0@sha256:469ef44ffbd9514f0fa22facfbc5b35d4ff47713f98ce3a77e6988ab11984858
  priorityClassName:
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-kapowarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # breaks kapowarr
    allowPrivilegeEscalation: false
    runAsUser: 568
    runAsGroup: 568
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  probes:
    liveness:
      enabled: false
    startup:
      enabled: false
    readiness:
      enabled: false
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: kapowarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    temp-downloads:
      enabled: true
      type: emptyDir
      mountPath: /app/temp_downloads
      sizeLimit: 50Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-kapowarr
          optional: true
    tmp: *tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5656
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 2Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: kapowarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/comics
        mkdir -p /storage/symlinks/comics

      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      resources: *default_resources
      securityContext: *default_securitycontext


calibreweb:
  enabled: false
  podLabels:
    app.elfhosted.com/name: calibre-web
  priorityClassName: tenant-normal  
  image:
    repository: ghcr.io/elfhosted/calibre-web-automated
    tag: v3.0.4@sha256:5b81a6f32f2947c7ed82b47afda8289e92158f9d19c9ea739fa30de98cecfa8a
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-calibre-web"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
    DOCKER_MODS: linuxserver/mods:universal-calibre
  envFrom:
  - configMapRef:
      name: elfbot-calibre-web
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: calibre-web
      volumeSpec:
        persistentVolumeClaim:
          claimName: config     
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-calibre-web
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 8083
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: calibre-web
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # There are symlinks pre-prepared for these
        mkdir -p /config/calibre-library
        mkdir -p /config/cwa-book-ingest

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: calibre-web
      resources: *default_resources
      securityContext: *default_securitycontext        
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

cwabookdownloader:
  enabled: false
  podLabels:
    app.elfhosted.com/name: cwa-book-downloader
  priorityClassName: tenant-normal  
  image:
    repository: ghcr.io/elfhosted/cwa-downloader
    tag: rolling@sha256:e0309a3197956ce4556edbfd6fc280d665ea88dcbb23bd9a5593cf29a9357fad
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-cwa-book-downloader"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
  envFrom:
  - configMapRef:
      name: elfbot-cwa-book-downloader
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /cwa-book-ingest/
      subPath: calibre-web/cwa-book-ingest
      volumeSpec:
        persistentVolumeClaim:
          claimName: config     
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-cwa-book-downloader
          optional: true
    logs:
      enabled: true
      type: custom
      mountPath: /var/log/cwa-book-downloader/
      subPath: cwa-book-downloader
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs   
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 8084
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: calibre-web-book-downloader
      - mountPath: /tmp
        name: tmp      
  additionalContainers:
    cloudflarebypassforscraping:
      image: ghcr.io/elfhosted/cloudflarebypassforscraping@sha256:7bdf614b57f57e47a6cecdaf6048869037123e0731456ed186b5430d6bfbefad
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

lazylibrarian:
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/lazylibrarian
    tag: rolling@sha256:5095b4270cd7132983e6b6b1088a552d20051c2cf0f1f7e265c733c2af899e00
  enabled: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-lazylibrarian"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    runAsUser: 568
    runAsGroup: 568
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: lazylibrarian
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-lazylibrarian
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 5299
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: lazylibrarian
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 96Mi
    limits:
      cpu: 1
      memory: 1Gi

mylar:
  enabled: false
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/mylar3
    tag: 0.8.2@sha256:89fdca5cc80457bde3373a6a7d69631487c41dff1e15d34bf78391eae85df0d9
  env:
    PUID: 568
    PGID: 568
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mylar" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: mylar
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-mylar
          optional: true

  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8090
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: mylar
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

komga:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/komga
    tag: 1.21.3@sha256:e00ee31324378812bc05ac80de958d1db26510c452c4b9f43bb84b87b6f6e130
  env:
    KOMGA_CONFIGDIR: /config
    KOMGA_REMEMBERME_KEY: yesplease
    JAVA_TOOL_OPTIONS: -Xmx2g
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-komga" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: komga
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-komga
          optional: true

  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 25600
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: komga
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 2Gi

kavita:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/kavita
    tag: 0.8.6@sha256:d11af52fe8fd0e1ba3579cfa7d85d2dd8be9cdb21eb6016cd83bc164a3fde57c
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-kavita" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /kavita/config
      subPath: kavita
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-kavita
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: kavita
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 5000
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 2
      memory: 1Gi

calibre:
  enabled: false
  # runtimeClassName: kata
  image:
    repository: quay.io/linuxserver.io/calibre
    tag: 8.4.0@sha256:f17b542ed5116513d6be65667f4e35826b99738d4f1ebccba6c85e81c011c2f7
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with s6
    allowPrivilegeEscalation: false # do we need this too?
    # runAsUser: 568
    # runAsGroup: 568
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-calibre"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: calibre
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-calibre
          optional: true
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
  env:
    PUID: 568
    PGID: 568
    TITLE: Calibre | ElfHosted
    START_DOCKER: false
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 8080
  resources:
    requests:
      cpu: 0m
      memory: 1Gi
    limits:
      cpu: 1
      memory: 4Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: calibre
      - mountPath: /tmp
        name: tmp
  envFrom:
  - configMapRef:
      name: elfbot-calibre
      optional: true

sonarr: &app_sonarr
  enabled: false
  podLabels:
    app.elfhosted.com/name: sonarr
    app.elfhosted.com/class: debrid
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/sonarr
    tag: 4.0.14.2939@sha256:8f0707281a5388dc6d57c371ce9951b2863a2cb22205f21716c6660f1f9b7ff5
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-sonarr,sonarr-env" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: sonarr-env
  - configMapRef:
      name: elfbot-sonarr
      optional: true       
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: sonarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    mediacover:
      enabled: true
      type: custom
      mountPath: /config/MediaCover
      subPath: sonarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: mediacovers            
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: sonarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: sonarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-sonarr
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory      
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8989
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: sonarr
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault    
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: sonarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/sonarr
        mkdir -p /storage/symlinks/series

        # for database to use 
        mkdir -p /config/postgresql/database        

        # if /config/MediaCover exists (on the config volume), purge it, since this is now handled on a dedicated volume
        if [ -d /config/MediaCover ]; then
          rm -rf /config/MediaCover
        fi
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      - mountPath: /config
        name: config
        subPath: sonarr
      resources: *default_resources
      securityContext: *default_securitycontext     
  resources:
    requests:
      cpu: 0m
      memory: 600Mi
    limits:
      cpu: 2
      memory: 6Gi
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: sonarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: sonarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: sonarr/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 2Gi         
    database-backup:
      image: ghcr.io/elfhosted/sonarr:4.0.14.2939@sha256:8f0707281a5388dc6d57c371ce9951b2863a2cb22205f21716c6660f1f9b7ff5
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: sonarr-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: sonarr/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi            

sonarr4k: &app_sonarr4k
  enabled: false
  podLabels:
    app.elfhosted.com/name: sonarr4k
    app.elfhosted.com/class: debrid
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/sonarr
    tag: 4.0.14.2939@sha256:8f0707281a5388dc6d57c371ce9951b2863a2cb22205f21716c6660f1f9b7ff5
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-sonarr4k,sonarr4k-env" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: sonarr4k-env
  - configMapRef:
      name: elfbot-sonarr4k
      optional: true      
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: sonarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    mediacover:
      enabled: true
      type: custom
      mountPath: /config/MediaCover
      subPath: sonarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: mediacovers            
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: sonarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: sonarr4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-sonarr4k
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory               
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8989
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: sonarr4k
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault     
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: sonarr4k
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for symlink downloads and imports
        mkdir -p /storage/symlinks/downloads/sonarr4k
        mkdir -p /storage/symlinks/series-4k

        # for database to use 
        mkdir -p /config/postgresql/database        
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      - mountPath: /config
        name: config
        subPath: sonarr4k     
      resources: *default_resources
      securityContext: *default_securitycontext       
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 2
      memory: 6Gi # reduce once sqlite-to-db-migration is done
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: sonarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: sonarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: sonarr4k/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi 
    database-backup:
      image: ghcr.io/elfhosted/sonarr:4.0.14.2939@sha256:8f0707281a5388dc6d57c371ce9951b2863a2cb22205f21716c6660f1f9b7ff5
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: sonarr4k-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: sonarr4k/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi             
  probes: # need a long startup for database migrations
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 3000
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 8989
        timeoutSeconds: 1

resiliosync:
  service:
    main:
      enabled: false
  command:
  - rslsync
  - --config
  - /sync.conf
  - --nodaemon
  enabled: false
  priorityClassName: tenant-bulk
  image:
    repository: ghcr.io/elfhosted/resilio-sync
    tag: 3.0.3.1065-1@sha256:544fe39e56b2beef67a0266f155a144ddcce20d5742a71b6113c7f1e4d9b6c28
  env:
    PUID: 568
    GUID: 568
    S6_READ_ONLY_ROOT: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # another s6 containeir!
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-resiliosync"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: resiliosync
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    setup-config:
      enabled: "true"
      mountPath: "/sync.conf"
      subPath: "sync.conf"
      type: "custom"
      volumeSpec:
        configMap:
          name: resiliosync-config
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-resiliosync
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: resiliosync
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi

prowlarr: &app_prowlarr
  enabled: false
  podLabels:
    app.elfhosted.com/name: prowlarr
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/prowlarr-develop
    tag: 1.36.3.5071@sha256:38638743d41be23f593aecd8c88bec14fcbc2a66b33d4fc3c55969a6d7cb8c68
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-prowlarr,prowlarr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: prowlarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: prowlarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: prowlarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-prowlarr
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory      
  envFrom:
  - configMapRef:
      name: prowlarr-env  
  - configMapRef:
      name: elfbot-prowlarr
      optional: true                   
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9696
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: prowlarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/sh
      - -c
      - |
        set -x
        set -e
        # Set auth to external
        sed -i  "s|<AuthenticationMethod>None</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml

        # for database to use 
        mkdir -p /config/postgresql/database

        # Clear out logs older than 24h
        if [ -d "/config/logs" ]; then
            # Find and delete files older than 7 days
            find "/config/logs" -type f -mtime +1 -exec rm -f {} \;
            echo "Files older than 1 day have been removed from /config/logs."
        fi

        # Get custom indexers
        mkdir -p /config/Definitions/Custom

        curl https://raw.githubusercontent.com/elfhosted/prowlarr-indexers/main/Custom/elfcomet.yml > /config/Definitions/Custom/elfcomet.yml

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: prowlarr
      resources: *default_resources
      securityContext: *default_securitycontext
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: prowlarr
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault       
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 6Gi
  env:
    S6_READ_ONLY_ROOT: 1
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: postgres
        - name: POSTGRES_DB
          value: prowlarr
        - name: POSTGRES_USER
          value: prowlarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: prowlarr/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi     
    database-backup:
      image: ghcr.io/elfhosted/prowlarr-develop:1.36.3.5071@sha256:38638743d41be23f593aecd8c88bec14fcbc2a66b33d4fc3c55969a6d7cb8c68 
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: prowlarr-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: prowlarr/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi
  probes: # need a long startup for database migrations, until we can bootstrap an entire database
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 3000
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 9696
        timeoutSeconds: 1  
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 300
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 9696
        timeoutSeconds: 1  
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 300
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 9696
        timeoutSeconds: 1                      


nzbhydra:
  enabled: false
  podLabels:
    app.elfhosted.com/name: nzbhydra
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/nzbhydra2
    tag: 7.13.2@sha256:8c2b2eb7203ca1e289a574265356e0551f060837c2d1fd75bb9d61a768b78182
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-nzbhydra"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: nzbhydra
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: nzbhydra
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-nzbhydra
          optional: true          
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5076
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: nzbhydra
      - mountPath: /tmp
        name: tmp
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi

lidarr:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/lidarr-develop
    tag: 2.12.1.4636@sha256:9420b7dce0dd7fe797fca42dbab7fa28a2f54e27f9c37194c77b9dcc3cf1fc9a  
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    # readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-lidarr" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: lidarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    s6:
      enabled: true
      type: emptyDir
      mountPath: /var/run/s6
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-lidarr
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory              
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8686
  envFrom:
  - configMapRef:
      name: lidarr-env
  - configMapRef:
      name: elfbot-lidarr
      optional: true             
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: lidarr
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault     
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: lidarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for database to use 
        mkdir -p /config/postgresql/database

        # Set auth to external
        sed -i  "s|<AuthenticationMethod>None</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml
        # Clean up wasteful temporary mediacover storage (Radarr will just re-download these)

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: lidarr
      resources: *default_resources
      securityContext: *default_securitycontext
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: lidarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: lidarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: lidarr/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi     
    database-backup:
      image: ghcr.io/elfhosted/lidarr-develop:2.12.1.4636@sha256:9420b7dce0dd7fe797fca42dbab7fa28a2f54e27f9c37194c77b9dcc3cf1fc9a  
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: lidarr-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: lidarr/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi      
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 1Gi

navidrome:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/navidrome
    tag: 0.55.2@sha256:b92b28e67e03ff8ed7850dbf6c3e589ccb070098b2794a937e3cb4a6e9d2c505
  priorityClassName: tenant-streaming
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-navidrome"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  env:
    ND_MUSICFOLDER: /tmp
    ND_DATAFOLDER: /config
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: navidrome
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-navidrome
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: navidrome
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      ports:
        http:
          port: 4533
  resources:
    requests:
      cpu: 0m
      memory: 32Mi
    limits:
      cpu: 2
      memory: 1Gi

readarr:
  enabled: false
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/readarr-develop
    tag: 0.4.16.2793@sha256:bdc57258692b487aff340e0f60c12692d2628820490fc2dbe6e0284ccb1879ee    
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-readarr" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: readarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: readarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    tmp-readarr-backup:
      enabled: true
      type: emptyDir
      mountPath: /tmp/readarr_backup
      sizeLimit: 32Mi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-readarr
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: readarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup        
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8787
  envFrom:
  - configMapRef:
      name: readarr-env
  - configMapRef:
      name: elfbot-readarr
      optional: true           
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: readarr
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault     
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: readarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for database to use 
        mkdir -p /config/postgresql/database

        # Set auth to external
        sed -i  "s|<AuthenticationMethod>None</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml
        sed -i  "s|<AuthenticationMethod>Basic</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: readarr
      resources: *default_resources
      securityContext: *default_securitycontext
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: readarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: readarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: readarr/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi     
    database-backup:
      image: ghcr.io/elfhosted/readarr-develop:0.4.16.2793@sha256:bdc57258692b487aff340e0f60c12692d2628820490fc2dbe6e0284ccb1879ee   
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: readarr-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: readarr/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi        
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 2
      memory: 16Gi
  probes: # need a long startup for database migrations
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 3000
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 8787
        timeoutSeconds: 1      

readarraudio:
  enabled: false
  priorityClassName: tenant-normal
  image:
    registry: ghcr.io
    repository: elfhosted/readarr-develop
    tag: 0.4.16.2793@sha256:bdc57258692b487aff340e0f60c12692d2628820490fc2dbe6e0284ccb1879ee   
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: readarr-env
  - configMapRef:
      name: elfbot-readarr
      optional: true    
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-readarraudio" # Reload the deployment every time the rclones change
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: readarraudio
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: readarraudio
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    tmp-readarr-backup:
      enabled: true
      type: emptyDir
      mountPath: /tmp/readarr_backup
      sizeLimit: 32Mi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-readarraudio
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory          
    backup:
      enabled: true
      type: custom
      mountPath: /config/Backups
      subPath: readarraudio
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup        
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8787
  initContainers:
    a-fix-permissions:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        chown elfie:elfie /config -R

        # Wipe database if it's on postgresql 16
        if [ -f /config/postgresql/database/PG_VERSION ]; then
          if grep 16 /config/postgresql/database/PG_VERSION; then
            rm -rf /config/postgresql/database/*
          fi
        fi

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: readarraudio
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault     
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: readarraudio
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # for database to use 
        mkdir -p /config/postgresql/database

        # Set auth to external
        sed -i  "s|<AuthenticationMethod>None</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml
        sed -i  "s|<AuthenticationMethod>Basic</AuthenticationMethod>|<AuthenticationMethod>External</AuthenticationMethod>|" /config/config.xml
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: readarraudio
      resources: *default_resources
      securityContext: *default_securitycontext
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: readarr
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_USER
          value: readarr
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: readarr/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi     
    database-backup:
      image: ghcr.io/elfhosted/readarr-develop:0.4.16.2793@sha256:bdc57258692b487aff340e0f60c12692d2628820490fc2dbe6e0284ccb1879ee   
      command: [ "/database-backup.sh" ]
      envFrom:
      - configMapRef:
          name: readarr-env
      volumeMounts:
      - mountPath: /backup
        name: backup
        subPath: readarr/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 100m
          memory: 1Gi         
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 16Gi
  probes: # need a long startup for database migrations
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 3000
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 8787
        timeoutSeconds: 1      

plex: &app_plex
  enabled: false
  priorityClassName: tenant-streaming
  podLabels:
    app.elfhosted.com/name: plex
    app.elfhosted.com/class: debrid
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M" # tested with _kilos in Discord on a 97Mbit remux
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    runAsUser: 568
    runAsGroup: 568    
    # fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex,elfbot-imagemaid,plex-config,imagemaid-env,plex-tinyproxy-conf" # Reload the deployment every time the rclones change
  image:
    registry: ghcr.io
    repository: elfhosted/plex
    tag: 1.41.7.9823-59f304c16@sha256:04b14748f9a3e5d23e1431ab2d4c5c07aca14a1fe5541b62481f0064eea8420e
  persistence: &app_plex_persistence
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-1g    
    phototranscoder:
      enabled: true
      mountPath: /phototranscoder
      type: emptyDir
      sizeLimit: 50Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex
          optional: true
    render-device: &streamer_render_device
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    tinyproxy-conf:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: plex-tinyproxy-conf         
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 32400
  envFrom:
  - configMapRef:
      name: plex-config
  - configMapRef:
      name: elfbot-plex
      optional: true
  resources:
    requests:
      cpu: "100m"
      memory: 1Gi
    limits:
      cpu: "2" # 1.5 works, but results in buffering when playback starts, see https://github.com/elfhosted/charts/issues/501
      memory: 4Gi
  initContainers:
    restart-with-zurg:
      image: ghcr.io/elfhosted/zurg-rc:2025.05.14.0031-nightly@sha256:513bcc2be2a9fb8b8df7fc458f76747bebcc884776bfbfd4393e16d61b2c1227
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when zurg is updated"
    restart-with-decypharr:
      image: ghcr.io/elfhosted/decypharr-beta:rolling@sha256:b310afa9976f950ab62d7ec20801b46cb9a28176121d1c275424d780825ca4ee
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when decypharr is updated"
    restart-with-debridav:
      image: ghcr.io/elfhosted/debridav:0.9.5@sha256:1e596ddb97a7e6a4827b1dfc0718992b0c432416841a8a4c3dd3cd7491850c3c
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when debridav is updated"        
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # Clean up wasteful temporary media storage (Plex will just re-download these)
        if [ -d "/config/Library/Application Support/Plex Media Server/Cache/PhotoTranscoder" ]; then
          rm -rf "/config/Library/Application Support/Plex Media Server/Cache/PhotoTranscoder"
          ln -s /phototranscoder '/config/Library/Application Support/Plex Media Server/Cache/PhotoTranscoder'
        fi
        
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: plex
      - mountPath: /phototranscoder
        name: phototranscoder
      # can't use default resources because the ephemeral limit kicks out /phototranscoder later
      # resources: *default_resources
      securityContext: *default_securitycontext 
    setup-warp:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      env:
        CHANGE_CONTAINER_PORTS: "True"
        HTTP_PORT: "3002"
        HTTPS_PORT: "3003"
      securityContext: *speedtest_securitycontext
    tinyproxy:
      image: ghcr.io/elfhosted/tinyproxy:v1.4.3@sha256:262bbdc0e468ee97c049203becc52b9ad7bf4c21405d58c82766d1aebb2e27e5
      volumeMounts:
      - mountPath: /etc/tinyproxy/tinyproxy.conf
        name: tinyproxy-conf
        subPath: tinyproxy.conf
    podinfo:
      image: stefanprodan/podinfo # used to test plex proxying      
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "32400,3000,8888,3001,3002"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /web/index.html
          port: 32400
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /web/index.html
          port: 32400
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /web/index.html
          port: 32400
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10

plexrequests:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/wests-blackhole-script
    tag: v1.5.1@sha256:71c8321c12ec2a643aca899c38f144fb31c4ba302612e139499ca966d633a316
  command: [ "/request.sh" ]
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plexrequests,plexrequests-env"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568 # need this so that the bootstrap can run
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: plexrequests-env
  - configMapRef:
      name: elfbot-plexrequests
      optional: true    
  persistence:
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plexrequests
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    app-cache:
      mountPath: /app/cache
      enabled: true
      type: emptyDir
      volumeSpec:
        medium: Memory                      
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 32501
  resources:
    requests:
      cpu: 0m
      memory: 10Mi
    limits:
      cpu: 0.5
      memory: 1Gi
  additionalContainers:
    auth:
      image: ghcr.io/elfhosted/wests-blackhole-script:v1.5.1@sha256:71c8321c12ec2a643aca899c38f144fb31c4ba302612e139499ca966d633a316
      command: [ "/auth.sh" ]
      envFrom:
      - configMapRef:
          name: plexrequests-env
      - configMapRef:
          name: elfbot-plexrequests
          optional: true    
      volumeMounts:
      - name: config
        mountPath: /config
        subPath: plexrequests     
    watchlist:
      image: ghcr.io/elfhosted/wests-blackhole-script:v1.5.1@sha256:71c8321c12ec2a643aca899c38f144fb31c4ba302612e139499ca966d633a316
      command: [ "/watchlist.sh" ]
      envFrom:
      - configMapRef:
          name: plexrequests-env
      - configMapRef:
          name: elfbot-plexrequests
          optional: true
      volumeMounts:
      - name: config
        mountPath: /config
        subPath: plexrequests             

jellyfin: &app_jellyfin
  hostname: elfhosted
  image:
    repository: ghcr.io/elfhosted/jellyfin-dev
    tag: 10.10.7@sha256:7ad104992e7908aa85b27cb4360886858caf406bc54abdc452b5b69dcce99b17
  enabled: false
  podLabels:
    app.elfhosted.com/name: jellyfin
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M" # tested with _kilos in Discord on a 97Mbit remux
  priorityClassName: tenant-streaming
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    runAsUser: 568
    runAsGroup: 568    
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jellyfin,jellyfin-env" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence: &app_jellyfin_persistence
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: "/config/log/"
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs             
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-1g  
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jellyfin
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  initContainers:
    restart-with-zurg:
      image: ghcr.io/elfhosted/zurg-rc:2025.05.14.0031-nightly@sha256:513bcc2be2a9fb8b8df7fc458f76747bebcc884776bfbfd4393e16d61b2c1227
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when zurg is updated"
    restart-with-decypharr:
      image: ghcr.io/elfhosted/decypharr-beta:rolling@sha256:b310afa9976f950ab62d7ec20801b46cb9a28176121d1c275424d780825ca4ee
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when decypharr is updated"
    restart-with-debridav:
      image: ghcr.io/elfhosted/debridav:0.9.5@sha256:1e596ddb97a7e6a4827b1dfc0718992b0c432416841a8a4c3dd3cd7491850c3c
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when debridav is updated"   
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jellyfin
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: false # necessary for probes
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
  resources:
    requests:
      cpu: "50m"
      memory: 1Gi
    limits:
      cpu: 2
      memory: 4Gi
  envFrom:
  - configMapRef:
      name: jellyfin-env
  - configMapRef:
      name: elfbot-jellyfin
      optional: true          
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      securityContext: *speedtest_securitycontext
    jellyfixer:
      image: quay.io/xsteadfastx/jellyfixer:latest
      env:
        JELLYFIXER_INTERNAL_URL: http://jellyfin:8096
        JELLYFIXER_EXTERNAL_URL: https://{{ .Release.Name }}-jellyfin.elfhosted.com
      

jellyfinranger:
  <<: *app_jellyfin
  podLabels:
    app.elfhosted.com/name: jellyfin
    app.elfhosted.com/class: dedicated
  podAnnotations:
    kubernetes.io/egress-bandwidth: "500M"
  enabled: false
  automountServiceAccountToken: false
  resources: *ranger_streamer_resources
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-jellyfin,elfbot-all"
  persistence:
    <<: *app_jellyfin_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

emby: &app_emby
  hostname: elfhosted
  image:
    registry: ghcr.io
    repository: elfhosted/emby
    tag: 4.9.1.0@sha256:3317fbd2715e0f0cb8620eebe669d8145a96a00589b0cf7a96448d41b7a628ca
  enabled: false
  priorityClassName: tenant-streaming
  podLabels:
    app.elfhosted.com/class: debrid
    app.elfhosted.com/name: emby
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M" # tested with _kilos in Discord on a 97Mbit remux
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem:
    privileged: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-emby,emby-env" # Reload the deployment every time the rclones change
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  envFrom:
  - configMapRef:
      name: emby-env
  - configMapRef:
      name: elfbot-emby
      optional: true      
  persistence: &app_emby_persistence
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: "/config/log/"
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs                
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-1g  
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-emby
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  initContainers:
  initContainers:
    restart-with-zurg:
      image: ghcr.io/elfhosted/zurg-rc:2025.05.14.0031-nightly@sha256:513bcc2be2a9fb8b8df7fc458f76747bebcc884776bfbfd4393e16d61b2c1227
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when zurg is updated"
    restart-with-decypharr:
      image: ghcr.io/elfhosted/decypharr-beta:rolling@sha256:b310afa9976f950ab62d7ec20801b46cb9a28176121d1c275424d780825ca4ee
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when decypharr is updated"
    restart-with-debridav:
      image: ghcr.io/elfhosted/debridav:0.9.5@sha256:1e596ddb97a7e6a4827b1dfc0718992b0c432416841a8a4c3dd3cd7491850c3c
      command:
      - /bin/bash
      - -c
      - |
        echo "This pod only exists to cause a restart when debridav is updated"   
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: emby
      - mountPath: /tmp
        name: tmp
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      securityContext: *speedtest_securitycontext
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: false # necessary for probes
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /
          port: 8096
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
  resources:
    requests:
      cpu: "50m"
      memory: 1Gi
    limits:
      cpu: 2
      memory: 4Gi

embyranger:
  <<: *app_emby
  podLabels:
    app.elfhosted.com/name: emby
    app.elfhosted.com/class: dedicated
  podAnnotations:
    kubernetes.io/egress-bandwidth: "500M"
  enabled: false
  automountServiceAccountToken: false
  resources: *ranger_streamer_resources
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-emby,elfbot-all"
  persistence:
    <<: *app_emby_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

homer:
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    runAsNonRoot: false
    runAsUser: 568
    runAsGroup: 568
  podSecurityContext:
    runAsNonRoot: false
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "Always"
  automountServiceAccountToken: false
  image:
    repository: ghcr.io/elfhosted/tooling
    tag: focal-20230605@sha256:6088a1e9fc0ce83aec9910af0899661c23b5f2025428d7da631b9b9390241b6c
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
  podLabels:
    app.elfhosted.com/role: nodefinder # let this be an anchor for replicationdestinations
  persistence:
    <<: *storagemounts
    logs:
      enabled: true
      type: custom
      mountPath: /logs
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    config:
      enabled: true
      type: custom
      mountPath: /config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config          
    backup:
      enabled: true
      type: custom
      mountPath: /backup
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup
    config-yml:
      enabled: "true"
      subPath: "config.yml"
      type: "custom"
      volumeSpec:
        configMap:
          name: homer-config
    custom-css:
      enabled: "true"
      subPath: "custom-css"
      type: "custom"
      volumeSpec:
        configMap:
          name: homer-config
    gatus-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: gatus-config
    disk-usage:
      enabled: "true"
      mountPath: "/usr/local/bin/disk_usage.sh"
      subPath: "disk_usage.sh"
      type: "custom"
      volumeSpec:
        configMap:
          name: homer-config
    message:
      enabled: true
      type: emptyDir
      mountPath: /www/assets/message
  command:
  - /bin/bash
  - /usr/local/bin/disk_usage.sh
  additionalContainers:
    ui:
      image: ghcr.io/elfhosted/homer:v25.05.2@sha256:60772bd0292281282161f10aa2bea105bba344158937f1c0022c6986ef5aedc3
      imagePullPolicy: IfNotPresent
      volumeMounts:
      - mountPath: /www/assets/config.yml
        name: config-yml
        subPath: "config.yml"
      - mountPath: /www/assets/custom.css
        name: custom-css
        subPath: "custom.css"
      - mountPath: /www/assets/message
        name: message
      - mountPath: /www/assets/backgrounds
        name: config
        subPath: homer/backgrounds
        readOnly: true
      resources: *default_resources
      securityContext: *default_securitycontext
  configmap:
    config:
      # -- Store homer configuration as a ConfigMap, but don't specify any config, since we'll supply our own
      enabled: false
  controller:
    replicas: 1
    strategy: RollingUpdate
    rollingUpdate:
      unavailable: 1
    annotations:
      configmap.reloader.stakater.com/reload: "homer-config, elfbot-homer" # Reload the deployment every time the yaml config changes
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 200m
      memory: 1Gi

traefikforwardauth:
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  whitelist: admin@elfhosted.com
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  controller:
    replicas: 1
    annotations:
      configmap.reloader.stakater.com/reload: traefik-forward-auth-config
    strategy: RollingUpdate
  image:
    repository: ghcr.io/elfhosted/traefik-forward-auth
    pullPolicy: IfNotPresent
    tag: 3.1.0@sha256:19cd990fae90c544100676bc049f944becc8c454639e57d20f6f48e27de90776

  middleware:
    # middleware.enabled -- Enable to deploy a preconfigured middleware
    enabled: false

  envFrom:
  - configMapRef:
      name: traefik-forward-auth-config

  ingress:
    main:
      enabled: false

  service:
    main:
      enabled: true # necessary for probes

  resources:
    requests:
      cpu: 0m
      memory: 6Mi
    limits:
      cpu: 1
      memory: 32Mi

gatus:
  image:
    repository: ghcr.io/elfhosted/gatus
    tag: 5.17.0@sha256:e2752cf7e1781478b12e0ca48159a40549905195afe39fe0a5e39ac2c7256dac
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 20Mi
    limits:
      cpu: 1
      memory: 128Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  controller:
    # strategy: RollingUpdate
    annotations:
      configmap.reloader.stakater.com/reload: "gatus-config"
  env:
    GATUS_CONFIG_PATH: /config/config.yaml
    SMTP_FROM: 'health@elfhosted.com'
    SMTP_PORT: 587
  persistence:
    gatus-config:
      enabled: "true"
      mountPath: /config
      type: "custom"
      volumeSpec:
        configMap:
          name: gatus-config
    config:
      enabled: true
      type: custom
      mountPath: /data/
      subPath: gatus
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  envFrom:
  - secretRef:
      name: gatus-smtp-config
  configmap:
    config:
      # -- Store homer configuration as a ConfigMap, but don't specify any config, since we'll supply our own
      enabled: false

gotify:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/gotify
    tag: 2.5.0@sha256:f5c89bb3ccbf857bca816e4550b46c442cfb6c0eae0f081975ba5c5099779c3f
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-gotify"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  env:
    GOTIFY_SERVER_PORT: 8080
  resources:
    requests:
      cpu: 0m
      memory: 32Mi
    limits:
      cpu: 1
      memory: 64Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/data/
      subPath: gotify
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-gotify
          optional: true
    tmp: *tmp # Avoids issues with readOnlyRootFilesystem
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: gotify
      - mountPath: /tmp
        name: tmp

flaresolverr: &app_flaresolverr
  enabled: false
  podLabels:
    app.elfhosted.com/name: flaresolverr
  image:
    registry: ghcr.io
    repository: elfhosted/byparr
    tag: v1.1.2@sha256:9df165b9291a18f287b20f0161ad0894b3feb670a0189d2ea925bb2458e85987
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # makes node unhappy
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-flaresolverr"
  # persistence:
  #   config:
  #     enabled: true
  #     type: custom
  #     mountPath: /app/screenshots
  #     volumeSpec:
  #       persistentVolumeClaim:
  #         claimName: config
  #     subPath: byparr/sreenshots
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 1000
    # runAsGroup: 1000
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 600m
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8191
  # env:
  #   WAIT_FOR_VPN: "true"
  #   LOG_LEVEL: debug
    # DRIVER: nodriver
  # initContainers:      
  #   setup-warp:
  #     image: *tooling_image
  #     imagePullPolicy: IfNotPresent
  #     command:
  #     - /bin/bash
  #     - -c
  #     - |
  #       set -x
  #       set -e

  #       cd /shared

  #       # Create cloudflare account
  #       wgcf register --accept-tos

  #       # Create gluetun config
  #       wgcf generate -p /shared/wg0.conf

  #       # grab the values from the profile and put them into env vars for gluetun to consume
  #       echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
  #       echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
  #       echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

  #       echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

  #     volumeMounts:
  #     - mountPath: /shared
  #       name: shared
  #     securityContext:
  #       seccompProfile:
  #         type: RuntimeDefault
  #       readOnlyRootFilesystem: false    
  # addons:
  #   vpn:
  #     enabled: false # in case we ever need it
  #     gluetun:
  #       image:
  #         repository: ghcr.io/elfhosted/gluetun
  #         tag: 3.40.0@sha256:ac01473025f4ed729e49f26282702267785949518fcccb3ba1a523fe201def0d
  #     env:
  #       FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
  #       DNS_KEEP_NAMESERVER: "on"
  #       HTTP_CONTROL_SERVER_PORT: "8000"
  #       HTTP_CONTROL_SERVER_ADDRESS: ":8000"
  #       VPN_TYPE: wireguard
  #       VPN_SERVICE_PROVIDER: custom
  #       FIREWALL_INPUT_PORTS: "8191"
  #       WIREGUARD_MTU: "1280"
  #       VPN_ENDPOINT_PORT: "2408"
  #       DOT: "off"
  #     securityContext:
  #       privileged: true
  #       runAsUser: 0
  #       capabilities:
  #         add:
  #           - NET_ADMIN
  #           - SYS_MODULE
  #     config: # We have to set this to null so that we can override with our own config
  #     volumeMounts:
  #     - mountPath: /shared
  #       name: shared

seafile:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/seafile
    tag: 10.0.1@sha256:810be7e7c5a6ae2c73cf21d2ec48226cbb1afbc86ead3a59738c3eef593cb9f6
  priorityClassName: tenant-bulk
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't seem to work with seafile, no output from container either
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 568 # has to run as root, see https://github.com/haiwen/seafile-docker/issues/86
    # runAsGroup: 568
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-seafile"
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1024m
      memory: 512Mi
  env:
    # -- Set the container timezone
    TIME_ZONE: Etc/UTC
    # -- The hostname of your database
    DB_HOST: "{{ .Release.Name }}-seafile-mysql"
    # -- The root password for mysql (used for initial setup)
    DB_ROOT_PASSWD: wLu5UUuT@3Zu33eT
    # -- The initial admin user's password
    SEAFILE_ADMIN_PASSWORD: changeme
    # -- The hostname for the server (set to your ingress hostname)
    SEAFILE_SERVER_HOSTNAME: "{{ .Release.Name }}-seafile.elfhosted.com"
    SEAFILE_SERVER_LETSENCRYPT: false
    FORCE_HTTPS_IN_CONF: true
    NON_ROOT: true # yes, and with our custom image, this runs the seafile/seahub components as user 568
  envFrom:
  - configMapRef:
      name: seafile-config
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # debug whether this gets us probes
  memcached:
    nameOverride: seafile-memcached
    enabled: true
  mysql:
    nameOverride: seafile-mysql
    enabled: true
    architecture: standalone
    commonAnnotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-seafile"

    auth:
      rootPassword: "wLu5UUuT@3Zu33eT"
      database: "seafile"
      username: "seafile"
      password: "nXCXSmqU4TMk3okD"

    primary:
      readinessProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      livenessProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      startupProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      persistence:
        enabled: true
        existingClaim: config
        subPath: seafile/database
      resources:
        requests:
          cpu: 5m
          memory: 1Gi
        limits:
          cpu: 2
          memory: 1024Mi
      containerSecurityContext:
        enabled: true
        seccompProfile:
          type: RuntimeDefault
        runAsUser: 568
        runAsGroup: 568
      podSecurityContext:
        enabled: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
      extraVolumeMounts:
      - mountPath: /opt/bitnami/mysql/tmp/
        name: tmp
      extraVolumes:
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi
      - name: backup-database-script
        configMap:
          name: seafile-backup

  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /shared/seafile
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
      subPath: seafile/data
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-seafile
          optional: true

tunarr:
  enabled: false
  image:
    registry: ghcr.io
    repository: chrisbenincasa/tunarr
    tag: 0.19.3-vaapi
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    privileged: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-tunarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podAnnotations:
    kubernetes.io/egress-bandwidth: "128M"  
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 1
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /root/.local
      subPath: tunarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-tunarr
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    media: # in case users use /tmp
      enabled: true
      mountPath: /streams
      type: emptyDir
      sizeLimit: 50Gi
    cache: # in case users use /tmp
      enabled: true
      mountPath: /root/.cache/pkg
      type: emptyDir
      sizeLimit: 10Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: tunarr
      - mountPath: /tmp
        name: tmp

ersatztv:
  enabled: false
  image:
    registry: docker.io
    repository: jasongdove/ersatztv
    tag: develop-vaapi@sha256:745c2796dc58a146ec89e512097d5e9d96e5d512f2f8771a225734a2a8419b1b
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    privileged: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-erzatztv"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 1
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8409
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /root/.local/share/ersatztv
      subPath: ersatztv
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    transcode:
      enabled: true
      type: custom
      mountPath: /root/.local/share/etv-transcode
      volumeSpec: *volumespec_ephemeral_volume_50g          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-tunarr
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: ersatztv
      - mountPath: /tmp
        name: tmp

threadfin:
  enabled: false
  image:
    registry: ghcr.io
    repository: elfhosted/threadfin
    tag: 1.2.34@sha256:945c7ccdbc0a56473d4f8d31c41e81c96a693b09cd15fd9cb984704fdafe8b49
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-threadfin"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 256Mi
    limits:
      cpu: 1
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 34400
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /home/threadfin/conf/
      subPath: threadfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-tunarr
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: threadfin
      - mountPath: /tmp
        name: tmp

thelounge:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/thelounge
    tag: "4.4.3@sha256:74ae8d9fc36d5a8396bb70cfaa58d222730fa69d2f71c3e2ec3ae010f2a0b264"
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-thelounge"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because the node modules in /app try to create files
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  env:
    TZ: UTC
    THELOUNGE_HOME: /config/thelounge # avoids attempts to chown /config
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 40Mi
    limits:
      cpu: 100m
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9000
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: thelounge
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-thelounge
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: thelounge
      - mountPath: /tmp
        name: tmp
    create-user:
      image: ghcr.io/elfhosted/thelounge:4.4.3@sha256:74ae8d9fc36d5a8396bb70cfaa58d222730fa69d2f71c3e2ec3ae010f2a0b264
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e


        # If we don't already have a config, create one
        if [ ! -f /config/thelounge/config.json ];
        then
          mkdir -p /config/thelounge
          cp /config-bootstrap/* /config/thelounge/ -R
        fi

        # If we don't already have a user, create one
        if [ ! -f /config/thelounge/users/${USERNAME}.json ];
        then
          thelounge add ${USERNAME} --password ${PASSWORD}
        fi
      volumeMounts:
      - mountPath: /config
        subPath: thelounge
        name: config
      env:
      - name: THELOUNGE_HOME
        value: /config/thelounge # avoids attempts to chown /config
      - name: USERNAME
        valueFrom:
          configMapKeyRef:
            name: elfhosted-user-config
            key: USERNAME
      - name: PASSWORD
        value: ireadthedocs
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true

symlinkcleaner:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/symlink-cleaner
    tag: v1.4.7@sha256:2b980b95269aa534c46bc735a613efe1934af948d602e281958ac2891006acb6
  imagePullSecrets:
  - name: ghcr-io-elfhosted
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-symlink-cleaner,symlink-cleaner-example-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because the node modules in /app try to create files
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 40Mi
    limits:
      cpu: 100m
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5000
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 5000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 5000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 45
        httpGet:
          path: /health
          port: 5000
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 5
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/config
      subPath: symlink-cleaner
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /app/logs
      subPath: symlink-cleaner
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-symlink-cleaner
          optional: true          
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: symlink-cleaner-example-config          
  envFrom:
  - configMapRef:
      name: symlink-cleaner-env
  - configMapRef:
      name: elfbot-symlink-cleaner
      optional: true    
  initContainers:  
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: symlink-cleaner
      - mountPath: /tmp
        name: tmp      
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e
        # If we don't already have an example config, create one
        if [ ! -f /config/config.json ];
        then
          cp /bootstrap/config.json /config/
        fi
      volumeMounts:
      - mountPath: /config/
        name: config
        subPath: symlink-cleaner
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext: *default_securitycontext  

overseerr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/overseerr
    tag: 1.34.0@sha256:d9cfaf805e8512684cec4b5c897e372d898eeff522cef0801f3245cb3d00caf6
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: overseerr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-overseerr,overseerr-config"
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: overseerr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-overseerr
          optional: true
    tmp: *tmp
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
    overseerr-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: overseerr-config
          optional: true 
    run:
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi                   
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: overseerr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        # run the setup script from the configmap, so that we can make templated changes
        bash /bootstrap/setup.sh
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: overseerr
      - name: overseerr-config
        mountPath: "/bootstrap/"
    # We do this so that we can override the /app/jellyseer/public path with our own, allowing the user to customize the branding
    copy-branding:
      image: ghcr.io/elfhosted/overseerr:1.34.0@sha256:d9cfaf805e8512684cec4b5c897e372d898eeff522cef0801f3245cb3d00caf6
      command:
        - /bin/bash
        - -c
        - |
          mkdir -p /config/branding
          cp --no-clobber -rf /app/overseerr/public/logo_* /config/branding
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: overseerr
      resources: *default_resources
      securityContext: *default_securitycontext
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5055
  resources:
    requests:
      cpu: 0m
      memory: 175Mi
    limits:
      cpu: 2
      memory: 2Gi
  additionalContainers:
    branding:
      image: nginxinc/nginx-unprivileged
      volumeMounts:
      - mountPath: /usr/share/nginx/html
        name: config
        subPath: overseerr/branding
        readOnly: true
      - mountPath: /tmp
        name: tmp
      - mountPath: /run
        name: run
      resources: *default_resources
      securityContext: *default_securitycontext

jellyseerr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/jellyseerr
    tag: 2.5.2@sha256:f8bc515864f06a4eec9dfa09d1d63a41c687e2839654da35f6a96815717a9187
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: jellyseerr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jellyseerr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: jellyseerr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jellyseerr
          optional: true
    tmp: *tmp
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
    jellyseerr-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: jellyseerr-config
          optional: true
    run:
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi            
  envFrom:
  - configMapRef:
      name: jellyseerr-env
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jellyseerr
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        # run the setup script from the configmap, so that we can make templated changes
        bash /bootstrap/setup.sh
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: jellyseerr
      - name: jellyseerr-config
        mountPath: "/bootstrap/"        
    # We do this so that we can override the /app/jellyseer/public path with our own, allowing the user to customize the branding
    copy-branding:
      image: ghcr.io/elfhosted/jellyseerr:2.5.2@sha256:f8bc515864f06a4eec9dfa09d1d63a41c687e2839654da35f6a96815717a9187
      command:
        - /bin/ash
        - -c
        - |
          mkdir -p /config/branding
          cp --no-clobber -rf /app/public/logo_* /config/branding/
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: jellyseerr
      resources: *default_resources
      securityContext: *default_securitycontext
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5055
  resources:
    requests:
      cpu: 0m
      memory: 160Mi
    limits:
      cpu: 2
      memory: 1Gi
  additionalContainers:
    branding:
      image: nginxinc/nginx-unprivileged
      volumeMounts:
      - mountPath: /usr/share/nginx/html
        name: config
        subPath: jellyseerr/branding
        readOnly: true
      - mountPath: /tmp
        name: tmp
      - mountPath: /run
        name: run
      resources: *default_resources
      securityContext: *default_securitycontext

jellystat:
  enabled: false
  podLabels:
    app.elfhosted.com/name: jellystat
  image:
    repository: ghcr.io/elfhosted/jellystat
    tag: 1.1.6@sha256:39bb3980570b6dd56fac1becbb52cde40f821d6ac73eaef97dd9c3d9ec9d97bd
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jellystat,jellystat-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with ilikedanger currently
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: jellystat-env
  - configMapRef:
      name: elfbot-jellystat
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 20Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: jellystat/config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    backup:
      enabled: true
      type: custom
      mountPath: /app/backup-data
      subPath: jellystat
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jellystat
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jellystat
      - mountPath: /tmp
        name: tmp
    setup-postgres:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/database
        chown elfie:elfie /config/database -R

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: jellystat
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault    
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: jellystat
        - name: POSTGRES_DB
          value: jellystat
        - name: POSTGRES_USER
          value: jellystat
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: jellystat/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 1Gi

booklore:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/booklore
    tag: v0.12.0@sha256:3688b5ba13beea77bdc6763f50e3c64d9b98db147fb670facd3e8d9f861271b4
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-booklore,booklore-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    allowPrivilegeEscalation: false
    runAsNonRoot: false # because we need the permission-fixer to run as root
    capabilities:
      drop:
      - ALL        
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: booklore-env
  - configMapRef:
      name: elfbot-booklore
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 20Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6060    
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/data
      subPath: booklore
      volumeSpec:
        persistentVolumeClaim:
          claimName: config        
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-booklore
          optional: true
  command: 
  - java 
  - -jar 
  - /app/app.jar     
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: booklore
      - mountPath: /tmp
        name: tmp
    setup-mariadb:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/database
        chown elfie:elfie /config/database -R

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: booklore
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault            
  additionalContainers:
    frontend:
      image: ghcr.io/elfhosted/booklore:v0.12.0@sha256:3688b5ba13beea77bdc6763f50e3c64d9b98db147fb670facd3e8d9f861271b4
      command: 
      - /usr/sbin/nginx 
      - -g 
      - "daemon off;"
      securityContext:
        readOnlyRootFilesystem: false
        runAsUser: 0 # nginx insists on running as root
    database:
      image: mariadb:11 
      env:
        - name: MYSQL_ROOT_PASSWORD
          value: booklore
        - name: MYSQL_DATABASE
          value: booklore
        - name: MYSQL_USER
          value: booklore
        - name: MYSQL_PASSWORD
          value: booklore
      volumeMounts:
      - mountPath: /var/lib/mysql
        name: config
        subPath: booklore/database
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 1Gi

peertube:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/peertube
    tag: v7.1.1-bookworm@sha256:17d5e43a6825cdc739731d97d294a6d1df5e6e4ddbad17bef5db19f99a3f022d
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-peertube,peertube-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    privileged: true # needed for transcoding
    runAsNonRoot: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    supplementalGroups:
    - 993      
    allowPrivilegeEscalation: false
    runAsNonRoot: false
    capabilities:
      drop:
      - ALL        
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: peertube-env
  - configMapRef:
      name: elfbot-peertube
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 20Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9000
    probes:
      liveness:
        custom: true
        enabled: true
        spec:
          failureThreshold: 5
          httpGet:
            path: /api/v1/ping
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 10
      readiness:
        custom: true
        enabled: true
        spec:
          failureThreshold: 5
          httpGet:
            path: /api/v1/ping
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 10
      startup:
        custom: true
        enabled: true
        spec:
          failureThreshold: 5
          httpGet:
            path: /api/v1/ping
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 10          
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: peertube/app
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    backup:
      enabled: true
      type: custom
      subPath: peertube
      volumeSpec:
        persistentVolumeClaim:
          claimName: backup          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-peertube
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"         
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: peertube
      - mountPath: /tmp
        name: tmp
    setup-postgres:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/database
        chown elfie:elfie /config/database -R

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: peertube
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault    
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: peertube
        - name: POSTGRES_DB
          value: peertube
        - name: POSTGRES_USER
          value: peertube
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: peertube/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 1Gi

audiobookshelf:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/audiobookshelf
    tag: v2.23.0@sha256:5aadfe6e11ae0059fdadd325cc6f262dc0978472449e1b515bfe206cfb453973
  priorityClassName: tenant-streaming
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: audiobookshelf
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-audiobookshelf,elfbot-all"
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: audiobookshelf
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    # never used, just satisfies startup scripts
    metadata:
      enabled: true
      type: emptyDir
      mountPath: /metadata
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-audiobookshelf
          optional: true
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  env:
    METADATA_PATH: /config/metadata
    SOURCE: ElfHosted
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: audiobookshelf
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 2
      memory: 1Gi

storyteller:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/storyteller
    tag: v1.4.0-ctc.7@sha256:1081edd3ec29b3ef32df8887e5e388f7686ce288bde4c7b9cff8e448e12a26ae
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-storyteller,elfbot-all"
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data/
      subPath: storyteller
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    # never used, just satisfies startup scripts
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-storyteller
          optional: true
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
    cache: 
      enabled: true
      type: emptyDir
      mountPath: /app/.next/standalone/web/.next/cache         
    whisper-builds:
      enabled: true
      type: emptyDir
      mountPath: /app/.next/standalone/web/whisper-builds
    home-elfie:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie
  envFrom:
  - configMapRef:
      name: storyteller-env
  - secretRef:
      name: storyteller-env      
  - configMapRef:
      name: elfbot-storyteller
      optional: true          
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: storyteller
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8001
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 12Gi


audiobookrequest:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/audiobookrequest
    tag: 1.4.8@sha256:21043d365b7964b51c2e4017eae11c2998997cdc722f65ff82b4e6a761ab7d41
  priorityClassName: tenant-streaming
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-audiobookrequest,elfbot-all"
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: audiobookrequest
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-audiobookrequest
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: audiobookrequest
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 2
      memory: 1Gi

audiobookbayautomated:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/audiobookbay-automated
    tag: rolling@sha256:207f581bd9afd84f6c7e845973b538c766daa1f07b875ab7b73fbe2089a62e42
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-audiobookbay-automated"
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: audiobookbay-automated
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-audiobookbay-automated
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: audiobookbay-automated
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5078
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 2
      memory: 1Gi      
  envFrom:
  - configMapRef:
      name: audiobookbay-automated-env      

openbooks:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/openbooks
    tag: 4.5.0@sha256:c17d8e86d55e35cd1edb20fe1e1ff95a8e93cbef4c571ead2bfeb07276845477
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  persistence:
    <<: *storagemounts
  command:
  - /bin/bash
  - -c
  - |
    set -x
    set -e
    sleep 5s
    USER=$(tr -dc A-Za-z0-9 </dev/urandom | head -c 13 ; echo '')
    ./openbooks server \
      --dir ${DATA_DIR-/tmp} \
      --port 8000 \
      --name $USER \
      --tls=false \
      --persist \
      --server irc.irchighway.net:6661 \
      --no-browser-downloads \
      --debug
  envFrom:
  - configMapRef:
      name: elfbot-openbooks
      optional: true
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-openbooks"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 2
      memory: 1Gi

vaultwarden:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/vaultwarden
    tag: 1.34.1@sha256:1637be9c0a684e74c890dfba9c5d4243e123f7127536a299d62bb2a6c0e105c3
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-vaultwarden"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: elfbot-vaultwarden
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: vaultwarden
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-vaultwarden
          optional: true
    tmp: *tmp
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: vaultwarden
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 1
      memory: 1Gi


notifiarr:
  enabled: false
  hostname: elfhosted
  image:
    repository: ghcr.io/elfhosted/notifiarr
    tag: 0.8.3@sha256:16d9f57524a249fd6181b9282a0c2c66e24caf417b692199d4dec2681784fc4c
  priorityClassName: tenant-normal
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because the node modules in /app try to create files
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-notifiarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"

  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 2
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5454
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: notifiarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: notifiarr-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-notifiarr
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: notifiarr
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [ ! -f /config/notifiarr.conf ];
        then
          cp /bootstrap/notifiarr.conf /config/
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: notifiarr
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true

shoko:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/shokoserver
    tag: v5.1.0@sha256:a386f39bcfd7c1e8f9c38b4171c8e59162e3e1b2c579728465fd4ea52978a69e
  env:
    PUID: 568
    PGID: 568
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-shoko"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # again, s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /home/shoko/.shoko/
      subPath: shoko
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-shoko
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: shoko
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8111
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi

filebot:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/filebot-node
    tag: 0.4.8@sha256:2f35281e7f6d4a1566dfe6427dae83480c76d1cebe1e5d6d2379064db1846ba7
  env:
    PUID: 568
    PGID: 568
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-filebot"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # again, s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: filebot
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-filebot
          optional: true

    tmp: # to avoid errors about storing java prefs
      enabled: true
      type: emptyDir
      mountPath: /home/seedy
      sizeLimit: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: filebot
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5452
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 2
      memory: 1Gi

rpdb:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rpdb
    tag: 0.3.3@sha256:fc651aeb123bedbcb92eeda3227b51005540b3c350bf15640c7928f5e8fd1d4b
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,storage-changed,elfbot-rpdb"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /.config
      subPath: rpdb
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rpdb
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rpdb
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8750
  resources:
    requests:
      cpu: 0m
      memory: 40Mi
    limits:
      cpu: 1
      memory: 1Gi

kometa: &app_kometa
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/kometa
    tag: v2.2.0@sha256:d0d56d58de2ef623d7167e1dbf4901aa7bc2dc5ca4ec7206dbf1de5b2906d7d1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: kometa
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-kometa"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: elfbot-kometa
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: kometa
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs/
      subPath: kometa
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-kometa
          optional: true
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: kometa-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: kometa
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [ ! -f /config/config.yml ];
        then
          cp /bootstrap/config.yml /config/
        fi

        # Create directories we need by default
        mkdir -p /config/kometa/assets
        mkdir -p /config/kometa/logs
        mkdir -p /config/kometa/metadata
        mkdir -p /config/kometa/missing
        mkdir -p /config/kometa/overlays

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: kometa
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 4Gi

imagemaid:
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/imagemaid
    tag: v1.1.1@sha256:574796393e4366e19b602f530d61d22835b15d3367253c9225587a692e3e060b
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: imagemaid
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-imagemaid"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: imagemaid-env
  - configMapRef:
      name: elfbot-imagemaid
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-imagemaid
          optional: true
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: kometa-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: imagemaid
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 4Gi  

cinesync:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/cinesync
    tag: v2.4.1@sha256:701497547924a2923e696d9ac38bec1ac6d58b9c23aa1c8a519c61ff541d5f85
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: cinesync
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-cinesync,cinesync-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: cinesync-env
  - secretRef:
      name: cinesync-env      
  - configMapRef:
      name: elfbot-cinesync
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/db
      subPath: cinesync
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-cinesync
          optional: true  
    logs:
      enabled: true
      type: custom
      mountPath: /app/logs
      subPath: cinesync
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs              
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: cinesync
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x

        # Create directory structure if it doesn't exist yet
        mkdir -p /storage/symlinks/movies
        mkdir -p /storage/symlinks/movies-4k
        mkdir -p /storage/symlinks/movies-anime
        mkdir -p /storage/symlinks/series
        mkdir -p /storage/symlinks/series-4k
        mkdir -p /storage/symlinks/series-anime
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      resources: *default_resources
      securityContext: *default_securitycontext        
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 2Gi 

arrtools:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/arrtools
    tag: rolling@sha256:91a4b1133de4212f86fe3f3b2a5deaf2f622cf4216d70aeb735aa1c562133c46
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  podLabels:
    app.elfhosted.com/name: arrtools
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-arrtools"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: arrtools
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: arrtools-config          
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 2Gi 
  initContainers:
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [[ ! -f /config/confighd.ini ]];
        then
          cp /bootstrap/confighd.ini /config/
        fi
        if [[ ! -f /config/config4k.ini ]];
        then
          cp /bootstrap/config4k.ini /config/
        fi        
        cp /bootstrap/config-placeholder.ini /config/
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: arrtools
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true      

seerrbridge:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/seerrbridge
    tag: v0.7.2@sha256:cb1158de27c48e960f253141c0b5346d0ddf86305fb5d86187ba0e46b21dc89b
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # needs for temporary files I think
  podLabels:
    app.elfhosted.com/name: seerrbridge
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-seerrbridge,seerrbridge-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
    backup: *backup
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory      
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-seerrbridge
          optional: true                
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: seerrbridge
      volumeSpec:
        persistentVolumeClaim:
          claimName: config  
    logs:
      enabled: true
      type: custom
      mountPath: /app/logs
      subPath: seerrbridge
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs     
    npm: # for bridgeboard
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
    chromedriver: # needs to download chromedriver on start
      enabled: true
      mountPath: /app/seerr/chromedriver
      type: emptyDir
      sizeLimit: 1Gi           
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: seerrbridge-env
  - secretRef:
      name: seerrbridge-env      
  - configMapRef:
      name: elfbot-seerrbridge
      optional: true
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 2Gi  
  additionalContainers:
    bridgeboard:
      image: ghcr.io/elfhosted/seerrbridge-bridgeboard:v0.7.2@sha256:b2db8a98f7f0f006e47b98979de8e2a390217f21987f6428d67791019da1c13d
      env:
        - name: SEERRBRIDGE_URL
          value: "http://localhost:8777"
        - name: SEERRBRIDGE_LOG_PATH
          value: /app/logs/seerbridge.log
      volumeMounts:
      - mountPath: /app/data
        name: config
        subPath: seerrbridge
      - mountPath: /app/logs
        name: logs
        subPath: seerrbridge
      - mountPath: /.npm
        name: npm
  initContainers:
    setup-warp:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false   
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: seerrbridge
      - mountPath: /tmp
        name: tmp          
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "3001,8777,3777"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared               

listsync:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/listsync
    tag: 0.5.9@sha256:2e03962c8cfbe81c337680e2542a7809bf4067904de80845f4b21cf64fede31c
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # needs to create its own .venv
  podLabels:
    app.elfhosted.com/name: listsync
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-listsync,listsync-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    backup: *backup
    tmp: *tmp     
    config:
      enabled: true
      type: custom
      mountPath: /usr/src/app/data
      subPath: listsync
      volumeSpec:
        persistentVolumeClaim:
          claimName: config  
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-listsync
          optional: true    
    logs:
      enabled: true
      type: custom
      mountPath: /logs
      subPath: listsync
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs                         
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: listsync-env
  - configMapRef:
      name: elfbot-listsync
      optional: true
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 2Gi  
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: debridav
      - mountPath: /tmp
        name: tmp  

plextraktsync:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/plextraktsync
    tag: 0.34.10@sha256:b23bedb51450ca04346db71ae280718ea03e5eeb7f57cd3e3af894c61f3bca7e
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: plextraktsync
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plextraktsync"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: elfbot-plextraktsync
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /home/elfie/.config/PlexTraktSync
      subPath: plextraktsync
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plextraktsync
          optional: true
    state: # plextraktsync needs this
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mountPath: /home/elfie/.local/state
    cache: # plextraktsync needs this
      enabled: true
      type: emptyDir
      sizeLimit: 1Gi
      mountPath: /home/elfie/.cache      
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plextraktsync
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 100m # no way this should be using so much resources
      memory: 1Gi

letterboxdtraktsync:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/letterboxd-trakt-sync
    tag: v1.0.1@sha256:368112e9ef5eda2934b67f676368da4e478265af1b37770343670a3d0c341e4c
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-letterboxd-trakt-sync"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  env:
    RUN_ON_START: "true"
  envFrom:
  - configMapRef:
      name: elfbot-letterboxd-trakt-sync
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: letterboxd-trakt-sync
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-letterboxd-trakt-sync
          optional: true   
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: letterboxd-trakt-sync
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 100m # no way this should be using so much resources
      memory: 1Gi

decluttarr: &app_decluttarr
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/decluttarr
    tag: rolling@sha256:647bb99019dd571b5cd91fa8e32d65bcfb7e0b933dca4d28aa27b5689e14fdf3
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: decluttarr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-decluttarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: elfbot-decluttarr
      optional: true
  - configMapRef:
      name: zenv-decluttarr
      optional: true      
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi

rdebridui:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rdebrid-ui
    tag: rolling@sha256:2c9885f1918097d8b762e0e60fcee5e12f147c10d79f13decbe4f3b73d24e4ad
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podLabels:
    app.elfhosted.com/name: rdebrid-ui
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rdebrid-ui"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  persistence:
    tmp: *tmp
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  envFrom:
  - configMapRef:
      name: elfbot-rdebrid-ui
      optional: true  
  - configMapRef:
      name: zenv-rdebrid-ui
  - secretRef:
      name: zenv-rdebrid-ui      
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi

suggestarr:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/suggestarr
    tag: v1.0.20@sha256:f567f7416f2e2092d54e68037e0886908ce583126281c9673c92bde0969bf1c5
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with app :(
  podLabels:
    app.elfhosted.com/name: suggestarr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-suggestarr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  env:
    TZ: UTC
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /app/config/
      subPath: suggestarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /var/log
      subPath: suggestarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-suggestarr
          optional: true          
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5000
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: suggestarr
      - mountPath: /tmp
        name: tmp      

decluttarr4k: 
  <<: *app_decluttarr
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-decluttarr4k"  
  envFrom:
  - configMapRef:
      name: elfbot-decluttarr4k
      optional: true
  - configMapRef:
      name: zenv-decluttarr4k
      optional: true      


rcloneui:
  enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.69.3@sha256:9396f8c3a649dba9f91169a62142c9f3e6da00974bda573d864b62c07f78234e
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rcloneui,elfhosted-user-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    privileged: true
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    backup: *backup
    cache: 
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      # volumeSpec: *volumespec_ephemeral_volume_10g    
    mount:
      enabled: true
      type: emptyDir
      mountPath: /mount
      sizeLimit: 1Gi 
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: rclone
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    rclone-remote-storage:
      enabled: "true"
      subPath: "rclone-remote-storage"
      type: "custom"
      volumeSpec:
        configMap:
          name: rclonefm-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rclonebrowser
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rclone
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # add local remote if it doesn't exist
        grep -q '/storage' /config/rclone.conf || cat /rclone-remote-storage >> /config/rclone.conf

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: rclone
      - mountPath: /tmp # need this for cating into a file
        name: tmp
      - mountPath: /rclone-remote-storage
        subPath: rclone-remote-storage
        name: rclone-remote-storage
      resources: *default_resources
      securityContext: *default_securitycontext
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5572
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 1
      memory: 1Gi

rclonefm:
  enabled: true
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.69.3@sha256:9396f8c3a649dba9f91169a62142c9f3e6da00974bda573d864b62c07f78234e
  command:
  - /rclonefm.sh
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rclonefm,rclonefm-config"
  podAnnotations:
    kubernetes.io/egress-bandwidth: "40M"
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    cache:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      sizeLimit: 1Gi
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: rclone
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    rclonefm-config:
      enabled: "true"
      mountPath: /var/lib/rclonefm/js/settings.js
      subPath: "settings.js"
      type: "custom"
      volumeSpec:
        configMap:
          name: rclonefm-config
    rclone-remote-storage:
      enabled: "true"
      subPath: "rclone-remote-storage"
      type: "custom"
      volumeSpec:
        configMap:
          name: rclonefm-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rclonefm
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rclone
      - mountPath: /tmp
        name: tmp
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # add local remote if it doesn't exist
        grep -q '/storage' /config/rclone.conf || cat /rclone-remote-storage >> /config/rclone.conf

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: rclone
      - mountPath: /tmp # need this for cating into a file
        name: tmp
      - mountPath: /rclone-remote-storage
        subPath: rclone-remote-storage
        name: rclone-remote-storage
      resources: *default_resources
      securityContext: *default_securitycontext
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5573
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi

webdav: &webdav
  enabled:
    false
  podLabels:
    app.elfhosted.com/name: webdav
  podAnnotations:
    kubernetes.io/egress-bandwidth: "40M"
  priorityClassName: tenant-normal
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.69.3@sha256:9396f8c3a649dba9f91169a62142c9f3e6da00974bda573d864b62c07f78234e
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-webdav-plus,elfbot-webdav"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  command:
  - /webdav.sh
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: webdav-config
  - configMapRef:
      name: elfbot-webdav
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /storage/config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-webdav-plus
          optional: true
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  service:
    main:
      enabled: false # necessary for probes
      ports:
        http:
          port: 5574
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi

storagehub:
  enabled: false # down for now
  podLabels:
    app.elfhosted.com/name: storagehub
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "storagehub-scripts,storagehub-env"
      secret.reloader.stakater.com/reload: ",storagehub-config,storagehub-env"
  # affinity:
  #   podAffinity:
  #     # prefer to be located with zurg, if tolerations permit
  #     preferredDuringSchedulingIgnoredDuringExecution:
  #     - weight: 100
  #       podAffinityTerm:
  #         labelSelector:
  #           matchExpressions:
  #           - key: app.elfhosted.com/name
  #             operator: In
  #             values:
  #             - zurg
  #         topologyKey: "kubernetes.io/hostname"
  # tolerations:
  # - key: node-role.elfhosted.com/download-only
  #   operator: Exists
  # - key: node-role.elfhosted.com/dedicated
  #   operator: Exists
  priorityClassName: tenant-critical
  image:
    repository: itsthenetwork/nfs-server-alpine
    tag: latest
  env:
    SHARED_DIRECTORY: /export
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    privileged: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    # # this is an ephemeral volume
    # storage:
    #   enabled: true
    #   type: custom
    #   mountPath: /storage
    #   volumeSpec: *volumespec_ephemeral_volume_50g
    # these are the persistent volumes we support currently
    # needed for the migration

    rclonemountrealdebridzurg: *rclonemountrealdebridzurg
    # this is the old symlinks on HDD
    # symlinks: *symlinks # these only get mounted on storagehub. Everything else accesses symlinks **through** storagehub
    # samba config
    # config:
    #   enabled: "true"
    #   subPath: "container.json"
    #   mountPath: /etc/samba/container.json
    #   type: "custom"
    #   volumeSpec:
    #     secret:
    #       secretName: storagehub-config
    # storagehub-scripts:
    #   enabled: "true"
    #   type: "custom"
    #   volumeSpec:
    #     configMap:
    #       name: storagehub-scripts
    #       defaultMode: 0755
    tmp:
      enabled: true
      type: emptyDir
      mountPath: /tmp
      sizeLimit: 1Gi
  service:
    main:
      enabled: false # necessary for probes
      ports:
        http:
          port: 2049
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 10Mi
    limits:
      cpu: 1
      memory: 512Mi
  # envFrom:
  # - configMapRef:
  #     name: storagehub-env
  # - secretRef:
  #     name: storagehub-env
  # initContainers:
    # setup:
    #   image: *tooling_image
    #   imagePullPolicy: IfNotPresent
    #   command:
    #   - /bin/bash
    #   - -c
    #   - |
    #     # don't error on failure
    #     # not doing this yet
    #     # /storagehub-scripts/restore.sh

    #     echo nothing
    #     # if [[ ! -f /storage/symlinks/i-was-migrated-to-storagehub ]]
    #     # then
    #     #   if [[ ! -z "$(ls -A /migration)" ]]
    #     #   then
    #     #     echo "Tar-migrating from /migration/..."
    #     #     tar -cf - -C /migration/ . | tar xvmf - -C /storage/symlinks/
    #     #     touch /storage/symlinks/i-was-migrated-to-storagehub
    #     #   fi
    #     # fi

    #   envFrom:
    #   - configMapRef:
    #       name: storagehub-env
    #   - configMapRef:
    #       name: elfhosted-user-config
    #   - secretRef:
    #       name: storagehub-env
    #   volumeMounts:
    #   - mountPath: /storagehub-scripts
    #     name: storagehub-scripts
    #   - mountPath: /home/elfie
    #     name: tmp
    #   - mountPath: /migration
    #     name: migration
    #   - mountPath: /storage/symlinks
    #     name: symlinks
    #   resources: *default_resources
    #   securityContext: *default_securitycontext
  # additionalContainers:
  #   backup-on-termination:
  #     image: *tooling_image
  #     command:
  #     - /usr/bin/dumb-init
  #     - /bin/bash
  #     - -c
  #     - /storagehub-scripts/backup.sh
  #     envFrom:
  #     - configMapRef:
  #         name: storagehub-env
  #     - configMapRef:
  #         name: elfhosted-user-config
  #     - secretRef:
  #         name: storagehub-env
  #     volumeMounts:
  #     - mountPath: /storagehub-scripts
  #       name: storagehub-scripts
  #     - mountPath: /home/elfie
  #       name: tmp
  #     - mountPath: /ephemeral
  #       name: ephemeral
  #     # need a folder here for each app. what a pita
  #     - mountPath: /persistent/plex
  #       name: persistent-plex
  #     - mountPath: /symlinks
  #       name: symlinks
  # terminationGracePeriodSeconds: "3600" # take up to an hour to backup

webdavplus:
  enabled: false
  <<: *webdav
  podLabels:
    app.elfhosted.com/name: webdav-plus
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M"
  envFrom:
  - configMapRef:
      name: webdav-plus-config

jfa:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/jfa-go
    tag: v0.5.1@sha256:ad4d6052d9fb27c0a7d972ecd4916aa0e51eb303a515e93a9a5e8a3ee80b1dd8
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jfa"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: jfa
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jfa
          optional: true
    tmp:
      enabled: true
      type: emptyDir
      mountPath: /tmp
      sizeLimit: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jfa
      - mountPath: /tmp
        name: tmp
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8056
  resources:
    requests:
      cpu: 0m
      memory: 150Mi
    limits:
      cpu: 2
      memory: 1Gi

mattermost:
  enabled: false
  # Default values for mattermost-team-edition.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.
  image:
    repository: mattermost/mattermost-team-edition
    tag: 10.8.1@sha256:b335c73564a63220b7768901bdcfb9f09bf2b370925cd7fe9659e40b2060bd13
    imagePullPolicy: IfNotPresent
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mattermost"

  initContainerImage:
    repository: appropriate/curl
    tag: latest
    imagePullPolicy: IfNotPresent

  extraInitContainers: []

  ## Deployment Strategy
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  deploymentStrategy:
    type: Recreate
    rollingUpdate: null

  ## How many old ReplicaSets for Mattermost Deployment you want to retain
  revisionHistoryLimit: 1

  ## Enable persistence using Persistent Volume Claims
  ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
  ## ref: https://docs.gitlab.com/ee/install/requirements.html#storage
  ##
  persistence:
    ## This volume persists generated data from users, like images, attachments...
    ##
    data:
      enabled: true
      size: 10Gi
      ## If defined, volume.beta.kubernetes.io/storage-class: <storageClass>
      ## Default: volume.alpha.kubernetes.io/storage-class: default
      ##
      # storageClass:
      accessMode: ReadWriteOnce
      existingClaim: "config"
      subPath: mattermost/data
    plugins:
      enabled: false # these just end up under data anyway

  service:
    type: ClusterIP
    externalPort: 8065
    internalPort: 8065
    annotations: {}
    # loadBalancerIP:
    loadBalancerSourceRanges: []

  ingress:
    enabled: false
    path: /
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # certmanager.k8s.io/issuer:  your-issuer
      # nginx.ingress.kubernetes.io/proxy-body-size: 50m
      # nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
      # nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
      # nginx.ingress.kubernetes.io/proxy-buffering: "on"
      # nginx.ingress.kubernetes.io/configuration-snippet: |
      #   proxy_cache mattermost_cache;
      #   proxy_cache_revalidate on;
      #   proxy_cache_min_uses 2;
      #   proxy_cache_use_stale timeout;
      #   proxy_cache_lock on;
      #### To use the nginx cache you will need to set an http-snippet in the ingress-nginx configmap
      #### http-snippet: |
      ####     proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=mattermost_cache:10m max_size=3g inactive=120m use_temp_path=off;
    hosts:
      - mattermost.example.com
    tls:
      # - secretName: mattermost.example.com-tls
      #   hosts:
      #     - mattermost.example.com

  route:
    enabled: false

  ## If use this please disable the mysql chart by setting mysql.enable to false
  externalDB:
    enabled: true

    ## postgres or mysql
    externalDriverType: "mysql"

    ## postgres:  "<USERNAME>:<PASSWORD>@<HOST>:5432/<DATABASE_NAME>?sslmode=disable&connect_timeout=10"
    ## mysql:     "<USERNAME>:<PASSWORD>@tcp(<HOST>:3306)/<DATABASE_NAME>?charset=utf8mb4,utf8&readTimeout=30s&writeTimeout=30s"
    externalConnectionString: "mattermost:IUzI1NiJ9.eyJhdWQiOiIwMDk1MTkyYjhjZWIyYjVhNDQwMT@tcp(mattermost-mysql:3306)/mattermost?charset=utf8mb4,utf8&readTimeout=30s&writeTimeout=30s"

  mysql:
    nameOverride: mattermost-mysql
    enabled: true
    architecture: standalone
    # nameOverride: mattermost-mariadb

    commonAnnotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mattermost"

    auth:
      rootPassword: "3uAaJYGJLR3d2qbBM2FsSThJ"
      database: "mattermost"
      username: "mattermost"
      password: "IUzI1NiJ9.eyJhdWQiOiIwMDk1MTkyYjhjZWIyYjVhNDQwMT"

    primary:
      readinessProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      livenessProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      startupProbe:
        enabled: false # probes can make helm fail/restart under some conditions. Either do or do not, there is no try
      persistence:
        enabled: true
        existingClaim: config
        subPath: mattermost/database
      resources:
        requests:
          cpu: 5m
          memory: 512Mi
        limits:
          cpu: 2
          memory: 1024Mi
      containerSecurityContext:
        enabled: true
        seccompProfile:
          type: RuntimeDefault
        runAsUser: 568
        runAsGroup: 568
      podSecurityContext:
        enabled: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
      extraVolumeMounts:
      - mountPath: /opt/bitnami/mysql/tmp/
        name: tmp
      extraVolumes:
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi
      - name: backup-database-script
        configMap:
          name: mattermost-backup
      - name: confighdd
        persistentVolumeClaim:
          claimName: config
          subPath: mattermost/database
      sidecars:
        - name: backup-database
          image: *tooling_image
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: 3uAaJYGJLR3d2qbBM2FsSThJ
            - name: MYSQL_DATABASE
              value: mattermost
          command:
          - /usr/bin/dumb-init
          - /bin/bash
          - -c
          - |

            sleep 2m # give mysql time to start up
            while true
            do
              now=$(date +"%s_%Y-%m-%d")
              /usr/bin/mysqldump --opt -h mattermost-mysql -u root -p${MYSQL_ROOT_PASSWORD} ${MYSQL_DATABASE} > "/backup/${now}_${MYSQL_DATABASE}.sql"
              sleep 1d
            done

  ## Additional pod annotations
  extraPodAnnotations: {}

  ## Additional env vars
  extraEnvVars: []
    # This is an example of extra env vars when using with the deployment with GitLab Helm Charts
    # - name: POSTGRES_PASSWORD_GITLAB
    #   valueFrom:
    #     secretKeyRef:
    #       # NOTE: Needs to be manually created
    #       # kubectl create secret generic gitlab-postgresql-password --namespace <NAMESPACE> --from-literal postgres-password=<PASSWORD>
    #       name: gitlab-postgresql-password
    #       key: postgres-password
    # - name: POSTGRES_USER_GITLAB
    #   value: gitlab
    # - name: POSTGRES_HOST_GITLAB
    #   value: gitlab-postgresql
    # - name: POSTGRES_PORT_GITLAB
    #   value: "5432"
    # - name: POSTGRES_DB_NAME_MATTERMOST
    #   value: mm5
    # - name: MM_SQLSETTINGS_DRIVERNAME
    #   value: "postgres"
    # - name: MM_SQLSETTINGS_DATASOURCE
    #   value: postgres://$(POSTGRES_USER_GITLAB):$(POSTGRES_PASSWORD_GITLAB)@$(POSTGRES_HOST_GITLAB):$(POSTGRES_PORT_GITLAB)/$(POSTGRES_DB_NAME_MATTERMOST)?sslmode=disable&connect_timeout=10

  ## Additional init containers
  # extraInitContainers: []
    # This is an example of extra Init Container when using with the deployment with GitLab Helm Charts
    # - name: bootstrap-database
    #   image: "postgres:9.6-alpine"
    #   imagePullPolicy: IfNotPresent
    #   env:
    #     - name: POSTGRES_PASSWORD_GITLAB
    #       valueFrom:
    #         secretKeyRef:
    #           name: gitlab-postgresql-password
    #           key: postgres-password
    #     - name: POSTGRES_USER_GITLAB
    #       value: gitlab
    #     - name: POSTGRES_HOST_GITLAB
    #       value: gitlab-postgresql
    #     - name: POSTGRES_PORT_GITLAB
    #       value: "5432"
    #     - name: POSTGRES_DB_NAME_MATTERMOST
    #       value: mm5
    #   command:
    #     - sh
    #     - "-c"
    #     - |
    #       if PGPASSWORD=$POSTGRES_PASSWORD_GITLAB psql -h $POSTGRES_HOST_GITLAB -p $POSTGRES_PORT_GITLAB -U $POSTGRES_USER_GITLAB -lqt | cut -d \| -f 1 | grep -qw $POSTGRES_DB_NAME_MATTERMOST; then
    #       echo "database already exist, exiting initContainer"
    #       exit 0
    #       else
    #       echo "Database does not exist. creating...."
    #       PGPASSWORD=$POSTGRES_PASSWORD_GITLAB createdb -h $POSTGRES_HOST_GITLAB -p $POSTGRES_PORT_GITLAB -U $POSTGRES_USER_GITLAB $POSTGRES_DB_NAME_MATTERMOST
    #       echo "Done"
    #       fi

  # Add additional volumes and mounts, for example to add SAML keys in the app or other files the app server may need to access
  extraVolumes: []
    # - hostPath:
    #     path: /var/log
    #   name: varlog
  extraVolumeMounts:
    - name: mattermost-data
      mountPath: mattermost/mattermost/logs
      subPath: mattermost/logs
      readOnly: true

  ## Node selector
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
  nodeSelector: {}

  ## Affinity
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}

  ## Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  ## Pod Security Context
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  securityContext:
    fsGroup: 568
    runAsGroup: 568
    runAsUser: 568
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL

  serviceAccount:
    create: false
    name:
    annotations: {}

  ## Configuration
  ## The config here will be injected as environment variables in the deployment
  ## Please refer to https://docs.mattermost.com/administration/config-settings.html#configuration-in-database for more information
  ## You can add any config here, but need to respect the format: MM_<GROUPSECTION>_<SETTING>. ie: MM_SERVICESETTINGS_ENABLECOMMANDS: false
  config:
    MM_PLUGINSETTINGS_CLIENTDIRECTORY: "./client/plugins"

syncthing:
  enabled: false
  hostname: elfhosted
  priorityClassName: tenant-bulk
  image:
    repository: ghcr.io/elfhosted/syncthing
    tag: 1.29.6@sha256:d7d222a79ebec9b60eb7ae01a0ac3099db5ae41bac00b79abe13ac803c44f15e
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-syncthing"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: syncthing
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-syncthing
          optional: true

  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8384
  resources:
    requests:
      cpu: 0m
      memory: 70Mi
    limits:
      cpu: 1
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: syncthing
      - mountPath: /tmp
        name: tmp
    setup:
      image: ghcr.io/elfhosted/syncthing:1.29.6@sha256:d7d222a79ebec9b60eb7ae01a0ac3099db5ae41bac00b79abe13ac803c44f15e
      imagePullPolicy: IfNotPresent
      envFrom:
      - configMapRef:
          name: elfhosted-user-config
      command:
      - /bin/ash
      - -c
      - |
        set -x
        set -e

        # Generate a new config if necessary
        if [ ! -f /config/config.xml ]
        then
          # We are generating a new config
          syncthing generate --config=/config
        fi

        # Apply the port every time (incase the user changes it and reboots)
        # sed -i  "s/<listenAddress>tcp.*/<listenAddress>tcp:\/\/0.0.0.0:${PORT_SYNCTHING}<\/listenAddress>/" /config/config.xml
        # sed -i  "s/<listenAddress>quic.*/<listenAddress>quic:\/\/0.0.0.0:${PORT_SYNCTHING}<\/listenAddress>/" /config/config.xml

        # # And if it's defaulted...
        # sed -i  "s/<<listenAddress>default<\/listenAddress>/<listenAddress>tcp:\/\/0.0.0.0:${PORT_SYNCTHING}<\/listenAddress>\n\t<listenAddress>quic:\/\/0.0.0.0:${PORT_SYNCTHING}<\/listenAddress>/" /config/config.xml

        # Ignore the fact that we have no password set
        # grep '<insecureAdminAccess>true</insecureAdminAccess>' /config/config.xml || sed -i  "s/<\/gui>/<insecureAdminAccess>true<\/insecureAdminAccess>\n\t<\/gui>/" /config/config.xml

        # Avoid foolish use of capital letters in default sync folder
        # sed -i  "s/\/storage\/elfstorage\/Sync/\/storage\/elfstorage\/syncthing/" /config/config.xml

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: syncthing
      resources: *default_resources
      securityContext: *default_securitycontext

rdtclient: &app_rdtclient
  enabled: false
  hostname: elfhosted
  priorityClassName: tenant-bulk
  podLabels:
    app.elfhosted.com/class: debrid
  image:
    repository: ghcr.io/elfhosted/rdtclient
    tag: v2.0.113@sha256:e274c86872f823296d063cc04eaec10410bf5ea4590ccb1660ebe51339803464
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rdtclient"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data/db
      subPath: rdtclient
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rdtclient
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 6500
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
      ephemeral-storage: 50Mi
    limits:
      cpu: 100m
      memory: 2Gi
      ephemeral-storage: 100Mi # a safety net against node ephemeral space exhaustion
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rdtclient
      - mountPath: /tmp
        name: tmp

rdtclientpremiumize:
  enabled: false
  <<: *app_rdtclient
  podLabels:
    app.elfhosted.com/name: rdtclient-premiumize
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rdtclient-premiumize"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data/db
      subPath: rdtclient-premiumize
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rdtclient-premiumize
          optional: true
    download: # in case users use /tmp
      enabled: true
      type: custom
      mountPath: /data/downloads
      volumeSpec: *volumespec_ephemeral_volume_1g  
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rdtclient-premiumize
      - mountPath: /tmp
        name: tmp

rdtclienttorbox:
  enabled: false
  <<: *app_rdtclient
  podLabels:
    app.elfhosted.com/name: rdtclient-torbox
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rdtclient-torbox"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data/db
      subPath: rdtclient-torbox
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rdtclient-torbox
          optional: true
    download: # in case users use /tmp
      enabled: true
      type: custom
      mountPath: /data/downloads
      volumeSpec: *volumespec_ephemeral_volume_1g  
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rdtclient-torbox
      - mountPath: /tmp
        name: tmp

# RDTClient for AllDebrid
rdtclientalldebrid:
  enabled: false
  <<: *app_rdtclient
  podLabels:
    app.elfhosted.com/name: rdtclient-alldebrid
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rdtclient-alldebrid"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /data/db
      subPath: rdtclient-alldebrid
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-rdtclient-alldebrid
          optional: true
    download: # in case users use /tmp
      enabled: true
      type: custom
      mountPath: /data/downloads
      volumeSpec: *volumespec_ephemeral_volume_1g  
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: rdtclient-alldebrid
      - mountPath: /tmp
        name: tmp      
  addons:
    vpn:
      enabled: true
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      envFrom:
      - configMapRef:
          name: gluetun-config
      env:
        DOT: "off"
        FIREWALL_INPUT_PORTS: "6500"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

jdownloader:
  enabled: false
  hostname: elfhosted
  # runtimeClassName: kata
  priorityClassName: tenant-bulk
  image:
    repository: jlesage/jdownloader-2
    tag: v25.02.1
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jdownloader"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 568
    # runAsGroup: 568
    fsGroup: 568 # need this so that the bootstrap can run
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    JDOWNLOADER_HEADLESS: 1
    APP_NICENESS: 19
  envFrom:
  - configMapRef:
      name: jdownloader-config
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
      mountPath: /output
      subPath: jdownloader/downloads/completed/jdownloader
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jdownloader
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9898
  resources:
    requests:
      cpu: 0m
      memory: 10Mi
    limits:
      cpu: 0.5
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jdownloader
      - mountPath: /tmp
        name: tmp
  additionalContainers:
    podinfo:
      image: stefanprodan/podinfo # used to run probes from gatus

miniflux:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/miniflux
    tag: 2.2.9
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-miniflux,miniflux-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1500m # if par threads is 1, this leaves 0.5cpu for downloading
      memory: 1Gi
  envFrom:
  - configMapRef:
      name: miniflux-config
  postgresql:
    enabled: true
    nameOverride: miniflux-postgresql
    commonAnnotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-miniflux"
    auth:
      username: miniflux
      password: miniflux
      database: miniflux
      postgresPassword: miniflux
    primary:
      affinity: *standard_affinity
      tolerations: *standard_tolerations
      persistence:
        enabled: true
        existingClaim: config
        subPath: miniflux/database
      resources:
        requests:
          cpu: 5m
          memory: 128Mi
        limits:
          cpu: 2
          memory: 1024Mi
      containerSecurityContext:
        enabled: true
        seccompProfile:
          type: RuntimeDefault
        runAsUser: 568
        runAsGroup: 568
      podSecurityContext:
        enabled: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
      extraVolumeMounts:
      - mountPath: /opt/bitnami/postgresql/conf/
        name: conf
      - mountPath: /opt/bitnami/postgresql/tmp/
        name: tmp
      extraVolumes:
      - name: conf
        emptyDir:
          sizeLimit: 1Gi
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi
      sidecars:
        - name: backup-database
          image: *tooling_image
          env:
            - name: POSTGRES_PASSWORD
              value: miniflux
            - name: POSTGRES_DATABASE
              value: miniflux
            - name: POSTGRES_USER
              value: miniflux
          command:
          - /usr/bin/dumb-init
          - /bin/bash
          - -c
          - |

            set +e # for debug
            sleep 2m # give postgres time to start up
            while true
            do
              now=$(date +"%s_%Y-%m-%d")
              PGPASSWORD=$POSTGRES_PASSWORD pg_dump -U $POSTGRES_USER -h localhost -d $POSTGRES_DATABASE -F c -f /backup/${now}_${POSTGRES_DATABASE}.psql
              sleep 1d
            done

joplinserver:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/geek-cookbook/joplin-server
    tag: v2.14.2@sha256:b4f52bffce08541dd54e823b78bbfa18e091d53064ed507f69fe9e1ca92b719b
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-joplinserver,joplinserver-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # breaks migrations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1500m # if par threads is 1, this leaves 0.5cpu for downloading
      memory: 1Gi
  envFrom:
  - configMapRef:
      name: joplinserver-config
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: joplinserver/data
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  postgresql:
    enabled: true
    nameOverride: joplinserver-postgresql
    commonAnnotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-joplinserver"
    auth:
      username: joplinserver
      password: joplinserver
      database: joplinserver
      postgresPassword: joplinserver
    primary:
      affinity: *standard_affinity
      tolerations: *standard_tolerations
      persistence:
        enabled: true
        existingClaim: config
        subPath: joplinserver/database
      resources:
        requests:
          cpu: 5m
          memory: 128Mi
        limits:
          cpu: 2
          memory: 1024Mi
      containerSecurityContext:
        enabled: true
        seccompProfile:
          type: RuntimeDefault
        runAsUser: 568
        runAsGroup: 568
      podSecurityContext:
        enabled: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
      extraVolumeMounts:
      - mountPath: /opt/bitnami/postgresql/conf/
        name: conf
      - mountPath: /opt/bitnami/postgresql/tmp/
        name: tmp
      extraVolumes:
      - name: conf
        emptyDir:
          sizeLimit: 1Gi
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi
      sidecars:
        - name: backup-database
          image: *tooling_image
          env:
            - name: POSTGRES_PASSWORD
              value: joplinserver
            - name: POSTGRES_DATABASE
              value: joplinserver
            - name: POSTGRES_USER
              value: joplin
          command:
          - /usr/bin/dumb-init
          - /bin/bash
          - -c
          - |

            set +e # for debug
            sleep 2m # give postgres time to start up
            while true
            do
              now=$(date +"%s_%Y-%m-%d")
              PGPASSWORD=$POSTGRES_PASSWORD pg_dump -U $POSTGRES_USER -h localhost -d $POSTGRES_DATABASE -F c -f /backup/${now}_${POSTGRES_DATABASE}.psql
              sleep 1d
            done

homepage:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/gethomepage/homepage
    tag: v1.2.0
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-homepage,homepage-config,homepage-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
    privileged: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    # runAsUser: 568
    # runAsGroup: 568
    fsGroup: 568 # need this so that the bootstrap can run
    fsGroupChangePolicy: "OnRootMismatch"
  serviceAccount:
    create: true
    name: homepage
  automountServiceAccountToken: true
  env:
    PUID: 568
    PGID: 568
  envFrom:
  - configMapRef:
      name: homepage-env
  - configMapRef:
      name: elfbot-homepage
      optional: true
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /app/config
      subPath: homepage
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    config-default:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: homepage-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-homepage
          optional: true
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 250m # deliberately hobble the CPU in favor of GPU transcoding
      memory: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: homepage
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /app/config/user-change-these/
        touch /app/config/user-change-these/JELLYFIN_KEY
        touch /app/config/user-change-these/PLEX_KEY
        touch /app/config/user-change-these/EMBY_KEY
        touch /app/config/user-change-these/NAVIDROME_USER
        touch /app/config/user-change-these/NAVIDROME_TOKEN
        touch /app/config/user-change-these/NAVIDROME_SALT
        touch /app/config/user-change-these/CALIBREWEB_USERNAME
        touch /app/config/user-change-these/CALIBREWEB_PASSWORD
        touch /app/config/user-change-these/KOMGA_USERNAME
        touch /app/config/user-change-these/KOMGA_PASSWORD
        touch /app/config/user-change-these/KAVITA_USERNAME
        touch /app/config/user-change-these/KAVITA_PASSWORD
        touch /app/config/user-change-these/AUDIOBOOKSHELF_KEY
        touch /app/config/user-change-these/OMBI_KEY
        touch /app/config/user-change-these/OVERSEERR_KEY
        touch /app/config/user-change-these/JELLYSEERR_KEY
        touch /app/config/user-change-these/TAUTULLI_KEY
        touch /app/config/user-change-these/tunarr_USERNAME
        touch /app/config/user-change-these/tunarr_PASSWORD
        touch /app/config/user-change-these/MINIFLUX_KEY
        touch /app/config/user-change-these/UPTIMEKUMA_SLUG
        touch /app/config/user-change-these/GOTIFY_KEY

        # If we don't already have an example config, create one
        if [ ! -f /app/config/dont-overwrite-me ];
        then
          cp /bootstrap/* /app/config/
        fi
      volumeMounts:
      - mountPath: /app/config
        name: config
        subPath: homepage
      - name: config-default
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true


wallabag:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: stefanprodan/podinfo
    tag: latest
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-wallabag,wallabag-config"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
    privileged: false
  # runtimeClassName: kata
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568 # for the mounted volumes
  persistence:
    config:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  resources:
    requests:
      cpu: 0m
      memory: 32Mi
    limits:
      cpu: 100m
      memory: 1Gi
  additionalContainers:
    ui:
      image: ghcr.io/elfhosted/wallabag:2.6.12@sha256:3398b2af57d0e51727ebaf2d674c71566718040c685e01a26db532bd4848ebcb
      volumeMounts:
      - mountPath: /var/www/wallabag/data
        name: config
        subPath: wallabag/data
      - mountPath: /var/www/wallabag/images
        name: config
        subPath: wallabag/images
      envFrom:
      - configMapRef:
          name: elfbot-wallbag
          optional: true
      - configMapRef:
          name: wallabag-config
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        allowPrivilegeEscalation: false
      resources:
        requests:
          cpu: 0m
          memory: 100Mi
        limits:
          cpu: 500m
          memory: 200Mi

gotosocial:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/gotosocial
    tag: 0.19.0@sha256:538111e5b96b5dd7bf6142bcaf1de1aa86180d5d3d09e20d76425cc38036cb9e
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-gotosocial"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work with s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 1
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  env:
    S6_READ_ONLY_ROOT: "true"
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: gotosocial
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: gotosocial-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-gotosocial
          optional: true
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: gotosocial
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [[ ! -f /config/config.yaml ]];
        then
          cp /bootstrap/config.yaml /config/
        fi

        # Setup the fish env
        mkdir -p /config/.config/fish
        cp /bootstrap/config.fish /config/.config/fish
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: gotosocial
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /livez
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /readyz
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 60 # allow 10 min of failures to start up
        httpGet:
          path: /readyz
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10  

blueskypds:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/bluesky-pds
    tag: 0.4.140@sha256:1e16e26c68c7fee12c7a8d05dfdc23e3ca04231db6c45048cfce322e1b6cf8d9
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-bluesky-pds"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work with s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 1
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  env:
    S6_READ_ONLY_ROOT: "true"
  envFrom:
  - configMapRef:
      name: bluesky-pds-env
  - configMapRef:
      name: elfbot-bluesky-pds
      optional: true    
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: bluesky-pds
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-bluesky-pds
          optional: true
    run: # used for s6-init with non-root
      enabled: true
      type: emptyDir
      mountPath: /run
      sizeLimit: 1Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: bluesky-pds
      - mountPath: /tmp
        name: tmp
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /xrpc/_health
          port: 3000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /xrpc/_health
          port: 3000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 60 # allow 10 min of failures to start up
        httpGet:
          path: /xrpc/_health
          port: 3000
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10  

riven: &app_riven
  enabled: false
  podLabels:
    app.elfhosted.com/name: riven
  image:
    repository: ghcr.io/elfhosted/riven
    tag: v0.21.21@sha256:bf1a9cab579d5eaac0ed72e20584b139547db7c38753a03b3756151af5e0ac13
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-riven,riven-env,riven-setup"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with ilikedanger currently
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: riven-env
  - configMapRef:
      name: elfbot-riven
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 20Mi
    limits:
      cpu: 2
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /riven/data
      subPath: riven
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /riven/data/logs
      subPath: riven
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-riven
          optional: true
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory
    setup:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: riven-setup        
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: riven
      - mountPath: /tmp
        name: tmp
    setup-postgres:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/postgresql/database
        mkdir -p /config/postgresql/backups
        chown elfie:elfie /config -R

      volumeMounts:
      - mountPath: /config
        name: config
        subPath: riven
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        # run the setup script from the configmap, so that we can make templated changes
        bash /setup/setup.sh
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      - mountPath: /config
        name: config
        subPath: riven
      - name: setup
        mountPath: "/setup/"              
  additionalContainers:
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310      
      env:
        - name: POSTGRES_PASSWORD
          value: postgres
        - name: POSTGRES_DB
          value: riven
        - name: POSTGRES_USER
          value: postgres
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: riven/postgresql/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi   

rivenvpn: 
  enabled: false
  <<: *app_riven
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-riven,riven-env,riven-setup,gluetun-config"  
  addons:
    vpn:
      enabled: true
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      envFrom:
      - configMapRef:
          name: gluetun-config
      env:
        DOT: "off"
        FIREWALL_INPUT_PORTS: "3001,8080" # 3001 is ttyd, 8080 is the backend
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"        
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

rivenfrontend: &app_rivenfrontend
  podLabels:
    app.elfhosted.com/name: riven-frontend
  image:
    repository: ghcr.io/elfhosted/riven-frontend
    tag: v0.20.0@sha256:c4d792868a8113a2f57f504fe371cbcd8c52172d734c812c366bff352672998e
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-rivenfrontend,riven-frontend-env,riven-frontend-config"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with ilikedanger currently
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    config:
      enabled: "true"
      type: "custom"
      mountPath: /riven/config
      volumeSpec:
        configMap:
          name: riven-frontend-config          
  envFrom:
  - configMapRef:
      name: riven-frontend-env
  - configMapRef:
      name: elfbot-rivenfrontend
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 20Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000

# So that we can toggle it on with rivenvpn
rivenfrontendvpn:
  enabled: false
  <<: *app_rivenfrontend

airdcpp: &app_airdcpp
  enabled: false
  image:
    repository: ghcr.io/geek-cookbook/airdcpp
    tag: 2.9.0@sha256:d9f6e597bcfc38946d0c4cafce775a559e7b8cf7c66397c9c506cb695ea01205
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: airdcpp
  podAnnotations:
    kubernetes.io/egress-bandwidth: "100M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-airdcpp"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 16Mi
    limits:
      cpu: 2
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5600
  env:
    WAIT_FOR_VPN: "true"
    PORT_FILE: /.airdcpp/forwarded-port
  probes:
    liveness:
      enabled: false
    startup:
      enabled: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /.airdcpp/
      subPath: airdcpp
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-airdcpp
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: airdcpp
      - mountPath: /tmp
        name: tmp
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      envFrom:
      - configMapRef:
          name: airdcpp-pia-config
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
        subPath: airdcpp
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus

airdcpppia:
  enabled: false
  <<: *app_airdcpp

airdcppgluetun:
  enabled: false
  <<: *app_airdcpp
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      envFrom:
      - configMapRef:
          name: airdcpp-gluetun-config
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      additionalVolumeMounts:
      - mountPath: /config
        name: config
        subPath: airdcpp
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus

jackett:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/jackett
    tag: 0.22.1960@sha256:e984fb3d5fab9718e23f3a4326d1341365a9cfbcaa3b64e3c77f8faaf8cc23dc
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jackett"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  # Always prefer to cohabit with zurg
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9117
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: jackett
      volumeSpec:
        persistentVolumeClaim:
          claimName: config        
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-jackett
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: jackett
      - mountPath: /tmp
        name: tmp

wizarr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/wizarr
    tag: 2025.5.d@sha256:58c2e28b6fbe604b6a6b4478944ff5071b6700d1a81a58708b70359c58e325ca
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-wizarr"
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1024Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5690
  envFrom:
  - configMapRef:
      name: wizarr-env
  - configMapRef:
      name: elfbot-wizarr
      optional: true            
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /data/database
      subPath: wizarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config        
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-wizarr
          optional: true
    wizarr-steps:
      enabled: true
      type: emptyDir
      mountPath: /data/wizard_steps          
    wizarr-steps-plex:
      enabled: "true"
      type: "custom"
      mountPath: /opt/default_wizard_steps/plex
      volumeSpec:
        configMap:
          name: wizarr-steps-plex
    wizarr-steps-jellyfin:
      enabled: "true"
      type: "custom"
      mountPath: /opt/default_wizard_steps/jellyfin
      volumeSpec:
        configMap:
          name: wizarr-steps-jellyfin
    cache:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache  
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: wizarr
      - mountPath: /tmp
        name: tmp        

flixio:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/stremio-web
    tag: rolling@sha256:de20030d591000b871519899ccbcead9e958a8e1eb5e98d05b65e033286a7e2a
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: flixio
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-flixio"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work with iprom's patching trick
    privileged: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: flixio-env
  - configMapRef:
      name: elfbot-flixio
      optional: true  
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080

flixioapi:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/flixio-api
    tag: rolling@sha256:0e30427d9c311f806610951cfc8d8a71c1ba6b65c3ea1a93b829ed75e37138c9
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: flixio-api
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-flixio-api"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    runAsUser: 568
    runAsGroup: 568
  automountServiceAccountToken: false
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-flixio-api
          optional: true
    config:
      enabled: true
      type: custom
      mountPath: /app/data
      subPath: flixio-api
      volumeSpec:
        persistentVolumeClaim:
          claimName: config   
  envFrom:
  - configMapRef:
      name: flixio-api-env
  - configMapRef:
      name: elfbot-flixio-api
      optional: true           
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080

stremiojackett:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/stremio-jackett
    tag: v4.2.6@sha256:e835d4a2579a474394dce8d19175d3e4a89bbdbd307422b9ce6d3452950ee1b2
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-stremio-jackett"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: stremio-jackett-env
  resources:
    requests:
      cpu: 10m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  persistence:
    pm2:
      enabled: true
      type: emptyDir
      mountPath: /.pm2
      sizeLimit: 1Gi
    npm:
      enabled: true
      type: emptyDir
      mountPath: /.npm
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-stremio-jackett
          optional: true

pairdrop:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/pairdrop
    tag: v1.11.2@sha256:d4e52400813a6af412433a801645cd59e7ad27e3e73540af698e085125eead8c
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: pairdrop
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-pairdrop"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  # envFrom:
  # - configMapRef:
  #     name: pairdrop-env
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-pairdrop
          optional: true

actual:
  enabled: false
  image:
    repository: ghcr.io/actualbudget/actual-server
    tag: 25.5.0-alpine
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: actual
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-actual"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5006
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-actual
          optional: true
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: actual
      volumeSpec:
        persistentVolumeClaim:
          claimName: config          

petio:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/petio
    tag: v0.5.5@sha256:a28b7ffb5b1b04a8ad798112604c410a9a09f8882e9bf98b9f24e8f41571f505
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-petio"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 7777
  persistence:
    backup: *backup
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-petio
          optional: true
    config:
      enabled: true
      type: custom
      mountPath: /app/api/config/
      subPath: petio/config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    tmp: *tmp
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: petio
      - mountPath: /tmp
        name: tmp
  additionalContainers:
    mongodb:
      image: mongodb/mongodb-community-server:8.0.9-ubi8
      volumeMounts:
        - name: config
          subPath: petio/mongodb
          mountPath: /data/db/
        - name: tmp
          mountPath: /tmp
      securityContext: *default_securitycontext

nightscout:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/nightscout
    tag: 15.0.3@sha256:9fb5de043a782232738fc7fc185a0f4d9655de7ae3177fed5b3c13fe51840ee3
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-nightscout,nightscout-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: nightscout-env
  - configMapRef:
      name: elfbot-nightscout
      optional: true  
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 1337
  persistence:
    tmp: *tmp
    config:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  additionalContainers:
    mongodb:
      image: mongodb/mongodb-community-server:8.0.9-ubi8
      volumeMounts:
        - name: config
          subPath: nightscout/mongodb
          mountPath: /data/db/
        - name: tmp
          mountPath: /tmp
      securityContext: *default_securitycontext

pgadmin:
  enabled: false
  image:
    repository: dpage/pgadmin4
    tag: "9.3"
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-pgadmin"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    capabilities:
      add:
      - NET_BIND_SERVICE
      drop:
      - ALL
  automountServiceAccountToken: false
  envFrom:
  - configMapRef:
      name: pgadmin-env
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 80
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: pgadmin
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    tmp: *tmp

redisinsight:
  enabled: false
  image:
    repository: redislabs/redisinsight
    tag: v2@sha256:7fef8b7ecf2e8597037f906fc69863345dd846d36577210569396f7917333355
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-redisinsight"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
    capabilities:
      add:
      - IPC_LOCK
      drop:
      - ALL
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5540
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: redisinsight
      volumeSpec:
        persistentVolumeClaim:
          claimName: config

mongoexpress:
  enabled: false
  image:
    repository: mongo-express
    tag: 1.0.2-18
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mongoexpress"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8081
  envFrom:
  - configMapRef:
      name: elfbot-mongoexpress
      optional: true

comet:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/comet
    tag: v2.4.4@sha256:ea4d87db35b74d44fd57a7f600249efa22339c5d96c19084b4239efc8b20fb20
  priorityClassName: tenant-normal
  podAnnotations:
    kubernetes.io/egress-bandwidth: "64M"
  podLabels:
      app.elfhosted.com/name: comet
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-comet,comet-env"
      secret.reloader.stakater.com/reload: "comet-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568    
  initContainers:
    setup-warp:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false    
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
    config:
      enabled: true
      type: custom
      mountPath: /app/data
      subPath: comet
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    cache:
      enabled: true
      type: emptyDir
      mountPath: /.cache
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-comet
          optional: true
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory          
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 2
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  envFrom:
  - configMapRef:
      name: comet-env
  - secretRef:
      name: comet-env
  - configMapRef:
      name: elfbot-comet
      optional: true
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      env:
        CHANGE_CONTAINER_PORTS: "True"
        HTTP_PORT: "3002"
        HTTPS_PORT: "3003"        
      securityContext: *speedtest_securitycontext
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8001"
        HTTP_CONTROL_SERVER_ADDRESS: ":8001"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "8000"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared      

jackettio:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/jackettio
    tag: v1.7.1@sha256:6bff317e8c7eb286cf53a8e1740a9b06092e2d3536f80168c728bfeb04a54289
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-jackettio,jackettio-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: jackettio
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory              
  resources:
    requests:
      cpu: 10m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 4000
  envFrom:
  - configMapRef:
      name: jackettio-env
  - configMapRef:
      name: elfbot-jackettio
      optional: true
  initContainers:      
    setup-warp:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false    
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "4000"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared

stremthru:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/stremthru
    tag: 0.76.0@sha256:bb387617d1ca65513cc76f3bfc715da1189aa2a307c7229151a6f5d0c4f292eb
  priorityClassName: tenant-normal
  podAnnotations:
    kubernetes.io/egress-bandwidth: "64M"  
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-stremthru,stremthru-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: stremthru
      volumeSpec:
        persistentVolumeClaim:
          claimName: config      
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755           
    # shared:
    #   enabled: true
    #   mountPath: /shared
    #   type: emptyDir
    #   volumeSpec:
    #     medium: Memory                    
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  envFrom:
  - configMapRef:
      name: stremthru-env
  - configMapRef:
      name: elfbot-stremthru
      optional: true
  - secretRef:
      name: stremthru-env      
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      env:
        IPTABLES_BACKEND: nft
        KILLSWITCH: "true"
        LOCAL_NETWORK: 10.0.0.0/8
        LOC: de-frankfurt
        PORT_FORWARDING: "0"
        PORT_PERSIST: "1"
        NFTABLES: "1"
        VPNDNS: "0"
      envFrom:
      - secretRef:
          name: stremthru-vpn
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      env:
        CHANGE_CONTAINER_PORTS: "True"
        HTTP_PORT: "3002"
        HTTPS_PORT: "3003"    
      securityContext: *speedtest_securitycontext  

davio:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/davio
    tag: v1.0.4@sha256:3508249d413b6b55bb2860358bbb92dd8ccd760969fadb0473540ddb74218523
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-davio,davio-env"
      secret.reloader.stakater.com/reload: "davio-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config
      volumeSpec: *volumespec_ephemeral_volume_1g
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 4000
  envFrom:
  - configMapRef:
      name: davio-env
  - secretRef:
      name: davio-env
  - configMapRef:
      name: elfbot-davio
      optional: true

mediafusion:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/mediafusion
    tag: 4.3.31@sha256:71046c77757d3d883981c04fe5209dc9cd3682aaec7af2df62c111efaa921a31
  podLabels:
      app.elfhosted.com/name: mediafusion
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mediafusion,mediafusion-env"
      secret.reloader.stakater.com/reload: "mediafusion-env,mediafusion-vpn"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    tinyproxy-conf:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tinyproxy-conf 
  resources:
    requests:
      cpu: 50m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8000
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8000
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    startup:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8000
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10           
  envFrom:
  - configMapRef:
      name: mediafusion-env
  - configMapRef:
      name: elfbot-mediafusion
      optional: true
  - secretRef:
      name: mediafusion-env

piaproxy:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: thrnz/docker-wireguard-pia
    tag: latest
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "piaproxy-conf"
      secret.reloader.stakater.com/reload: "piaproxy-vpn"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations    
  env:
    IPTABLES_BACKEND: nft
    KILLSWITCH: "true"
    LOCAL_NETWORK: 10.0.0.0/8
    LOC: de-frankfurt
    PORT_FORWARDING: "0"
    PORT_PERSIST: "1"
    NFTABLES: "1"
    VPNDNS: "0"
  envFrom:
  - secretRef:
      name: piaproxy-vpn
  securityContext:
    privileged: true
    runAsUser: 0
    capabilities:
      add:
        - NET_ADMIN
        - SYS_MODULE
  persistence:
    tinyproxy-conf:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tinyproxy-conf 
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8888
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        exec:
          command:
          - /bin/bash
          - -c
          - curl -x http://localhost:8888 -s https://www.cloudflare.com/cdn-cgi/trace | grep www.cloudflare.com
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10
  additionalContainers:
    tinyproxy:
      image: ghcr.io/elfhosted/tinyproxy:v1.4.3@sha256:262bbdc0e468ee97c049203becc52b9ad7bf4c21405d58c82766d1aebb2e27e5
      volumeMounts:
      - mountPath: /etc/tinyproxy/tinyproxy.conf
        name: tinyproxy-conf
        subPath: tinyproxy.conf
      - mountPath: /shared
        name: shared
      # env:
      #   WAIT_FOR_VPN: "true"

vpnproxy:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/gluetun
    tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-vpnproxy"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations    
  securityContext:
    privileged: true
    runAsUser: 0
    capabilities:
      add:
        - NET_ADMIN
        - SYS_MODULE
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory          
  resources:
    requests:
      cpu: 0m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8888
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        exec:
          command:
          - /bin/ash
          - -c
          - curl -x http://localhost:8888 -s https://www.cloudflare.com/cdn-cgi/trace | grep www.cloudflare.com
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 10            
  env:
    VPN_TYPE: wireguard
    VPN_SERVICE_PROVIDER: custom
    VPN_ENDPOINT_PORT: "2408"
    HTTPPROXY: "on"
    FIREWALL_INPUT_PORTS: "8888"
    DOT: "off"
  initContainers:      
    setup-warp:
      image: ghcr.io/elfhosted/tooling:focal-20240530@sha256:458d1f3b54e9455b5cdad3c341d6853a6fdd75ac3f1120931ca3c09ac4b588de
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false    

tinyproxy:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/tinyproxy
    tag: v1.4.3@sha256:262bbdc0e468ee97c049203becc52b9ad7bf4c21405d58c82766d1aebb2e27e5
  podLabels:
      app.elfhosted.com/name: tinyproxy
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "tinyproxy-conf"
      secret.reloader.stakater.com/reload: "tinyproxy-vpn"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    tinyproxy-conf:
      enabled: "true"
      type: "custom"
      mountPath: /etc/tinyproxy/tinyproxy.conf
      subPath: tinyproxy.conf  
      volumeSpec:
        configMap:
          name: tinyproxy-conf 
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory          
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8888
  env:
    WAIT_FOR_VPN: "true"    
  initContainers:      
    setup-warp:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        cd /shared

        # Create cloudflare account
        wgcf register --accept-tos

        # Create gluetun config
        wgcf generate -p /shared/wg0.conf

        # grab the values from the profile and put them into env vars for gluetun to consume
        echo "export WIREGUARD_PRIVATE_KEY=$(grep PrivateKey /shared/wg0.conf | cut -f3 -d' ')" > /shared/env
        echo "export WIREGUARD_PUBLIC_KEY=$(grep PublicKey /shared/wg0.conf | cut -f3 -d' ')" >> /shared/env
        echo "export WIREGUARD_ADDRESSES=$(grep Address /shared/wg0.conf | grep '/32' | cut -f3 -d' ')" >> /shared/env

        echo "export VPN_ENDPOINT_IP=$(dig +short dig +short engage.cloudflareclient.com)" >> /shared/env

      volumeMounts:
      - mountPath: /shared
        name: shared
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: false    
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DNS_KEEP_NAMESERVER: "on"
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        VPN_TYPE: wireguard
        VPN_SERVICE_PROVIDER: custom
        FIREWALL_INPUT_PORTS: "8888"
        WIREGUARD_MTU: "1280"
        VPN_ENDPOINT_PORT: "2408"
        DOT: "off"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      volumeMounts:
      - mountPath: /shared
        name: shared

mediaflowproxy:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/mediaflow-proxy
    tag: 2.1.4@sha256:ea6a00c0537e092fea6a09ee10473c1ee446bf40568b4c1a6ee2c9a5f8260106
  podLabels:
      app.elfhosted.com/name: mediaflow-proxy
  priorityClassName: tenant-normal
  podAnnotations:
    kubernetes.io/egress-bandwidth: "64M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-mediaflow-proxy,mediaflow-proxy-env"
      secret.reloader.stakater.com/reload: "mediaflowproxy-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  additionalContainers:
    speedtest:
      image: openspeedtest/latest:latest@sha256:1745e913f596fe98882b286a67751efdae74774e9caa742a4934bb056e8748d2
      env:
        CHANGE_CONTAINER_PORTS: "True"
        HTTP_PORT: "3002"
        HTTPS_PORT: "3003"    
      securityContext: *speedtest_securitycontext
  resources:
    requests:
      cpu: 100m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8888
  envFrom:
  - configMapRef:
      name: mediaflow-proxy-env
  - configMapRef:
      name: elfbot-mediaflow-proxy
      optional: true   

youriptv:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/youriptv-nobranding
    tag: rolling@sha256:b5b7cb9858e5d6b2edd636f6167ef1e78198deb0badea677256a50613b43211e
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-youriptv"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PORT: 3649
  persistence:
    tmp: *tmp
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3649

aiostreams:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/aiostreams
    tag: v1.22.0@sha256:21ea0e748e85b36c2b3687e0eba3938809bdd309844e99b46ca9ce3d28a845fd
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-aiostreams,aiostreams-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
  resources:
    requests:
      cpu: 0m
      memory: 50Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  envFrom:
  - configMapRef:
      name: aiostreams-env   
  - configMapRef:
      name: elfbot-aiostreams
      optional: true

stremify:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/stremify
    tag: rolling@sha256:dccb712ee19e8e16512175a4ca87471e24cd50a05c5c1aed18f4a6f8b25b208c
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-stremify"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp:
      enabled: true
      type: emptyDir
      mountPath: /tmp
    nuxt-node-modules:
      enabled: true
      type: emptyDir
      mountPath: /nuxt/node_modules
    nuxt:
      enabled: true
      type: emptyDir
      mountPath: /nuxt/.nuxt
    nitro:
      enabled: true
      type: emptyDir
      mountPath: /home/node/app/.nitro
  resources:
    requests:
      cpu: 10m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  envFrom:
  - configMapRef:
      name: stremify-env
  - configMapRef:
      name: elfbot-stremify
      optional: true
  - secretRef:
      name: stremify-env
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      env:
        IPTABLES_BACKEND: nft
        KILLSWITCH: "true"
        LOCAL_NETWORK: 10.0.0.0/8
        LOC: de-frankfurt
        PORT_FORWARDING: "0"
        PORT_PERSIST: "1"
        NFTABLES: "1"
        VPNDNS: "0"
      envFrom:
      - secretRef:
          name: stremify-vpn
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

webstreamr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/webstreamr
    tag: v0.21.0@sha256:6a55e952843d72b528af4643680559c124f86f29ea20d2c93dc8aec0518e6174
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,webstreamr-env,elfbot-webstreamr"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    tmp:
      enabled: true
      type: emptyDir
      mountPath: /tmp
  resources:
    requests:
      cpu: 10m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 51546
  envFrom:
  - configMapRef:
      name: webstreamr-env
  - configMapRef:
      name: elfbot-webstreamr
      optional: true
  addons:
    vpn:
      enabled: true # in case we ever need it
      gluetun:
        image:
          repository: thrnz/docker-wireguard-pia
          tag: latest
      env:
        IPTABLES_BACKEND: nft
        KILLSWITCH: "true"
        LOCAL_NETWORK: 10.0.0.0/8
        LOC: de-frankfurt
        PORT_FORWARDING: "0"
        PORT_PERSIST: "1"
        NFTABLES: "1"
        VPNDNS: "0"
      envFrom:
      - secretRef:
          name: webstreamr-vpn
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

recyclarr:
  enabled: false
  image:
    repository: ghcr.io/recyclarr/recyclarr
    tag: latest@sha256:759540877f95453eca8a26c1a93593e783a7a824c324fbd57523deffb67f48e1
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-recyclarr"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9898
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: recyclarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: recyclarr-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-recyclarr
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: recyclarr
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        if [ ! -f /config/recyclarr.yaml ];
        then
          cp /bootstrap/recyclarr.yaml /config/
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: recyclarr
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
  additionalContainers:
    podinfo:
      image: stefanprodan/podinfo # used to run probes from gatus
    sync:
      image: ghcr.io/recyclarr/recyclarr:latest@sha256:759540877f95453eca8a26c1a93593e783a7a824c324fbd57523deffb67f48e1
      command:
      - /bin/bash
      - -c
      - |
        recyclarr sync
        sleep infinity
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: recyclarr
      envFrom:
      - configMapRef:
          name: recyclarr-env

knightcrawler: &app_knightcrawler
  enabled: false
  image:
    repository: ghcr.io/elfhosted/knightcrawler-addon
    tag: v2.0.28@sha256:0985eb3036a940fc751d78eb224f6791a78d1bd64b2e921983a503b60cd9fcc0
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: knightcrawler
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-knightcrawler,elfbot-torrentio,knightcrawler-env"
      secret.reloader.stakater.com/reload: "knightcrawler-env"
    strategy:
    rollingUpdate:
      unavailable: 1
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 100m
      memory: 256Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 7000
  persistence:
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-knightcrawler
          optional: true
    npm:
      enabled: true
      type: emptyDir
      mountPath: /.npm
    pm2:
      enabled: true
      mountPath: /.pm2
      type: emptyDir
  envFrom:
  - configMapRef:
      name: knightcrawler-env
  - secretRef:
      name: knightcrawler-env

zurg: &app_zurg
  enabled: false
  podLabels:
    app.elfhosted.com/class: debrid
    app.elfhosted.com/name: zurg
  podAnnotations:
    kubernetes.io/egress-bandwidth: "150M" # tested with _kilos in Discord on a 97Mbit remux
  image:
    repository: ghcr.io/elfhosted/zurg-rc
    tag: 2025.05.14.0031-nightly@sha256:513bcc2be2a9fb8b8df7fc458f76747bebcc884776bfbfd4393e16d61b2c1227
  imagePullSecrets:
  - name: ghcr-io-elfhosted    
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-zurg,zurg-env,gluetun-config"
    strategy: Recreate
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: node-role.elfhosted.com/contended
            operator: In
            values:
            - "true"         
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.elfhosted.com/role
              operator: In
              values:
              - nodefinder # use nodefinder in the absense of zurg...
          topologyKey: "kubernetes.io/hostname"
      - weight: 2
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.elfhosted.com/name
              operator: In
              values:
              - zurg # .. but prefer zurg
          topologyKey: "kubernetes.io/hostname"
          namespaceSelector: {}  # i.e., in the absense of any better signal, pick a node which already has zurg on it
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 32Mi
    limits:
      cpu: 1
      memory: 2Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9999
  persistence:
    tmp: *tmp
    backup: *backup # to pin zurg to the node with the backup PVC
    # rclonemountrealdebridzurg: *rclonemountrealdebridzurg
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: zurg
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: zurg
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs          
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: zurg-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-zurg
          optional: true
    shared:
      enabled: true
      mountPath: /shared
      type: emptyDir
      volumeSpec:
        medium: Memory
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755
  envFrom:
  - configMapRef:
      name: zurg-env # this is here so we can use env vars to detect whether to enable warp
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: zurg
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # We need a /config/logs folder
        mkdir -p /config/logs

        # If we don't already have an example config, create one
        if [[ ! -f /config/config.yml ]];
        then
          cp /bootstrap/config.yml /config/
        fi

        # If we don't already have an example plex_update, create one
        if [[ ! -f /config/plex_update.sh ]];
        then
          cp /bootstrap/plex_update.sh /config/
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: zurg
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
    setup:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        # run the setup script from the configmap, so that we can make templated changes
        bash /bootstrap/setup.sh
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: zurg
      - name: example-config
        mountPath: "/bootstrap/"
  addons:
    vpn: &zurg_addons_vpn
      enabled: false # in case we ever need it
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      envFrom:
      - configMapRef:
          name: gluetun-config
          optional: true
      - configMapRef:
          name: zurg-env # this is here so we can use env vars to detect whether to enable warp
      env:
        DOT: "off"
        FIREWALL_INPUT_PORTS: "9999" # 9999 is for zurg
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config
      scripts:
        up: |-
          #!/bin/ash
          echo "connected" > /shared/vpnstatus

        down: |-
          #!/bin/ash
          echo "disconnected" > /shared/vpnstatus
  probes:
    startup:
      spec:
        initialDelaySeconds: 0
        timeoutSeconds: 1
        ## This means it has a maximum of 5*120=720 seconds to start up before it fails
        periodSeconds: 5
        failureThreshold: 120

zurggluetun:
  <<: *app_zurg
  enabled: false
  podLabels:
    app.elfhosted.com/name: zurg
    app.elfhosted.com/class: debrid
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-zurg,zurg-gluetun-config,zurg-env"
  service:
    main:
      nameOverride: zurg
      enabled: true # necessary for probes, but probes aren't working with vpn addon currently
  env:
    WAIT_FOR_VPN: "true"
  addons:
    vpn:
      enabled: true
      <<: *zurg_addons_vpn
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      envFrom:
      - configMapRef:
          name: gluetun-config

zurgranger:
  <<: *app_zurg
  podLabels:
    app.elfhosted.com/name: zurg
    app.elfhosted.com/class: dedicated
  podAnnotations:
    kubernetes.io/egress-bandwidth: "500M"
  enabled: false
  automountServiceAccountToken: false
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-zurg"
  affinity: *dedicated_affinity # force zurg to go onto the dedicated nodes
  resources: *ranger_zurg_resources

plexdebrid: &app_plexdebrid
  enabled: false
  # podLabels:
  #   app.elfhosted.com/name: plexdebrid
  image:
    repository: ghcr.io/elfhosted/plex-debrid
    tag: rolling@sha256:0c0251d2aef532ba62660b719b7e37e72b4eb262ca18e21d68b9509d305e12a1
  podLabels:
    app.elfhosted.com/name: plex-debrid    
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex-debrid"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because of s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
  resources:
    requests:
      cpu: 2m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 3Gi
  ingress:
    main:
      enabled: false
  envFrom:
  - secretRef:
      name: plex-debrid-env
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plex-debrid
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex-debrid
      - mountPath: /tmp
        name: tmp

clidebrid: &app_clidebrid
  enabled: false
  podLabels:
    app.elfhosted.com/name: cli-debrid
  image:
    repository: ghcr.io/elfhosted/cli_debrid-dev
    tag: v0.6.57@sha256:31bff66ff07a1902cd9f1ce63414915d854562d28f22efa2d44c26597d9d48c4
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-cli-debrid,cli-debrid-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work because of s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
  envFrom:
  - configMapRef:
      name: cli-debrid-env
  - configMapRef:
      name: elfbot-cli-debrid
      optional: true        
  resources:
    requests:
      cpu: 2m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 3Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 5000
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /user/
      subPath: cli-debrid
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /user/logs
      subPath: cli-debrid
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-cli-debrid
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: cli-debrid
      - mountPath: /tmp
        name: tmp

puter:
  enabled: false
  podLabels:
    app.elfhosted.com/name: puter
  image:
    repository: ghcr.io/elfhosted/puter
    tag: v2.5.1@sha256:4566493190067df16091134e9fa845ffa21f6e517874eb146cb4bca9c4177ca9
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-puter"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false # doesn't work because of s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  env:
    PUID: 568
    PGID: 568
  resources:
    requests:
      cpu: 2m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 3Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 4100
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config/
      subPath: puter
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-puter
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: puter
      - mountPath: /tmp
        name: tmp

plexdebriddebridlink: 
  <<: *app_plexdebrid  
  enabled: false
  podLabels:
    app.elfhosted.com/name: plex-debrid-debridlink  
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex-debrid-debridlink"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plex-debrid-debridlink
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid-debridlink
          optional: true          
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex-debrid-debridlink
      - mountPath: /tmp
        name: tmp

plexdebridtorbox: 
  <<: *app_plexdebrid  
  enabled: false  
  podLabels:
    app.elfhosted.com/name: plex-debrid-torbox  
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex-debrid-torbox"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plex-debrid-torbox
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid-torbox
          optional: true     
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex-debrid-torbox
      - mountPath: /tmp
        name: tmp               

plexdebridpremiumize: 
  <<: *app_plexdebrid  
  enabled: false  
  podLabels:
    app.elfhosted.com/name: plex-debrid-premiumize    
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex-debrid-premiumize"  
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plex-debrid-premiumize
      volumeSpec:
        persistentVolumeClaim:
          claimName: config    
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid-premiumize
          optional: true                     
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex-debrid-premiumize
      - mountPath: /tmp
        name: tmp

# This is a copy of plexdebrid plumbed into the user's VPN
plexdebridalldebrid: 
  <<: *app_plexdebrid
  podLabels:
    app.elfhosted.com/name: plex-debrid-alldebrid    
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-plex-debrid-alldebrid"  
  enabled: false
  persistence:
    <<: *storagemounts
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: plex-debrid-alldebrid
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid-alldebrid
          optional: true   
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: plex-debrid-alldebrid
      - mountPath: /tmp
        name: tmp                 
  addons:
    vpn:
      enabled: true
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      envFrom:
      - configMapRef:
          name: gluetun-config
      env:
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8
        DOT: "off"
        FIREWALL_INPUT_PORTS: "3001" # 9999 is for rclone
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

codeserver:
  enabled: false
  # runtimeClassName: kata
  image:
    repository: ghcr.io/elfhosted/codeserver
    tag: 4.100.2@sha256:37db4c4e2c38376d91ba2a8983bb64c81c6e01d00550dbe922c060c917db28a4
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-codeserver"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true # doesn't work because of s6
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    fsGroup: 568
    # runAsUser: 568
    # runAsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 15m
      memory: 200Mi
    limits:
      cpu: 2
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config # no subpath, codeserver wants to see all
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-plex-debrid
          optional: true
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: codeserver-config
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: codeserver
      - mountPath: /tmp
        name: tmp
    copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        # If we don't already have an example config, create one
        mkdir -p /config/.config/code-server/
        if [ ! -f /config/.config/code-server/config.yaml ];
        then
          cp /bootstrap/config.yaml /config/.config/code-server/
        fi
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: codeserver
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true

doplarr: &app_doplarr
  enabled: false
  image:
    repository: ghcr.io/elfhosted/doplarr
    tag: v3.6.3@sha256:7703328fc7f9f4190606ba9f95e867a64db79bd06c19e923a83ad6f939f89097
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-doplarr,doplarr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  envFrom:
  - configMapRef:
      name: elfbot-doplarr
      optional: true

profilarr:
  enabled: false
  hostAliases:
  - ip: "127.0.0.1"
    hostnames:  
    - "backend" 
  image:
    repository: ghcr.io/elfhosted/profilarr-frontend
    tag: v1.0.1@sha256:1f2165783bc2030da7ca84f8b2aa4d709a24d51f05cd1de33cb3b4a438d399d0
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-profilarr,profilarr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3000
  envFrom:
  - configMapRef:
      name: elfbot-profilarr
      optional: true
  persistence:
    tmp: *tmp
    logs:
      enabled: true
      type: custom
      mountPath: /app/logs
      subPath: profilarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
    npm:
      enabled: true
      type: emptyDir
      mountPath: /.npm
      sizeLimit: 1Gi          
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: profilarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-profilarr
          optional: true   
    app:
      enabled: "true"
      type: emptyDir
      mountPath: /app
      volumeSpec:
        medium: Memory           
  additionalContainers:
    backend:
      image: ghcr.io/elfhosted/profilarr-backend:v1.0.1@sha256:b1f776ae78836917d2808bd4ab3a0311be58c2627b109e529e052c678c74e8c3     
      volumeMounts:
      - name: config
        mountPath: /config
        subPath: profilarr
  initContainers:
    setup:
      image: ghcr.io/elfhosted/profilarr-frontend:v1.0.1@sha256:1f2165783bc2030da7ca84f8b2aa4d709a24d51f05cd1de33cb3b4a438d399d0
      command:
      - /bin/bash
      - -c
      - |
        # copy the image's build-cache directory into the emptyDir
        cp /app/* /tmp -rfp
      volumeMounts:
      - mountPath: /tmp
        name: app


pulsarr: &app_pulsarr
  enabled: false
  image:
    repository: ghcr.io/elfhosted/pulsarr
    tag: v0.3.7@sha256:aa433d66f3a462076491e8767703d89fedb50c1da0ff5ca3d73c5912e0c8e42b
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-pulsarr,pulsarr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  persistence:
    backup: *backup
    tmp: *tmp
    logs:
      enabled: true
      type: custom
      mountPath: /app/data/logs
      subPath: pulsarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs    
    config:
      enabled: true
      type: custom
      mountPath: /app/data
      subPath: pulsarr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-pulsarr
          optional: true  
    app:
      enabled: "true"
      type: emptyDir
      mountPath: /app
      volumeSpec:
        medium: Memory                
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3003
  envFrom:
  - configMapRef:
      name: pulsarr-env
  - configMapRef:
      name: elfbot-pulsarr
      optional: true      
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: pulsarr
      - mountPath: /tmp
        name: tmp
    setup:
      image: ghcr.io/elfhosted/pulsarr:v0.3.7@sha256:aa433d66f3a462076491e8767703d89fedb50c1da0ff5ca3d73c5912e0c8e42b
      command:
      - /bin/ash
      - -c
      - |
        # copy the image's build-cache directory into the emptyDir
        cp /app/* /tmp -rfp
      volumeMounts:
      - mountPath: /tmp
        name: app
  additionalContainers:
    apprise-api:
      image: ghcr.io/elfhosted/apprise-api:v1.2.0@sha256:ed6e963a2cdd1bdef63c51e82ca7b8bd2549f285f2e0510ea78d6f0adac40588
      volumeMounts:
      - name: config
        mountPath: /config
        subPath: pulsarr/apprise
      env:
        PUID: 568
        PGID: 568
        APPRISE_STATEFUL_MODE: simple
        APPRISE_WORKER_COUNT: "1"

dispatcharr:
  enabled: false
  image:
    repository: ghcr.io/dispatcharr/dispatcharr
    tag: 0.5.0@sha256:c06d10cc04c4313620d56349206e2677e6d34f7c72fa8e57e782b4e5bf88f3e0
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-dispatcharr,dispatcharr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 993      
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /data
      subPath: dispatcharr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-dispatcharr
          optional: true
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"       
  command:
  - gunicorn 
  - --workers=4
  - --worker-class=gevent 
  - --timeout=300 
  - --bind 0.0.0.0:9191
  - dispatcharr.wsgi:application   
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 1
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9191
  envFrom:
  - configMapRef:
      name: dispatcharr-env
  - configMapRef:
      name: elfbot-dispatcharr
      optional: true      
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: dispatcharr
      - mountPath: /tmp
        name: tmp

requestrr:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/requestrr
    tag: v2.1.6@sha256:d9daf341af2608f8351ae4cfdb6f685c1ea675e18b88860d0bfbc6343202402b
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-requestrr,requestrr-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  persistence:
    tmp: *tmp
    backup: *backup
    config:
      enabled: true
      type: custom
      mountPath: /app/config
      subPath: requestrr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-requestrr
          optional: true
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 1Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 4545
  envFrom:
  - configMapRef:
      name: elfbot-requestrr
      optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: requestrr
      - mountPath: /tmp
        name: tmp

debridav: &app_debridav
  enabled: false
  priorityClassName: tenant-normal
  podLabels:
    app.elfhosted.com/name: debridav  
  image:
    repository: ghcr.io/elfhosted/debridav
    tag: 0.9.5@sha256:1e596ddb97a7e6a4827b1dfc0718992b0c432416841a8a4c3dd3cd7491850c3c
  imagePullSecrets:
  - name: ghcr-io-elfhosted    
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-debridav,debridav-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    <<: *storagemounts
    tmp: *tmp
    backup: *backup
    dev-shm:
      enabled: "true"
      type: emptyDir
      volumeSpec:
        medium: Memory    
    config:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs          
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-debridav
          optional: true    
    import:
      enabled: true
      type: emptyDir
      mountPath: /import
      sizeLimit: 1Gi                   
  envFrom:
  - configMapRef:
      name: debridav-env
  - configMapRef:
      name: elfbot-debridav
      optional: true      
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080      
  probes:
    liveness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /actuator/health/liveness
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10
    readiness:
      custom: true
      enabled: true
      spec:
        failureThreshold: 5
        httpGet:
          path: /actuator/health/readiness
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 120
        timeoutSeconds: 10         
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 2Gi
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: debridav
      - mountPath: /tmp
        name: tmp  
    # Only trigger this when moving to database
    setup-postgres:
      image: *tooling_image
      command:
      - /bin/bash
      - -c
      - |
        set -x
        set -e

        mkdir -p /config/database
        mkdir -p /config/backups
        chown elfie:elfie /config/database -R
      volumeMounts:
      - mountPath: /config
        name: config
        subPath: debridav
      securityContext:
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault                
  additionalContainers:
    make-folders:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - |

        # Define the folders to check/create
        FOLDERS=("/storage/debridav/movies" "/storage/debridav/movies-4k" "/storage/debridav/series" "/storage/debridav/series-4k")

        # Function to check and create folders
        create_folders() {
          local all_created=true
          
          for folder in "${FOLDERS[@]}"; do
            if [ ! -d "$folder" ]; then
              echo "Folder $folder does not exist. Creating..."
              
              # sleep so that we have time to avoid a race on local vs dbfolders
              sleep 20s
              mkdir -p "$folder"
              
              # Check if creation was successful
              if [ ! -d "$folder" ]; then
                echo "Error creating $folder. Will retry in 10 seconds."
                all_created=false
              else
                echo "Successfully created $folder."
              fi
            else
              echo "Folder $folder already exists."
            fi
          done
          
          return $([ "$all_created" = true ] && echo 0 || echo 1)
        }

        # Main loop to create folders
        while true; do
          if create_folders; then
            echo "All required folders exist or were successfully created."
            break
          else
            echo "Waiting 10 seconds before retrying..."
            sleep 10
          fi
        done

        echo "Success! All required folders are now available."
        echo "Script will now sleep indefinitely."

        # Sleep forever
        while true; do
          sleep 3600  # Sleep for an hour at a time (to be a bit nicer to the system than an infinite tight loop)
        done
      volumeMounts:
      - name: rclonemountdebridav
        mountPath: /storage/debridav
    database:
      image: postgres:17-alpine
      args:
      - -c
      - shared_buffers=500MB
      - -c
      - work_mem=4MB
      - -c
      - effective_cache_size=1GB
      - -c
      - max_connections=310
      env:
        - name: POSTGRES_PASSWORD
          value: debridav
        - name: POSTGRES_DB
          value: debridav
        - name: POSTGRES_USER
          value: debridav
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: config
        subPath: debridav/database
      - mountPath: /dev/shm
        name: dev-shm
      resources:
        requests:
          cpu: 0m
          memory: 1Mi
        limits:
          cpu: 500m
          memory: 8Gi
    fakearr:
      image: ghcr.io/elfhosted/fakearr:rolling@sha256:9734625ab6bc3f669d5ed5d8d58c6acd45000c068acfc36807c5c97f03531bbf
      envFrom:
      - configMapRef:
          name: debridav-env      
      - configMapRef:      
          name: elfbot-debridav
          optional: true  
      volumeMounts:
      - name: tmp
        mountPath: /tmp    
          
rclonedebridlink:
  enabled: false
  image:
    repository: ghcr.io/elfhosted/davdebrid
    tag: v1.2.2@sha256:6571776a022a36889d1cc8392440ad58fa2654535e82b9741f1c287e045c7fd0
  priorityClassName: tenant-normal
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  podLabels:
    app.elfhosted.com/name: debridlink
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-debridlink,debridlink-env"
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
    readOnlyRootFilesystem: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: debridlink
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
  envFrom:
  - configMapRef:
      name: debridlink-env
  - configMapRef:
      name: elfbot-debridlink
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 100Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8080


rclonealldebrid:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.69.3@sha256:9396f8c3a649dba9f91169a62142c9f3e6da00974bda573d864b62c07f78234e
  command:
  - /debrid-provider.sh
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-alldebrid,alldebrid-config,gluetun-config"
  podAnnotations:
    kubernetes.io/egress-bandwidth: "128M"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    cache:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      sizeLimit: 1Gi
    config: 
      enabled: "true"
      type: emptyDir
      mountPath: /config
    bootstrap:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: alldebrid-config
    tinyproxy-conf:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: alldebrid-tinyproxy-conf 
  additionalContainers:
    # Use this to provied proxied access to mediaflowproxy
    tinyproxy:
      image: docker.io/kalaksi/tinyproxy
      volumeMounts:
      - name: tinyproxy-conf
        mountPath: /etc/tinyproxy/tinyproxy.conf
        subPath: tinyproxy.conf                      
  initContainers:
    setup:
      image: ghcr.io/elfhosted/rclone:1.69.3@sha256:9396f8c3a649dba9f91169a62142c9f3e6da00974bda573d864b62c07f78234e
      command:
      - /bin/ash
      - -c
      - |
        set -x

        # Create directory structure

        OBSCURED_PASS=$(rclone obscure doesntmatter)
        cp /bootstrap/rclone-debrid-provider.conf /config/
        sed -i "s/REPLACEUSER/$USER/" /config/rclone-debrid-provider.conf
        sed -i "s/REPLACEPASS/$OBSCURED_PASS/" /config/rclone-debrid-provider.conf
        
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /bootstrap
        name: bootstrap
      resources: *default_resources
      securityContext: *default_securitycontext
      envFrom:
      - configMapRef:
          name: elfbot-alldebrid
          optional: true
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9999      
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi
  addons:
    vpn:
      enabled: true
      gluetun:
        image:
          repository: ghcr.io/elfhosted/gluetun
          tag: 3.40.0@sha256:59b0cc8a9412b9a4645d1f12dff0daba80b8be88acd1b5d1a0fb6dfa625eb9a5
      envFrom:
      - configMapRef:
          name: gluetun-config
      env:
        DOT: "off"
        FIREWALL_INPUT_PORTS: "9999,8888" # 9999 is for rclone, 8888 is tinyproxy
        HTTP_CONTROL_SERVER_PORT: "8000"
        HTTP_CONTROL_SERVER_ADDRESS: ":8000"
        HEALTH_SERVER_ADDRESS: "127.0.0.1:9991"
        FIREWALL_OUTBOUND_SUBNETS: 10.0.0.0/8,192.168.0.0/16,172.16.0.0/20
        DNS_KEEP_NAMESERVER: "on"        
      securityContext:
        privileged: true
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
            - SYS_MODULE
      config: # We have to set this to null so that we can override with our own config

rclonepremiumize:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.69.3@sha256:9396f8c3a649dba9f91169a62142c9f3e6da00974bda573d864b62c07f78234e
  command:
  - /debrid-provider.sh
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-premiumize,premiumize-config"
  podAnnotations:
    kubernetes.io/egress-bandwidth: "128M"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    cache:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      sizeLimit: 1Gi
    config: 
      enabled: "true"
      type: emptyDir
      mountPath: /config
    bootstrap:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: premiumize-config
  initContainers:
    setup:
      image: ghcr.io/elfhosted/rclone:1.69.3@sha256:9396f8c3a649dba9f91169a62142c9f3e6da00974bda573d864b62c07f78234e
      command:
      - /bin/ash
      - -c
      - |
        set -x

        # Create directory structure

        OBSCURED_PASS=$(rclone obscure "$PASS")
        cp /bootstrap/rclone-debrid-provider.conf /config/
        sed -i "s/REPLACEUSER/$USER/" /config/rclone-debrid-provider.conf
        sed -i "s/REPLACEPASS/$OBSCURED_PASS/" /config/rclone-debrid-provider.conf
        
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /bootstrap
        name: bootstrap
      resources: *default_resources
      securityContext: *default_securitycontext
      envFrom:
      - configMapRef:
          name: elfbot-premiumize
          optional: true
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9999      
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi

rclonetorbox:
  enabled: false
  priorityClassName: tenant-normal
  image:
    repository: ghcr.io/elfhosted/rclone
    tag: 1.69.3@sha256:9396f8c3a649dba9f91169a62142c9f3e6da00974bda573d864b62c07f78234e
  command:
  - /debrid-provider.sh
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-torbox,torbox-config"
  podAnnotations:
    kubernetes.io/egress-bandwidth: "128M"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  automountServiceAccountToken: false
  persistence:
    cache:
      enabled: true
      type: emptyDir
      mountPath: /home/elfie/.cache
      sizeLimit: 1Gi
    # we'll run the obscure command and copy the config into here
    config: 
      enabled: "true"
      type: emptyDir
      mountPath: /config
    bootstrap:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: torbox-config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-torbox
          optional: true          
  initContainers:
    setup:
      image: ghcr.io/elfhosted/rclone:1.69.3@sha256:9396f8c3a649dba9f91169a62142c9f3e6da00974bda573d864b62c07f78234e
      command:
      - /bin/ash
      - -c
      - |
        set -x

        # Create directory structure

        OBSCURED_PASS=$(rclone obscure "$PASS")
        cp /bootstrap/rclone-debrid-provider.conf /config/
        sed -i "s/REPLACEUSER/$USER/" /config/rclone-debrid-provider.conf
        sed -i "s/REPLACEPASS/$OBSCURED_PASS/" /config/rclone-debrid-provider.conf
        
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /bootstrap
        name: bootstrap
      - mountPath: /elfbot
        name: elfbot
      resources: *default_resources
      securityContext: *default_securitycontext
      envFrom:
      - configMapRef:
          name: elfbot-torbox
          optional: true

  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 9999
  envFrom:
  - configMapRef:
      name: elfbot-torbox
      optional: true              
  ingress:
    main:
      enabled: false
  resources:
    requests:
      cpu: 0m
      memory: 60Mi
    limits:
      cpu: 150m
      memory: 512Mi


decypharr: &app_decypharr
  enabled: false
  image:
    repository: ghcr.io/elfhosted/decypharr
    tag: v1.0.2@sha256:0103b9ac686eb7c84e5b5a7e88f2e0ce2a7ad0f3bf74ef686806e1beba0c191c
  priorityClassName: tenant-normal
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-decypharr,decypharr-example-config"
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
    readOnlyRootFilesystem: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 1
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8282
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config
      subPath: decypharr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    logs:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: decypharr
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs    
    example-config:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: decypharr-example-config    
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-decypharr
          optional: true                      
  initContainers:
    a-copy-example-config:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x
        # set -e
        # If we don't already have an example config, create one
        if [ ! -f /config/config.json ];
        then
          cp /bootstrap/config.json /config/
        fi
        touch /config/i-am-bootstrapped

        # Turn on WebDAV
        contents="$(jq '.debrids |= map(if type == "object" then . + {use_webdav: true} else . end)' /config/config.json)" && \
          echo -E "${contents}" > /config/config.json

        # Set webdav to filename mode
        contents="$(jq '.debrids |= map(. + {folder_naming: "filename"})' /config/config.json)" && \
          echo -E "${contents}" > /config/config.json          

        # Set repair to use webdav
        contents="$(jq '.debrids |= map(if type == "object" then . + {use_webdav: true} else . end)' /config/config.json)" && \
          echo -E "${contents}" > /config/config.json      

      volumeMounts:
      - mountPath: /config/
        name: config
        subPath: decypharr
      - name: example-config
        mountPath: "/bootstrap/"
      securityContext: *default_securitycontext  
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: decypharr
      - mountPath: /tmp
        name: tmp  
     

blackhole: &app_blackhole
  enabled: false
  image:
    repository: ghcr.io/elfhosted/wests-blackhole-script
    tag: v1.5.1@sha256:71c8321c12ec2a643aca899c38f144fb31c4ba302612e139499ca966d633a316
  priorityClassName: tenant-normal
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-blackhole,blackhole-env"
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: false
    readOnlyRootFilesystem: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
  automountServiceAccountToken: false
  env:
    BLACKHOLE_RADARR_PATH: "radarr"
    BLACKHOLE_SONARR_PATH: "sonarr"
  envFrom:
  - configMapRef:
      name: blackhole-env
  - configMapRef:
      name: elfbot-blackhole
      optional: true
  resources:
    requests:
      cpu: 0m
      memory: 1Mi
    limits:
      cpu: 100m
      memory: 100Mi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 3001
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: blackhole
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs

blackhole4k:
  <<: *app_blackhole
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: blackhole4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
  env:
    BLACKHOLE_RADARR_PATH: "radarr4k"
    BLACKHOLE_SONARR_PATH: "sonarr4k"
  envFrom:
  - configMapRef:
      name: blackhole-env
  - configMapRef:
      name: elfbot-blackhole
      optional: true   
  - configMapRef:
      name: elfbot-blackhole4k
      optional: true    

blackholetorbox:
  <<: *app_blackhole
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-blackholetorbox,blackholetorbox-env"
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: blackholetorbox
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
  env:
    BLACKHOLE_RADARR_PATH: "radarr"
    BLACKHOLE_SONARR_PATH: "sonarr"
  envFrom:
  - configMapRef:
      name: blackholetorbox-env
  - configMapRef:
      name: elfbot-blackholetorbox
      optional: true
  initContainers:
    setup:
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /bin/bash
      - -c
      - |
        set -x

        # Create directory structure
        mkdir -p /storage/symlinks/blackholetorbox/radarr/completed
        mkdir -p /storage/symlinks/blackholetorbox/radarr/processing
        mkdir -p /storage/symlinks/blackholetorbox/sonarr/completed
        mkdir -p /storage/symlinks/blackholetorbox/sonarr/processing
      volumeMounts:
      - mountPath: /storage/symlinks
        name: symlinks
      resources: *default_resources
      securityContext: *default_securitycontext

blackholetorbox4k:
  <<: *app_blackhole
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-blackholetorbox,blackholetorbox-env"
  persistence:
    <<: *storagemounts
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /config/logs
      subPath: blackholetorbox4k
      volumeSpec:
        persistentVolumeClaim:
          claimName: logs
  env:
    BLACKHOLE_RADARR_PATH: "radarr4k"
    BLACKHOLE_SONARR_PATH: "sonarr4k"
  envFrom:
  - configMapRef:
      name: blackholetorbox-env
  - configMapRef:
      name: elfbot-blackholetorbox
      optional: true
  - configMapRef:
      name: elfbot-blackholetorbox4k
      optional: true      

channelsdvr:
  enabled: false
  image:
    repository: fancybits/channels-dvr
    tag: latest@sha256:284fed6f4ee5150d41d9a7f247a63e190f6f1c3a4e4bc740f029df6d36955d29
  priorityClassName: tenant-normal
  podAnnotations:
    kubernetes.io/egress-bandwidth: "125M"
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-channelsdvr"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 15m
      memory: 200Mi
    limits:
      cpu: 1
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 8089
  persistence:
    <<: *storagemounts
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    tmp: *tmp
    config:
      enabled: true
      type: custom
      mountPath: /channels-dvr
      subPath: channelsdvr
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-channelsdvr
          optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: channelsdvr
      - mountPath: /tmp
        name: tmp


immich:
  enabled: false
  image:
    repository: ghcr.io/immich-app/immich-server
    tag: v1.134.0@sha256:073fc04c7e3d18ace466c20763809cf17aa55765ed610f12971b392a6a80b50c
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "elfbot-all,elfbot-immich,immich-env"
  affinity: *standard_affinity
  tolerations: *standard_tolerations
  securityContext:
    runAsUser: 568
    runAsGroup: 568
    privileged: true
  podSecurityContext:
    fsGroup: 568
    fsGroupChangePolicy: "Always"
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 993
  automountServiceAccountToken: false
  resources:
    requests:
      cpu: 15m
      memory: 200Mi
    limits:
      cpu: 500m
      memory: 4Gi
  ingress:
    main:
      enabled: false
  service:
    main:
      enabled: true # necessary for probes
      ports:
        http:
          port: 2283
  persistence:
    <<: *storagemounts
    render-device:
      enabled: "true"
      type: hostPath
      hostPath: "/dev/dri/renderD128"
      mountPath: "/dev/dri/renderD128"
    tmp: *tmp
    config:
      enabled: true
      type: custom
      subPath: immich
      volumeSpec:
        persistentVolumeClaim:
          claimName: config
    upload:
      enabled: true
      type: emptyDir
      mountPath: /usr/src/app/upload
      sizeLimit: 1Gi
    upload-encoded:
      enabled: true
      type: emptyDir
      mountPath: /usr/src/app/upload/encoded-video
      sizeLimit: 1Gi
    elfbot:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: elfbot-immich
          optional: true
  envFrom:
  - configMapRef:
      name: immich-env
  - configMapRef:
      name: elfbot-immich
      optional: true
  initContainers:
    bootstrap:
      <<: *bootstrap
      volumeMounts:
      - mountPath: /etc/elfbot
        name: elfbot
      - mountPath: /storage/backup
        name: backup
      - mountPath: /config
        name: config
        subPath: immich
      - mountPath: /tmp
        name: tmp
  additionalContainers:
    ml:
      image: ghcr.io/immich-app/immich-machine-learning:v1.131.2
      envFrom:
      - configMapRef:
          name: immich-env
      resources:
        requests:
          cpu: 15m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 4Gi
    database:
      image: ghcr.io/immich-app/postgres:14-vectorchord0.3.0-pgvectors0.2.0
      env:
        POSTGRES_INITDB_ARGS: '--data-checksums'
        POSTGRES_PASSWORD: immich
        POSTGRES_USER: immich
        POSTGRES_DB: immich
      volumeMounts:
        - name: config
          subPath: immich/database
          mountPath: /var/lib/postgresql/data
      resources:
        requests:
          cpu: 15m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 4Gi
    redis:
      image: docker.io/redis:7.4-alpine@sha256:f773b35a95e170d92dd4214a3ec4859b1b7960bf56896ae687646d695f311187
      envFrom:
      - configMapRef:
          name: immich-env
      resources:
        requests:
          cpu: 15m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 4Gi

kubernetesdashboard:

  ## Name of Priority Class of pods
  priorityClassName: "tenant-normal"

  ## Pod resource requests & limits
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 1
      memory: 256Mi

  extraArgs:
    - --enable-skip-login
    - --enable-insecure-login
    - --system-banner=Built</A> with ❤️ by <A HREF="https://funkypenguin.co.nz">@funkypenguin</A> and friends (<I><A HREF="https://chat.funkypenguin.co.nz">join us!</A></I>)

  ## Serve application over HTTP without TLS
  ##
  ## Note: If set to true, you may want to add --enable-insecure-login to extraArgs
  protocolHttp: true

  # Global dashboard settings
  settings:
    ## Cluster name that appears in the browser window title if it is set
    clusterName: "ElfHosted"
    # defaultNamespace: "{{ .Release.Namespace }}"
    # namespaceFallbackList: [ "{{ .Release.Namespace }}" ]

    ## Max number of items that can be displayed on each list page
    itemsPerPage: 10
    ## Number of seconds between every auto-refresh of logs
    logsAutoRefreshTimeInterval: 5
    ## Number of seconds between every auto-refresh of every resource. Set 0 to disable
    resourceAutoRefreshTimeInterval: 5
    ## Hide all access denied warnings in the notification panel
    disableAccessDeniedNotifications: true

  ## Metrics Scraper
  ## Container to scrape, store, and retrieve a window of time from the Metrics Server.
  ## refs: https://github.com/kubernetes-sigs/dashboard-metrics-scraper
  metricsScraper:
    ## Wether to enable dashboard-metrics-scraper
    enabled: true
    image:
      repository: kubernetesui/metrics-scraper
      tag: v1.0.9
    resources: {}
    ## SecurityContext especially for the kubernetes dashboard metrics scraper container
    ## If not set, the global containterSecurityContext values will define these values
    # containerSecurityContext:
    #   allowPrivilegeEscalation: false
    #   readOnlyRootFilesystem: true
    #   runAsUser: 1001
    #   runAsGroup: 2001
  #  args:
  #    - --log-level=info
  #    - --logtostderr=true

  # Don't auto-create RBAC for us, we'll do it manually
  rbac:
    create: false

  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: kubernetes-dashboard

# optional but disabled by default to prevent errors


gluetun:
  enabled: false # just to avoid errors
cometproxystreaming:
  enabled: false
mediafusionproxystreaming:
  enabled: false
elfassesment:
  enabled: false

# The hobbit apps
zurghobbit:
  <<: *app_zurg
  podAnnotations: *hobbit_streamer_podAnnotations
  # zurg is the "anchor" which keeps all the other apps on the same node
  affinity: *dedicated_affinity # force zurg to go onto the dedicated nodes
  resources: *hobbit_zurg_resources

zurghalfling:
  <<: *app_zurg
  podAnnotations: *halfling_streamer_podAnnotations
  # zurg is the "anchor" which keeps all the other apps on the same node
  affinity: *dedicated_affinity # force zurg to go onto the dedicated nodes
  resources: *halfling_zurg_resources

zurgnazgul:
  <<: *app_zurg
  podAnnotations: *nazgul_streamer_podAnnotations
  # zurg is the "anchor" which keeps all the other apps on the same node
  affinity: *dedicated_affinity # force zurg to go onto the dedicated nodes
  resources: *nazgul_zurg_resources

plexhobbit:
  <<: *app_plex
  podAnnotations: *hobbit_streamer_podAnnotations
  resources: *hobbit_streamer_resources
  persistence:
    <<: *app_plex_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

jellyfinhobbit:
  <<: *app_jellyfin
  podAnnotations: *hobbit_streamer_podAnnotations
  resources: *hobbit_streamer_resources
  persistence:
    <<: *app_jellyfin_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

embyhobbit:
  <<: *app_emby
  podAnnotations: *hobbit_streamer_podAnnotations
  resources: *hobbit_streamer_resources
  persistence:
    <<: *app_emby_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

plexranger:
  <<: *app_plex
  persistence:
    <<: *app_plex_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g
  podAnnotations: *ranger_streamer_podAnnotations
  resources: *ranger_streamer_resources

plexhalfling:
  <<: *app_plex
  persistence:
    <<: *app_plex_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  
  podAnnotations: *halfling_streamer_podAnnotations
  resources: *halfling_streamer_resources

jellyfinhalfling:
  <<: *app_jellyfin
  podAnnotations: *halfling_streamer_podAnnotations
  resources: *halfling_streamer_resources
  persistence:
    <<: *app_jellyfin_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: jellygfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

embyhalfling:
  <<: *app_emby
  podAnnotations: *halfling_streamer_podAnnotations
  resources: *halfling_streamer_resources
  persistence:
    <<: *app_emby_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

plexnazgul:
  <<: *app_plex
  persistence:
    <<: *app_plex_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: plex
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  
  podAnnotations: *nazgul_streamer_podAnnotations
  resources: *nazgul_streamer_resources

jellyfinnazgul:
  <<: *app_jellyfin
  podAnnotations: *nazgul_streamer_podAnnotations
  resources: *nazgul_streamer_resources
  persistence:
    <<: *app_jellyfin_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: jellyfin
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

embynazgul:
  <<: *app_emby
  podAnnotations: *nazgul_streamer_podAnnotations
  resources: *nazgul_streamer_resources
  persistence:
    <<: *app_emby_persistence
    transcode:
      enabled: true
      type: custom
      mountPath: /transcode
      subPath: emby
      volumeSpec:
        persistentVolumeClaim:
          claimName: transcode-50g  

# We use these to set aside resources for dedicated bundles
starter: &app_resource_reserver
  enabled: false
  image:
    repository: ghcr.io/elfhosted/tooling
    tag: focal-20240530@sha256:458d1f3b54e9455b5cdad3c341d6853a6fdd75ac3f1120931ca3c09ac4b588de
  priorityClassName: tenant-normal
  controller:
    annotations:
      configmap.reloader.stakater.com/reload: "tooling-scripts" # Reload the deployment every time the yaml config changes
  securityContext:
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  # we mount config to force the resource-reserver to run on the same node as the volumes
  persistence:
    config:
      enabled: true
      type: custom
      mountPath: /config
      volumeSpec:
        persistentVolumeClaim:
          claimName: config    
    tooling-scripts:
      enabled: "true"
      type: "custom"
      volumeSpec:
        configMap:
          name: tooling-scripts
          defaultMode: 0755          
  podSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 568
    runAsGroup: 568
    runAsNonRoot: true
    fsGroup: 568
    fsGroupChangePolicy: "OnRootMismatch"
  command:
  - /bin/bash
  - -c
  - |
    echo "This pod simple reserves contended resources for a tenant"

    sleep infinity
  affinity: *standard_affinity
  service:
    main:
      enabled: false
  probes:
    liveness:
      enabled: false
    startup:
      enabled: false
    readiness:
      enabled: false      
  resources:
    requests: 
      cpu: 250m
      memory: 1Mi
  initContainers:
    update-dns:  &update_dns_on_init
      image: *tooling_image
      imagePullPolicy: IfNotPresent
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - /tooling-scripts/update-dns-on-init.sh
      env:
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: ELF_TENANT_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['app.kubernetes.io/instance']
      envFrom:
      - secretRef:
          name: cloudflare-api-token
      volumeMounts:
      - mountPath: /tooling-scripts
        name: tooling-scripts
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        readOnlyRootFilesystem: true
  additionalContainers:
    clean-up-dns:
      <<: *update_dns_on_init
      command:
      - /usr/bin/dumb-init
      - /bin/bash
      - -c
      - /tooling-scripts/clean-up-dns-on-termination.sh      

streamer:
  enabled: false
  <<: *app_resource_reserver

hobbit:
  enabled: false
  <<: *app_resource_reserver

ranger:
  enabled: false
  <<: *app_resource_reserver

halfling:
  enabled: false
  <<: *app_resource_reserver

nazgul:
  enabled: false
  <<: *app_resource_reserver

# This file must end on a single newline
