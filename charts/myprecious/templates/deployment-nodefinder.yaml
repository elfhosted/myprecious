# This job is intended to start upon first installation, "anchoring" all of the replicationdestination restores to the desired node

# Here, we check for pre-existing volumes, so that later on we can optionally mount them, forcing nodefinder onto the node which already has our volumes:
{{- $pvc_config := (lookup "v1" "PersistentVolumeClaim" .Release.Namespace "config" ) }}
{{- $pvc_symlinks := (lookup "v1" "PersistentVolumeClaim" .Release.Namespace "symlinks" ) }}

apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}-nodefinder
  labels:
    app.elfhosted.com/role: nodefinder
  annotations:
    helm.sh/hook: pre-install
    helm.sh/hook-weight: "-10"  # Run before all the replicationdestinations
spec:
  selector:
    matchLabels: 
      app.elfhosted.com/role: nodefinder
  template:
    metadata:
      labels: 
        app.elfhosted.com/role: nodefinder
    spec:
      priorityClassName: tenant-normal
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              {{- if or .Values.hobbit.enabled .Values.ranger.enabled .Values.halfling.enabled .Values.nazgul.enabled }}
              - key: node-role.elfhosted.com/dedicated
              {{- else }}
              - key: node-role.elfhosted.com/contended
              {{- end }}
                operator: In
                values:
                - "true"
        # fill up the gretels, but only on elfhosted.com
        {{- if and (or .Values.hobbit.enabled .Values.ranger.enabled .Values.halfling.enabled .Values.nazgul.enabled) (eq .Values.dns_domain "elfhosted.com") }}
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.elfhosted.com/role
                  operator: In
                  values:
                  - nodefinder
              topologyKey: "kubernetes.io/hostname"
              namespaceSelector: {}  # i.e., in the absense of any better signal, pick a node which already has zurg on it
        {{- end }}
      containers:
      - name: "nodefinder"
        command: [ "/usr/bin/sleep" ]
        args: [ "infinity" ]
        image: ghcr.io/elfhosted/tooling:focal-20240530@sha256:458d1f3b54e9455b5cdad3c341d6853a6fdd75ac3f1120931ca3c09ac4b588de
        resources:
          {{- $resources := dict }}
          {{- if .Values.hobbit.enabled }}
            {{- $resources = .Values.hobbit.resources }}
          {{- else if .Values.ranger.enabled }}
            {{- $resources = .Values.ranger.resources }}
          {{- else if .Values.halfling.enabled }}
            {{- $resources = .Values.halfling.resources }}
          {{- else if .Values.nazgul.enabled }}
            {{- $resources = .Values.nazgul.resources }}
          {{- else }}
            {{- $resources = dict "requests" (dict "cpu" "500m" "memory" "4Gi") "limits" (dict "cpu" "2" "memory" "4Gi") }}
          {{- end }}
          {{ $resources | toYaml | indent 10 | trimPrefix "          " }}
        volumeMounts:
        {{- if $pvc_config }}
        - mountPath: /config
          name: config
        {{- end }}
        {{- if $pvc_symlinks }}
        - mountPath: /symlinks
          name: symlinks
        {{- end }}        
      volumes:
      {{- if $pvc_config }}
      - name: config
        persistentVolumeClaim:
          claimName: config
      {{ end }}
      {{- if $pvc_symlinks }}
      - name: symlinks
        persistentVolumeClaim:
          claimName: symlinks
      {{ end }}